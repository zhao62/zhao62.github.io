---
layout: post
title: "Deep Residual Shrinkage Network: Isang Artificial Intelligence Method para sa Highly Noisy Data"
date: 2025-11-29
tags: [Deep Learning, AI]
mathjax: true
---

**Ang Deep Residual Shrinkage Network ay isang improved variant ng Deep Residual Network. Basically, ito ay integration ng Deep Residual Network, attention mechanisms, at soft thresholding functions.**

**Sa madaling salita, ang working principle ng Deep Residual Shrinkage Network ay pwedeng intindihin nang ganito: gamit ang attention mechanisms, napapansin nito ang mga unimportant features at sine-set ang mga ito sa zero sa pamamagitan ng soft thresholding functions; sa kabilang banda, napapansin din nito ang mga important features at pinapanatili ang mga ito. Dahil dito, napapalakas ang kakayahan ng deep neural network na mag-extract ng useful features mula sa mga signal na may noise.**

## 1. Research Motivation
**Una, kapag nagko-classify ng samples, hindi maiiwasan na may kasamang noise, tulad ng Gaussian noise, pink noise, at Laplacian noise.** More broadly, ang mga samples ay madalas may lamang impormasyon na irrelevant sa current classification task, at pwede rin itong ituring na noise. Ang noise na ito ay pwedeng makaapekto ng masama sa classification performance. (Ang soft thresholding ay isang key step sa maraming signal denoising algorithms.)

Halimbawa, kapag nagkukwentuhan sa tabi ng kalsada, ang boses ng nag-uusap ay pwedeng mahaluan ng busina ng sasakyan, tunog ng gulong, at iba pa. Kapag nag-perform ng speech recognition sa mga signal na ito, ang results ay siguradong maaapektuhan ng mga ingay na ito. From a deep learning perspective, ang mga features na corresponding sa busina at gulong ay dapat matanggal sa loob ng deep neural network para hindi makaapekto sa effect ng speech recognition.

**Pangalawa, kahit sa parehong sample set, ang amount ng noise ay madalas nag-iiba-iba bawat sample.** (May pagkakatulad ito sa attention mechanisms; halimbawa sa isang image sample set, ang location ng target object ay pwedeng magkaiba sa bawat picture; kayang i-target ng attention mechanism ang specific location ng object sa bawat picture.)

Halimbawa, kapag nagte-train ng cat-dog classifier, para sa 5 images na may label na "dog": ang 1st image ay pwedeng may aso at daga, ang 2nd image ay may aso at gansa, ang 3rd image ay may aso at manok, ang 4th image ay may aso at asno, at ang 5th image ay may aso at pato. Habang nagte-train tayo ng cat-dog classifier, hindi natin maiiwasan ang interference mula sa mga irrelevant objects tulad ng daga, gansa, manok, asno, at pato, na nagiging dahilan ng pagbaba ng classification accuracy. Kung mapapansin natin ang mga irrelevant na daga, gansa, manok, asno, at pato na ito, at matanggal ang features na corresponding sa kanila, posible nating mapataas ang accuracy ng cat-dog classifier.

## 2. Soft Thresholding
**Ang soft thresholding ay isang core step sa maraming signal denoising algorithms. Tinatanggal nito ang mga features na mas mababa ang absolute value kaysa sa isang threshold, at ang mga lampas naman sa threshold ay "hinihila" o sine-shrink papalapit sa zero.** Pwede itong i-implement gamit ang sumusunod na formula:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Ang derivative ng output ng soft thresholding with respect sa input ay:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Tulad ng makikita sa itaas, ang derivative ng soft thresholding ay either 1 or 0. Ang property na ito ay katulad ng sa ReLU activation function. Kaya naman, nakakatulong din ang soft thresholding para mabawasan ang risk ng deep learning algorithms na magkaroon ng gradient vanishing at gradient exploding problems.

**Sa soft thresholding function, ang pag-set ng threshold ay kailangang sumunod sa dalawang conditions: Una, ang threshold ay dapat positive number; Pangalawa, ang threshold ay hindi pwedeng lumampas sa maximum value ng input signal, kung hindi, ang output ay magiging zero lahat.**

**Kasabay nito, mas okay kung nasusunod ang third condition: bawat sample ay dapat may sariling independent threshold base sa noise content nito.**

Ito ay dahil madalas magkaiba ang noise content ng maraming samples. Halimbawa, madalas mangyari na sa iisang sample set, ang Sample A ay may konting noise, habang ang Sample B ay may maraming noise. So, kung magsasagawa ng soft thresholding sa isang denoising algorithm, dapat gumamit ng mas maliit na threshold ang Sample A, at mas malaking threshold naman ang Sample B. Sa deep neural networks, kahit na nawawala ang explicit physical meaning ng mga features at thresholds na ito, ang basic logic ay pareho pa rin. Ibig sabihin, bawat sample ay dapat may sariling independent threshold na naka-base sa sarili nitong noise content.

## 3. Attention Mechanism
Madaling intindihin ang attention mechanisms sa field ng computer vision. Ang visual system ng mga hayop ay kayang mag-scan ng mabilis sa buong area, ma-detect ang target object, at mag-focus ng attention sa object na iyon para mag-extract ng mas maraming details habang binabalewala ang ibang irrelevant information. Para sa karagdagang detalye, pwedeng mag-refer sa mga articles tungkol sa attention mechanisms.

Ang Squeeze-and-Excitation Network (SENet) ay isang medyo bagong deep learning method na gumagamit ng attention mechanisms. Sa iba't ibang samples, ang contribution ng magkakaibang feature channels sa classification task ay madalas hindi pare-pareho. Gumagamit ang SENet ng isang maliit na sub-network para makakuha ng isang set ng weights, at pagkatapos ay minumultiply ang weights na ito sa features ng bawat channel para ma-adjust ang laki ng features. Ang process na ito ay pwedeng ituring bilang paglalagay ng iba't ibang level ng attention sa bawat feature channel.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-29-DRSN-fil/SENET_fil_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Sa ganitong paraan, bawat sample ay magkakaroon ng sarili nitong independent set ng weights. In other words, ang weights ng kahit anong dalawang samples ay magkaiba. Sa SENet, ang specific path para makuha ang weights ay "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-29-DRSN-fil/SENET_fil_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding under Deep Attention Mechanism
Ginaya ng Deep Residual Shrinkage Network ang structure ng sub-network ng SENet na nabanggit sa itaas para ma-implement ang soft thresholding under deep attention mechanism. Gamit ang sub-network (na nasa loob ng red box), pwedeng matutunan ang isang set ng thresholds para i-apply ang soft thresholding sa bawat feature channel.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-29-DRSN-fil/DRSN_fil_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Sa sub-network na ito, kinukuha muna ang absolute values ng lahat ng features sa input feature map. Tapos, pagkatapos ng global average pooling at averaging, makakakuha ng isang feature, na tatawaging A. Sa kabilang path naman, ang feature map pagkatapos ng global average pooling ay ipapasok sa isang maliit na fully connected network. Ang fully connected network na ito ay gumagamit ng Sigmoid function as its last layer para i-normalize ang output sa pagitan ng 0 at 1, para makuha ang isang coefficient, na tatawaging α. Ang final threshold ay pwedeng i-express bilang α × A. Samakatuwid, ang threshold ay isang number sa pagitan ng 0 at 1 na minultiply sa average ng absolute values ng feature map. **Ang paraang ito ay hindi lang nagsisigurado na positive ang threshold, kundi sinisigurado rin na hindi ito sobrang laki.**

**Isa pa, nagkakaroon ng magkakaibang thresholds ang iba't ibang samples. Dahil dito, to a certain extent, pwede itong intindihin bilang isang special kind ng attention mechanism: napapansin nito ang mga features na irrelevant sa current task, at sa pamamagitan ng dalawang convolutional layers, tina-transform nito ang mga features na ito para maging close sa 0, at tuluyang sine-set sa zero gamit ang soft thresholding; or, napapansin nito ang mga features na relevant sa current task, tina-transform ang mga ito palayo sa 0 gamit ang dalawang convolutional layers, at pinapanatili ang mga ito.**

Sa huli, sa pamamagitan ng pag-stack ng certain number ng basic modules kasama ang convolutional layers, batch normalization, activation functions, global average pooling, at fully connected output layers, mabubuo ang kumpletong Deep Residual Shrinkage Network.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-29-DRSN-fil/DRSN_fil_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability
Sa katunayan, ang Deep Residual Shrinkage Network ay isang general feature learning method. Ito ay dahil sa maraming feature learning tasks, ang mga samples ay more or less naglalaman ng noise, pati na rin ng irrelevant information. Ang mga noise at irrelevant information na ito ay pwedeng makaapekto sa effect ng feature learning. Halimbawa:

Sa image classification, kung ang picture ay may maraming ibang objects, ang mga objects na ito ay pwedeng intindihin bilang "noise"; baka pwedeng gamitin ng Deep Residual Shrinkage Network ang attention mechanism para mapansin ang mga "noise" na ito, at gamit ang soft thresholding, i-set sa zero ang features na corresponding sa "noise", na posibleng magpataas ng accuracy ng image classification.

Sa speech recognition naman, kung nasa environment na medyo maingay, tulad ng pakikipag-kwentuhan sa tabi ng kalsada o sa loob ng factory, ang Deep Residual Shrinkage Network ay baka makapagpataas ng accuracy ng speech recognition, o makapagbigay ng ideya kung paano mapapataas ang accuracy nito.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Ang paper na ito ay may mahigit 1,400 citations na sa Google Scholar.

Ayon sa partial statistics, ang Deep Residual Shrinkage Network ay nagamit na o na-improve sa mahigit 1,000 papers para sa iba't ibang fields tulad ng mechanical engineering, electric power, computer vision, medical, speech, text, radar, remote sensing, at marami pang iba.
