---
layout: post
title: "Kunzwisisa Deep Residual Shrinkage Network: Nzira ye Artificial Intelligence ye Data ine Noise Yakawandisa"
subtitle: "Deep Residual Shrinkage Network: An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-12
tags: [Deep Learning, AI]
mathjax: true
---

**Iyi Deep Residual Shrinkage Network imhando ye Deep Residual Network yakagadziridzwa (improved variant). Kutaura chokwadi, Deep Residual Shrinkage Network inobatanidza zvinhu zvitatu: Deep Residual Network, attention mechanisms, ne soft thresholding functions.**

**Tinogona kunzwisisa mashandiro e Deep Residual Shrinkage Network nenzira inotevera. Pekutanga, network inoshandisa attention mechanisms ku-identifier (kuziva) ma features asina kukosha. Kubva ipapo, network inoshandisa soft thresholding functions kuti i-setter ma features asina kukosha aya ku zero. Asi, network ino-identifier ma features akakosha yobva yachengeta ma features iwayo. Izvi zvinoita kuti deep neural network ive ne simba riri nani. Process iyi inobatsira network ku-extractor (kutora) ma features anobatsira kubva mumasaini (signals) ane noise.**

## 1. Research Motivation (Chikonzero cheKutsvagurudza)

**Chekutanga, noise hainzveki (is inevitable) kana algorithm ichiita classify ma samples. Mienzaniso ye noise iyi inosanganisira Gaussian noise, pink noise, ne Laplacian noise.** Kutaura zvakafara, ma samples anowanzoita ruzivo (information) rusinei ne current classification task yauri kuita. Tinogona kutora ruzivo rusingaenderane nebasa iri se noise. Noise iyi inogona kudzikisira mashandiro e classification. (Soft thresholding idanho rakakosha muma algorithms akawanda e signal denoising.)

Semuenzaniso, fungidzira vanhu vari kutaura paroadside (pamugwagwa). Audio yacho inogona kunge iine ruzha rwehon’a dzemota nemavhiri. Tinogona kuedza kuita speech recognition pamasaini aya. Ruzha ruri kumashure (background sounds) runokanganisa ma results. Tichitarisa neziso re deep learning, iyo deep neural network inofanira ku-eliminater (kubvisa) ma features anoenderana nehon’a nemavhiri. Ku-eliminater uku kunotadzisa ma features iwayo kukanganisa ma results e speech recognition.

**Chechipiri, huwandu hwe noise hunowanzo siyana pakati pema samples. Musiyano uyu unoitika kunyangwe mukati me dataset imwe chete.** (Musiyano uyu wakada kufanana ne attention mechanisms. Tora image dataset semuenzaniso. Nzvimbo ine target object inogona kusiyana mumufananidzo wega wega. Attention mechanisms inogona kuita focus panzvimbo chaiyo ine target object mumufananidzo wega wega.)

Semuenzaniso, fungidzira tichi-trainer classifier yekusiyanisa katsi nembwa (cat-and-dog classifier) tichishandisa mifananidzo mishanu yakanyorwa kuti "dog." Image 1 inogona kuva nembwa negonzo. Image 2 inogona kuva nembwa negeese. Image 3 inogona kuva nembwa nehuku. Image 4 inogona kuva nembwa nedhongi. Image 5 inogona kuva nembwa nedhadha. Pakuita training, zvinhu zvisinei nenyaya yacho zvinokanganisa classifier. Zvinhu izvi zvinosanganisira magonzo, mageese, huku, madhongi, nemadhadha. Kuvhiringidza uku kunoita kuti classification accuracy iderere. Saka, kana tikakwanisa ku-identifier zvinhu izvi zvisina basa. Tinogona kubva ta-eliminater ma features anoenderana nezvinhu izvi. Nenzira iyi, tinogona ku-improver accuracy ye cat-and-dog classifier.

## 2. Soft Thresholding

**Soft thresholding idanho guru (core step) muma algorithms akawanda e signal denoising. Algorithm ino-eliminater ma features kana absolute values dzema features acho dziri diki pane threshold yakati. Algorithm ino-shrinker (kuderedza) ma features kuenda ku zero kana absolute values dzema features acho dzakakura kudarika threshold iyoyo.** Ma researchers anogona ku-implementer soft thresholding achishandisa formula inotevera:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Derivative ye output ye soft thresholding zvichienderana ne input ndeye kuti:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Formula iri pamusoro inoratidza kuti derivative ye soft thresholding inenge iri 1 kana kuti 0. Hunhu (property) uhwu hwakafanana nehunhu hwe ReLU activation function. Saka, soft thresholding inogona kuderedza njodzi ye gradient vanishing ne gradient exploding muma algorithms e deep learning.

**Mu soft thresholding function, ma settings e threshold anofanira kuzadzisa zvinhu zviviri (conditions). Chekutanga, threshold inofanira kuva positive number. Chechipiri, threshold haifaniri kudarika maximum value ye input signal. Kana zvikasadaro, output inozongova zero yese.**

**Pamusoro pezvo, zviri nani kuti threshold izadzise condition yechitatu. Sample imwe neimwe inofanira kuva ne threshold yayo yakazvimirira (independent) zvichienderana nehuwandu hwe noise iri mu sample iyoyo.**

Chikonzero ndechekuti huwandu hwe noise hunowanzo siyana pakati pema samples. Semuenzaniso, Sample A inogona kunge iine noise shoma asi Sample B iine noise yakawanda mu dataset imwe chete. Panyaya iyi, Sample A inofanira kushandisa threshold diki pakuita soft thresholding. Sample B inofanira kushandisa threshold hombe. Mu deep neural networks, kunyangwe zvazvo ma features aya nema thresholds zvichirasikirwa ne explicit physical definitions. Asi, logic yekutanga inoramba iripo. Kureva kuti (In other words), sample imwe neimwe inofanira kuva ne threshold yakazvimirira. Noise content iripo ndiyo inotara threshold iyoyo.

## 3. Attention Mechanism

Ma researchers anogona kunzwisisa attention mechanisms zviri nyore mu field ye computer vision. Ma visual systems emhuka anogona kusiyanisa ma targets nekutarisa (scanning) nzvimbo yese nekukasira. Mushure mezvo, ma visual systems anoita focus attention pa target object chaiyo. Chiito ichi chinoita kuti ma systems akwanise ku-extractor ma details akawanda. Panguva imwe chete, ma systems ano-suppresser ruzivo rusingakoshi (irrelevant information). Kuti uwane zvizere, ndapota tarisa literature ine chekuita ne attention mechanisms.

Squeeze-and-Excitation Network (SENet) imhando itsva ye deep learning method inoshandisa attention mechanisms. Pama samples akasiyana, ma feature channels akasiyana anobatsira zvakasiyana pa classification task. SENet inoshandisa sub-network diki kuti iwane set ye ma weights. SENet inobva ya-multiplier ma weights aya ne ma features e ma channels acho. Operation iyi inogadzirisa (adjust) kukura kwe ma features mu channel imwe neimwe. Tinogona kuona process iyi sekushandisa ma levels akasiyana e attention pama feature channels akasiyana.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Munzira iyi, sample yega yega ine set ye ma weights yakazvimirira. Kureva kuti, ma weights e ma samples maviri api naapi akasiyana. Mu SENet, nzira chaiyo (specific path) yekuwana ma weights ndeye: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Deep Residual Shrinkage Network inoshandisa structure ye SENet sub-network. Network inoshandisa structure iyi ku-implementer soft thresholding pasi pe deep attention mechanism. Iyi sub-network (iri mu red box) inodzidza (Learn a set of thresholds). Network inobva yashandisa soft thresholding pa feature channel imwe neimwe ichishandisa ma thresholds aya.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Mu sub-network iyi, system inotanga ya-calculator ma absolute values e ma features ese ari mu input feature map. System inobva yaita global average pooling ne averaging kuti iwane feature, yatinodaidza kuti A. Kune rimwe divi (path), system ino-inputter feature map mu fully connected network diki mushure me global average pooling. Iyi fully connected network inoshandisa Sigmoid function se layer yekupedzisira. Function iyi ino-normalizer output kutiive pakati pa 0 na 1. Process iyi inoburitsa coefficient, yatinodaidza kuti α. Tinogona kunyora threshold yekupedzisira sa α × A. Saka, threshold i product ye manhamba maviri. Nhamba imwe iri pakati pa 0 na 1. Imwe nhamba i average ye ma absolute values e feature map. **Method iyi inoona kuti threshold inogara iri positive. Method iyi inoonawo kuti threshold hainyanyi kukura zvakanyanya.**

**Zvakare, ma samples akasiyana anoburitsa ma thresholds akasiyana. Nekudaro, tinogona kunzwisisa method iyi se attention mechanism yakakosha (specialized). Mechanism iyi ino-identifier ma features asinei ne current task. Mechanism iyi ino-transformer ma features aya kutiave ma values ari pedyo ne zero ichishandisa ma convolutional layers maviri. Mechanism inobva ya-setter ma features aya ku zero ichishandisa soft thresholding. Neimwe nzira, mechanism ino-identifier ma features ane chekuita ne current task. Mechanism iyi ino-transformer ma features aya kutiave ma values ari kure ne zero ichishandisa ma convolutional layers maviri. Pakupedzisira, mechanism ino-preserver (kuchengeta) ma features aya.**

Pakupedzisira, tino-stacker (Stack many basic modules) huwandu hwakati hwe ma basic modules. Tinosanganisirawo ma convolutional layers, batch normalization, activation functions, global average pooling, ne fully connected output layers. Process iyi inovaka Deep Residual Shrinkage Network yakakwana.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability (Kugona Kushanda Kwakawanda)

Deep Residual Shrinkage Network inzira ye general method ye feature learning. Chikonzero ndechekuti muma task akawanda e feature learning, ma samples anowanzoita noise. Ma samples ane ruzivo rusingakoshi (irrelevant information). Noise iyi neruzivo rusingakoshi zvinogona kukanganisa mashandiro e feature learning. Semuenzaniso:

Funga nezve image classification. Mufananidzo unogona kunge uine zvimwe zvinhu zvakawanda panguva imwe chete. Tinogona kunzwisisa zvinhu izvi se "noise." Deep Residual Shrinkage Network inogona kukwanisa kushandisa attention mechanism. Network inoona "noise" iyi. Network inobva yashandisa soft thresholding ku-setter ma features anoenderana ne "noise" iyi ku zero. Chiito ichi chinogona ku-improver accuracy ye image classification.

Funga nezve speech recognition. Kunyanya mumamiriro ane ruzha rwakawanda (noisy environments) sekutaura paroadside kana mukati me workshop ye factory. Deep Residual Shrinkage Network inogona ku-improver accuracy ye speech recognition. Kana zvirinani, network inopa methodology. Methodology iyi inokwanisa ku-improver accuracy ye speech recognition.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact (Kukosha kwayo muDzisvondo)

Pepa iri (paper) rawana ma citations anodarika 1,400 pa Google Scholar.

Zvichienderana ne statistics dzisina kukwana, ma researchers akashandisa Deep Residual Shrinkage Network (DRSN) mumabhuku/zvidzidzo (publications/studies) zvinodarika 1,000. Mashandisirwo aya anobata ma fields akawanda kwazvo. Ma fields aya anosanganisira mechanical engineering, electrical power, vision, healthcare, speech, text, radar, ne remote sensing.
