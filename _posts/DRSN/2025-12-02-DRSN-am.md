---
layout: post
title: "Deep Residual Shrinkage Network: ለ Highly Noisy Data የሚሆን የ Artificial Intelligence Method"
date: 2025-12-02
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network የ Deep Residual Network improved variant ነው። በመሠረቱ፣ የ Deep Residual Network፣ Attention Mechanisms እና Soft Thresholding Functions ውህደት (integration) ነው።"
---

**Deep Residual Shrinkage Network የ Deep Residual Network improved variant ነው። በመሠረቱ፣ የ Deep Residual Network፣ Attention Mechanisms እና Soft Thresholding Functions ውህደት (integration) ነው።**

**በተወሰነ ደረጃ፣ የ Deep Residual Network working principle እንደሚከተለው ሊረዳ ይችላል፡ Attention Mechanismsን በመጠቀም unimportant featuresን ይለያል (identify ያደርጋል) እና Soft Thresholding Functionsን በመጠቀም ወደ zero ይቀይራቸዋል፤ በሌላ በኩል ደግሞ important featuresን ለይቶ ያስቀራቸዋል። ይህ ሂደት Deep Neural Networkው Noise ከያዙ Signals ላይ useful featuresን extract ለማድረግ ያለውን ability ያሻሽላል።**

## 1. Research Motivation

**አንደኛ፣ Samplesን classify ሲደረግ፣ እንደ Gaussian noise፣ Pink noise፣ እና Laplacian noise ያሉ Noise መኖራቸው የማይቀር (inevitable) ነው።** በሰፊው ሲታይ፣ Samples ብዙውን ጊዜ ለ current classification task irrelevant የሆነ information ይይዛሉ፣ ይህም እንደ noise ሊወሰድ ይችላል። ይህ noise በ classification performance ላይ negative effect ሊኖረው ይችላል። (Soft thresholding በብዙ signal denoising algorithms ውስጥ ቁልፍ step ነው።)

ለምሳሌ፣ በመንገድ ዳር በሚደረግ conversation ወቅት፣ audioው ከ car horns እና wheels ድምፅ ጋር ሊቀላቀል ይችላል። በእነዚህ signals ላይ **Speech Recognition** ሲሰራ፣ results በእነዚህ background sounds መጎዳታቸው አይቀርም። ከ **Deep Learning** perspective፣ ለ horns እና wheels የሚሆኑት features በ **Speech Recognition** results ላይ ተጽእኖ እንዳያሳድሩ በ **Deep Neural Network** ውስጥ መወገድ (eliminated መደረግ) አለባቸው።

**ሁለተኛ፣ በአንድ Dataset ውስጥ እንኳን፣ የ Noise amount ብዙውን ጊዜ ከ sample ወደ sample ይለያያል።** (ይህ ከ Attention Mechanisms ጋር ተመሳሳይነት አለው፤ የ image datasetን እንደ example ብንወስድ፣ የ target object location በ images መካከል ሊለያይ ይችላል፣ እና Attention Mechanisms በእያንዳንዱ image ውስጥ ባለው የ target object specific location ላይ focus ሊያደርጉ ይችላሉ።)

ለምሳሌ፣ የ cat-and-dog classifier በሚሰለጥንበት (train በሚደረግበት) ጊዜ፣ "dog" ተብለው የተሰየሙ አምስት images እንውሰድ። የመጀመሪያው image dog እና mouse ሊይዝ ይችላል፣ ሁለተኛው dog እና goose፣ ሶስተኛው dog እና chicken፣ አራተኛው dog እና donkey፣ አምስተኛው ደግሞ dog እና duck ሊይዙ ይችላሉ። Training በሚደረግበት ጊዜ፣ classifierው እንደ mice፣ geese፣ chickens፣ donkeys እና ducks ባሉ irrelevant objects interference መጋለጡ አይቀርም፣ ይህም የ classification accuracy እንዲቀንስ ያደርጋል። እነዚህን irrelevant objects—mice፣ geese፣ chickens፣ donkeys እና ducks—identify ማድረግ እና የእነሱን corresponding features eliminate ማድረግ ከተቻለ፣ የ cat-and-dog classifierን accuracy ማሻሻል ይቻላል።

## 2. Soft Thresholding

**Soft thresholding በብዙ signal denoising algorithms ውስጥ core step ነው። Absolute valuesአቸው ከተወሰነ threshold በታች የሆኑ featuresን eliminate ያደርጋል እና absolute valuesአቸው ከዚህ threshold በላይ የሆኑ featuresን ወደ zero shrink ያደርጋል።** የሚከተለውን formula በመጠቀም implement ሊደረግ ይችላል：

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

የ **Soft thresholding** output derivative ከ input አንፃር ሲታይ：

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

ከላይ እንደተመለከተው፣ የ **Soft thresholding** derivative ወይ 1 ነው ወይ ደግሞ 0 ነው። ይህ property ከ **ReLU Activation Function** ጋር ተመሳሳይ ነው። ስለዚህ፣ **Soft thresholding**፣ **Deep Learning Algorithms** የ gradient vanishing እና gradient exploding riskን እንዲቀንሱ ሊረዳ ይችላል።

**በ Soft thresholding function ውስጥ፣ የ threshold setting ሁለት conditionsን ማሟላት (satisfy ማድረግ) አለበት： አንደኛ፣ threshold positive number መሆን አለበት፤ ሁለተኛ፣ threshold ከ input signal maximum value መብለጥ የለበትም፣ አለበለዚያ outputው በሙሉ zero ይሆናል።**

**በተጨማሪም፣ threshold ሶስተኛ condition ቢያሟላ ይመረጣል： እያንዳንዱ sample በውስጡ በያዘው noise content ላይ በመመስረት የራሱ independent threshold ሊኖረው ይገባል።**

ምክንያቱም የ noise content ብዙውን ጊዜ በ samples መካከል ይለያያል። ለምሳሌ፣ በአንድ Dataset ውስጥ Sample A ያነሰ noise ሲኖረው Sample B ደግሞ ብዙ noise መያዙ የተለመደ ነው። በዚህ case፣ በ denoising algorithm ውስጥ soft thresholding ሲሰራ፣ Sample A ትንሽ threshold መጠቀም ሲገባው፣ Sample B ደግሞ ትልቅ threshold መጠቀም ይኖርበታል። ምንም እንኳን እነዚህ features እና thresholds በ **Deep Neural Networks** ውስጥ explicit physical definitions ቢጠፉም፣ መሠረታዊው logic ተመሳሳይ ነው። በሌላ አነጋገር፣ እያንዳንዱ sample በእራሱ specific noise content የሚወሰን የራሱ independent threshold ሊኖረው ይገባል።

## 3. Attention Mechanism

**Attention Mechanisms** በ **Computer Vision** field ውስጥ ለመረዳት ቀላል ናቸው። የ animals visual systems ሙሉውን area በፍጥነት scan በማድረግ targetsን መለየት (distinguish ማድረግ) ይችላሉ፣ ከዚያም irrelevant informationን suppress በማድረግ ለተጨማሪ details በ target object ላይ attention focus ያደርጋሉ። ለዝርዝር መረጃ፣ ስለ Attention Mechanisms የተጻፉ literatureን ይመልከቱ።

**Squeeze-and-Excitation Network (SENet)**፣ Attention Mechanismsን የሚጠቀም አንጻራዊ አዲስ **Deep Learning** method ነው። በተለያዩ samples ውስጥ፣ ለ classification task የ feature channels contribution ብዙውን ጊዜ ይለያያል። SENet አንድ small sub-network በመጠቀም set of weights ያገኛል፣ ከዚያም በየ channelው ያለውን feature magnitude adjust ለማድረግ እነዚህን weights ከ respective channels features ጋር ያባዛል (multiply ያደርጋል)። ይህ process በተለያዩ feature channels ላይ varying levels of attention እንደ መስጠት ሊታይ ይችላል።

<p align="center">
  <img src="/assets/img/DRSN/2025-12-02-DRSN-am/SENET_am_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

በዚህ approach፣ እያንዳንዱ sample የራሱ independent set of weights አለው። በሌላ አነጋገር፣ ለማንኛውም ሁለት arbitrary samples፣ weights የተለያዩ ናቸው። በ SENet ውስጥ፣ weightsን ለማግኘት specific path "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function" ነው።

<p align="center">
  <img src="/assets/img/DRSN/2025-12-02-DRSN-am/SENET_am_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

**Deep Residual Shrinkage Network** በ deep attention mechanism ስር soft thresholdingን implement ለማድረግ ከላይ ከተጠቀሰው SENet sub-network structure inspiration ይወስዳል። በ sub-networkው አማካኝነት (በ red box ውስጥ የሚታየው)፣ ለእያንዳንዱ feature channel soft thresholding apply ለማድረግ የሚያገለግሉ set of thresholds መማር (learn ማድረግ) ይቻላል።

<p align="center">
  <img src="/assets/img/DRSN/2025-12-02-DRSN-am/DRSN_am_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

በዚህ sub-network ውስጥ፣ በመጀመሪያ የ input feature map ሁሉም features absolute values calculate ይደረጋሉ። ከዚያም፣ በ **Global Average Pooling** እና averaging አማካኝነት፣ *A* ተብሎ የሚጠራ feature ይገኛል። በሌላኛው path፣ ከ Global Average Pooling በኋላ ያለው feature map ወደ ትንሽ **Fully Connected Network** input ይደረጋል። ይህ Fully Connected Network፣ outputን በ 0 እና 1 መካከል normalize ለማድረግ **Sigmoid Function**ን እንደ final layer ይጠቀማል፣ ይህም *α* ተብሎ የሚጠራ coefficient ይሰጣል። Final threshold በ *α × A* ሊገለፅ ይችላል። ስለዚህ፣ threshold በ 0 እና 1 መካከል ያለ ቁጥር እና የ feature map absolute values average ብዜት (product) ነው። **ይህ method፣ threshold positive መሆኑን ብቻ ሳይሆን ከመጠን በላይ ትልቅ (excessively large) አለመሆኑንም ensure ያደርጋል።**

**በተጨማሪም፣ different samples different thresholds ያስገኛሉ። በዚህም ምክንያት፣ በተወሰነ ደረጃ፣ ይህ እንደ specialized Attention Mechanism ሊተረጎም ይችላል： ለ current task irrelevant የሆኑ featuresን identify ያደርጋል፣ በሁለት convolutional layers አማካኝነት ወደ zero ቅርብ ወደሆኑ values transform ያደርጋቸዋል፣ እና soft thresholdingን በመጠቀም zero ያደርጋቸዋል፤ በሌላ በኩል ደግሞ፣ ለ current task relevant የሆኑ featuresን identify ያደርጋል፣ በሁለት convolutional layers አማካኝነት ከ zero ወደ ራቁ values transform ያደርጋቸዋል፣ እና preserve ያደርጋቸዋል።**

በመጨረሻም፣ የተወሰኑ basic modules ከ convolutional layers፣ batch normalization፣ activation functions፣ global average pooling፣ እና fully connected output layers ጋር stack በማድረግ፣ ሙሉው **Deep Residual Shrinkage Network** ይገነባል።

<p align="center">
  <img src="/assets/img/DRSN/2025-12-02-DRSN-am/DRSN_am_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

**Deep Residual Shrinkage Network**፣ እንደውም general feature learning method ነው። ምክንያቱም፣ በብዙ feature learning tasks ውስጥ፣ samples ብዙም ይሁን ትንሽ (more or less) የተወሰነ noise እንዲሁም irrelevant information ይይዛሉ። ይህ noise እና irrelevant information የ feature learning performanceን affect ሊያደርጉ ይችላሉ። ለምሳሌ：

በ **Image Classification** ጊዜ፣ አንድ image በተመሳሳይ ጊዜ ብዙ ሌሎች objects ከያዘ፣ እነዚህ objects እንደ "noise" ሊወሰዱ (understood ሊደረጉ) ይችላሉ። Deep Residual Shrinkage Network ይህንን "noise" notice ለማድረግ attention mechanismን ሊጠቀም ይችላል፣ ከዚያም የዚህን "noise" features zero ለማድረግ soft thresholdingን ይጠቀማል፣ በዚህም የ image classification accuracyን improve ሊያደርግ ይችላል።

በ **Speech Recognition** ውስጥ፣ በተለይም እንደ መንገድ ዳር ወይም በ factory workshop ውስጥ ባሉ relatively noisy environments ውስጥ፣ Deep Residual Shrinkage Network የ speech recognition accuracyን improve ሊያደርግ ይችላል፣ ወይም ቢያንስ speech recognition accuracyን improve ማድረግ የሚችል methodology ያቀርባል።

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

ይህ paper በ **Google Scholar** ላይ ከ1,400 ጊዜ በላይ cite ተደርጓል።

ባልተሟላ statistics መሠረት፣ **Deep Residual Shrinkage Network (DRSN)** በ Mechanical Engineering፣ Electrical Power፣ Vision፣ Healthcare፣ Speech፣ Text፣ Radar፣ እና Remote Sensingን ጨምሮ በተለያዩ fields ውስጥ ከ1,000 በላይ በሚሆኑ publications/studies ውስጥ በቀጥታ ተተግብሯል (applied) ወይም ተሻሽሎ (modified) ጥቅም ላይ ውሏል።
