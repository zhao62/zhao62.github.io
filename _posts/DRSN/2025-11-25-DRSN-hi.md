---
layout: post
title: "गहन अवशिष्ट संकुचन नेटवर्क (Deep Residual Shrinkage Network): High Noise Data के लिए एक Artificial Intelligence मेथड"
date: 2025-11-25
tags: [Deep Learning, AI]
mathjax: true
---

**गहन अवशिष्ट संकुचन नेटवर्क (Deep Residual Shrinkage Network), Deep Residual Network (ResNet) का ही एक उन्नत (Improved) संस्करण है। वास्तव में, यह Deep Residual Network, Attention Mechanism और Soft Thresholding Function का एक इंटीग्रेशन (Integration) है।**

**एक हद तक, गहन अवशिष्ट संकुचन नेटवर्क (DRSN) के काम करने के सिद्धांत को इस तरह समझा जा सकता है: यह Attention Mechanism के माध्यम से "गैर-महत्वपूर्ण फीचर्स" (Unimportant Features) को नोटिस करता है और Soft Thresholding Function के जरिए उन्हें शून्य (Zero) कर देता है; या दूसरे शब्दों में, यह महत्वपूर्ण फीचर्स को नोटिस करता है और उन्हें सुरक्षित रखता है। इससे Deep Neural Network की Noise वाले सिग्नल्स से उपयोगी फीचर्स (Useful Features) निकालने की क्षमता बढ़ जाती है।**

## 1. अनुसंधान की प्रेरणा (Research Motivation)

**सबसे पहले, जब हम Samples को classify करते हैं, तो Samples में अनिवार्य रूप से कुछ Noise होता ही है, जैसे Gaussian Noise, Pink Noise, Laplacian Noise आदि।** अधिक व्यापक रूप से कहें तो, **Sample** में ऐसी जानकारी भी हो सकती है जिसका वर्तमान **Classification Task** से कोई लेना-देना नहीं है; इस जानकारी को भी **Noise** माना जा सकता है। यह **Noise** हमारे **Classification** के परिणाम पर बुरा असर डाल सकता है। (**Soft Thresholding** कई **Signal Denoising Algorithms** में एक महत्वपूर्ण स्टेप है)।

उदाहरण के लिए, सड़क के किनारे बात करते समय, बातचीत की आवाज़ में गाड़ियों के हॉर्न और पहियों की आवाज़ भी मिल सकती है। जब इन वॉयस सिग्नल्स पर **Speech Recognition** किया जाता है, तो हॉर्न और पहियों की आवाज़ के कारण परिणाम पर असर पड़ना स्वाभाविक है। **Deep Learning** के नज़रिए से, इन हॉर्न और पहियों की आवाज़ से जुड़े **Features** को **Deep Neural Network** के अंदर ही डिलीट कर दिया जाना चाहिए, ताकि **Speech Recognition** पर इनका कोई प्रभाव न पड़े।

**दूसरा, एक ही Sample Set में भी, अलग-अलग Samples का Noise Level अक्सर अलग-अलग होता है।** (यह **Attention Mechanism** से काफी मिलता-जुलता है; एक **Image Sample Set** का उदाहरण लें, तो अलग-अलग तस्वीरों में टारगेट ऑब्जेक्ट की लोकेशन अलग-अलग हो सकती है; **Attention Mechanism** हर तस्वीर के लिए टारगेट ऑब्जेक्ट की लोकेशन को नोटिस कर सकता है)।

उदाहरण के लिए, जब हम एक **Cat-Dog Classifier** को ट्रेन कर रहे होते हैं, तो "Dog" लेबल वाली 5 तस्वीरों के लिए:
- पहली तस्वीर में कुत्ते के साथ चूहा हो सकता है,
- दूसरी तस्वीर में कुत्ते के साथ हंस हो सकता है,
- तीसरी तस्वीर में कुत्ते के साथ मुर्गी हो सकती है,
- चौथी में गधा और पांचवीं में बत्तख हो सकती है।

जब हम **Classifier** को ट्रेन करते हैं, तो चूहा, हंस, मुर्गी, गधा और बत्तख जैसे irrelevant objects हमारे मॉडल में **Interference** पैदा करते हैं, जिससे **Classification Accuracy** कम हो जाती है। यदि हम इन irrelevant चूहे, हंस, आदि को नोटिस कर सकें और उनसे जुड़े **Features** को हटा दें, तो **Cat-Dog Classifier** की **Accuracy** में सुधार हो सकता है।

## 2. सॉफ्ट थ्रेशोल्डिंग (Soft Thresholding)

**Soft Thresholding कई Signal Denoising Algorithms का मुख्य स्टेप है। यह उन फीचर्स को हटा देता है जिनकी Absolute Value एक निश्चित Threshold से कम होती है, और जिनकी Value इस Threshold से ज्यादा होती है, उन्हें शून्य (Zero) की दिशा में सिकोड़ (Shrink) देता है।** इसे निम्नलिखित फॉर्मूले द्वारा लागू किया जा सकता है:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Soft Thresholding** का आउटपुट, इनपुट के सापेक्ष (Derivative) इस प्रकार है:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

जैसा कि ऊपर देखा जा सकता है, **Soft Thresholding** का डेरिवेटिव या तो 1 है या 0। यह गुण **ReLU Activation Function** के समान ही है। इसलिए, **Soft Thresholding** भी **Deep Learning Algorithms** में **Gradient Vanishing** और **Gradient Exploding** के जोखिम को कम करने में सक्षम है।

**Soft Thresholding Function में, Threshold सेट करते समय दो शर्तों का पालन करना आवश्यक है:**
1. **Threshold** एक पॉजिटिव नंबर (Positive Number) होना चाहिए।
2. **Threshold** इनपुट सिग्नल की मैक्सिमम वैल्यू (**Maximum Value**) से बड़ा नहीं होना चाहिए, अन्यथा आउटपुट पूरी तरह से शून्य हो जाएगा।

**इसके साथ ही, Threshold को तीसरी शर्त भी पूरी करनी चाहिए: हर Sample का उसके अपने Noise Content के आधार पर अपना एक स्वतंत्र Threshold होना चाहिए।**

ऐसा इसलिए है क्योंकि कई बार **Samples** में **Noise** की मात्रा अलग-अलग होती है। उदाहरण के लिए, एक ही **Sample Set** में Sample A में कम **Noise** हो सकता है और Sample B में ज्यादा। ऐसे में, **Denoising Algorithm** में **Soft Thresholding** करते समय, Sample A के लिए बड़ा **Threshold** और Sample B के लिए छोटा **Threshold** इस्तेमाल किया जाना चाहिए। **Deep Neural Network** में, भले ही इन **Features** और **Thresholds** का स्पष्ट भौतिक अर्थ (Physical Meaning) न हो, लेकिन मूल तर्क वही रहता है। यानी, हर **Sample** का उसके **Noise Content** के अनुसार अपना अलग **Threshold** होना चाहिए।

## 3. अटेंशन मैकेनिज्म (Attention Mechanism)

**Computer Vision** के क्षेत्र में **Attention Mechanism** को समझना काफी आसान है। जानवरों का विजुअल सिस्टम तेजी से पूरे क्षेत्र को स्कैन कर सकता है, टारगेट ऑब्जेक्ट को ढूंढ सकता है, और फिर **Attention** को उस टारगेट पर केंद्रित कर सकता है ताकि अधिक विवरण (Details) निकाले जा सकें और irrelevant जानकारी को दबाया (Suppress) जा सके। अधिक जानकारी के लिए आप **Attention Mechanism** पर अन्य लेख देख सकते हैं।

**Squeeze-and-Excitation Network (SENet)**, **Attention Mechanism** के तहत एक अपेक्षाकृत नई **Deep Learning** विधि है। अलग-अलग **Samples** में, अलग-अलग **Feature Channels** का **Classification Task** में योगदान (Contribution) अक्सर अलग-अलग होता है। **SENet** एक छोटे **Sub-network** का उपयोग करके **Weights** का एक सेट प्राप्त करता है, और फिर इन **Weights** को संबंधित **Channels** के फीचर्स से गुणा करता है ताकि फीचर्स के आकार (Size) को समायोजित किया जा सके। इस प्रक्रिया को अलग-अलग **Feature Channels** पर अलग-अलग मात्रा में **Attention** लगाने के रूप में देखा जा सकता है।

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-hi/SENET_hi_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

इस तरीके में, हर **Sample** के पास **Weights** का अपना एक स्वतंत्र सेट होगा। दूसरे शब्दों में, किन्हीं भी दो **Samples** के **Weights** अलग-अलग होंगे। **SENet** में, **Weights** प्राप्त करने का रास्ता है: "**Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function**"।

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-hi/SENET_hi_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. डीप अटेंशन मैकेनिज्म के तहत सॉफ्ट थ्रेशोल्डिंग (Soft Thresholding under Deep Attention Mechanism)

**गहन अवशिष्ट संकुचन नेटवर्क (DRSN)** ने **Deep Attention Mechanism** के तहत **Soft Thresholding** को लागू करने के लिए ऊपर बताए गए **SENet** के **Sub-network** स्ट्रक्चर को अपनाया है। ब्लू बॉक्स (Blue Box) के अंदर मौजूद **Sub-network** के जरिए, यह **Thresholds** का एक सेट सीख (Learn) सकता है और प्रत्येक **Feature Channel** पर **Soft Thresholding** लागू कर सकता है।

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-hi/DRSN_hi_1.png" alt="गहन अवशिष्ट संकुचन नेटवर्क (Deep Residual Shrinkage Network)" width="75%">
</p>

इस **Sub-network** में, सबसे पहले इनपुट **Feature Map** के सभी फीचर्स की **Absolute Value** निकाली जाती है। फिर **Global Average Pooling** और एवरेजिंग के बाद एक फीचर प्राप्त होता है, जिसे 'A' माना जाता है। दूसरे रास्ते में, **Global Average Pooling** के बाद प्राप्त **Feature Map** को एक छोटे **Fully Connected Network** में इनपुट किया जाता है। इस **Fully Connected Network** की आखिरी लेयर **Sigmoid Function** होती है, जो आउटपुट को 0 और 1 के बीच **Normalize** करती है, जिससे एक गुणांक (Coefficient) प्राप्त होता है, जिसे 'α' (alpha) माना जाता है।

अंतिम **Threshold** को α × A के रूप में दर्शाया जा सकता है। इसलिए, **Threshold** का मतलब है: 0 और 1 के बीच की एक संख्या × **Feature Map** की **Absolute Value** का औसत। **यह तरीका न केवल यह सुनिश्चित करता है कि Threshold पॉजिटिव हो, बल्कि यह भी कि वह बहुत बड़ा न हो।**

**इसके अलावा, अलग-अलग Samples के लिए अलग-अलग Thresholds होते हैं। इसलिए, एक हद तक, इसे एक विशेष प्रकार का Attention Mechanism समझा जा सकता है: जो मौजूदा टास्क से असंबंधित (Irrelevant) फीचर्स को नोटिस करता है, दो Convolutional Layers के माध्यम से उन्हें 0 के करीब ले जाता है, और Soft Thresholding के जरिए उन्हें पूरी तरह शून्य कर देता है; या फिर, जो संबंधित (Relevant) फीचर्स को नोटिस करता है और उन्हें सुरक्षित रखता है।**

अंत में, कुछ बेसिक मॉड्यूल्स के साथ **Convolutional Layers**, **Batch Normalization**, **Activation Function**, **Global Average Pooling** और **Fully Connected Output Layer** को स्टैक (Stack) करके पूरा **गहन अवशिष्ट संकुचन नेटवर्क (DRSN)** तैयार होता है।

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-hi/DRSN_hi_2.png" alt="गहन अवशिष्ट संकुचन नेटवर्क (Deep Residual Shrinkage Network)" width="65%">
</p>

## 5. व्यापकता (Generality)

**गहन अवशिष्ट संकुचन नेटवर्क** वास्तव में एक सामान्य (General) **Feature Learning Method** है। ऐसा इसलिए है क्योंकि कई **Feature Learning Tasks** में, **Samples** में थोड़ा-बहुत **Noise** या असंबंधित जानकारी होती ही है। यह **Noise** और असंबंधित जानकारी **Feature Learning** के प्रभाव को कम कर सकती है। उदाहरण के लिए:

- **Image Classification** में, यदि तस्वीर में कई अन्य वस्तुएं भी हैं, तो इन वस्तुओं को "**Noise**" समझा जा सकता है; **गहन अवशिष्ट संकुचन नेटवर्क** अपने **Attention Mechanism** की मदद से इस "**Noise**" को नोटिस कर सकता है और **Soft Thresholding** का उपयोग करके इनसे जुड़े फीचर्स को शून्य कर सकता है, जिससे **Image Classification** की **Accuracy** बढ़ सकती है।

- **Speech Recognition** में, यदि वातावरण शोरगुल वाला है (जैसे सड़क किनारे या फैक्ट्री में बातचीत), तो **गहन अवशिष्ट संकुचन नेटवर्क** **Accuracy** को बेहतर बना सकता है, या कम से कम सटीकता बढ़ाने का एक नया दृष्टिकोण (Approach) प्रदान कर सकता है।

## संदर्भ (References):

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2942898}
}
```

## प्रभाव (Impact)

इस पेपर (Research Paper) के **Google Scholar** पर 1400 से अधिक **Citations** हैं।

अपूर्ण आंकड़ों के अनुसार, **गहन अवशिष्ट संकुचन नेटवर्क (DRSN)** का उपयोग या सुधार करके 1000 से अधिक शोध पत्रों (Research Papers) में इसे मैकेनिकल, पावर, विजन, मेडिकल, वॉयस, टेक्स्ट, रडार, और रिमोट सेंसिंग जैसे कई क्षेत्रों में लागू किया गया है।
