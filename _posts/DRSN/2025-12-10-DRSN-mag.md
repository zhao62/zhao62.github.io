---
layout: post
title: "Deep Residual Shrinkage Network: Highly Noisy Data खातिर एगो Artificial Intelligence Method"
date: 2025-12-10
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network, Deep Residual Network के एगो सुधरल रूप (improved variant) हे। मोटामोटी, Deep Residual Shrinkage Network में Deep Residual Network, Attention mechanisms, और Soft thresholding functions के एक साथ मिलावल गेल हे।"
---

**Deep Residual Shrinkage Network, Deep Residual Network के एगो सुधरल रूप (improved variant) हे। मोटामोटी, Deep Residual Shrinkage Network में Deep Residual Network, Attention mechanisms, और Soft thresholding functions के एक साथ मिलावल गेल हे।**

**हमनी Deep Residual Shrinkage Network के काम करे के तरीका अइसन समझ सकही। सबसे पहले, नेटवर्क Attention mechanisms का यूज़ कर के unimportant features के पहचान कर हे। फिर, नेटवर्क Soft thresholding functions लगा के ई सब unimportant features के जीरो (zero) सेट कर दे हे। ओकर उल्टा, नेटवर्क important features के पहचाने हे और उनका बचा के रखे हे। ई प्रोसेस से Deep neural network के ताकत बढ़ जा हे। ई प्रोसेस नेटवर्क के noise वाला सिग्नल में से काम के features निकाले में मदद कर हे।**

## 1. Research के मकसद (Research Motivation)

**सबसे पहले, जब algorithm samples के classify कर हे, तब noise तो आना तय हे। ई noise के example हे Gaussian noise, pink noise, और Laplacian noise.** थोड़ा और खुल के बोली, त samples में अक्सर अइसन जानकारी होवे हे जेकर current classification task से कोई लेना-देना न हे। हमनी ई बेकार जानकारी के noise समझ सकही। ई noise classification के performance खराब कर सके हे। (**Soft thresholding** बहुत सारा signal denoising algorithms में एक जरूरी स्टेप हे।)

जैसे कि, मान ल रोड किनारे बात-चीत हो रहल हे। ऑडियो में गाड़ी के हॉर्न और चक्का के आवाज आ सके हे। हमनी ई signals पर speech recognition कर सकही। ई background sounds result पर असर डालते। **Deep learning** के नजर से देखल जाए, त **Deep neural network** के हॉर्न और चक्का वाला features के हटा देना चाहिए। अइसन करने से ई features speech recognition के result खराब न करते।

**दुसरा बात, अलग-अलग samples में noise के मात्रा अक्सर अलग-अलग होवे हे। ई अंतर एक ही dataset के अंदर भी देखे के मिले हे।** (ई चीज **Attention mechanisms** से काफी मिलता-जुलता हे। एगो image dataset के example ल। हर फोटो में target object अलग जगह पर हो सके हे। **Attention mechanisms** हर image में target object के सही जगह पर ध्यान लगा सके हे।)

Example खातिर, मान ल हमनी एक cat-and-dog classifier train कर रहल ही जिसमे 5 image पर "dog" label लगल हे। Image 1 में dog और mouse हो सके हे। Image 2 में dog और goose हो सके हे। Image 3 में dog और chicken हो सके हे। Image 4 में dog और donkey हो सके हे। Image 5 में dog और duck हो सके हे। Training के दौरान, बेकार के objects classifier के disturb करते। ई objects हे mice, geese, chickens, donkeys, और ducks। ई disturbance से classification accuracy कम हो जाए हे। अगर हमनी ई बेकार objects के पहचान लेही। तब, हमनी ई objects वाला features के हटा सकही। ई तरीका से, हमनी cat-and-dog classifier के accuracy बढ़ा सकही।

## 2. Soft Thresholding (Soft Thresholding)

**Soft thresholding ढेर सारा signal denoising algorithms के मेन स्टेप (core step) हे। अगर features के absolute values एक threshold से कम हे, त algorithm उ features के हटा दे हे। अगर features के absolute values threshold से ज्यादा हे, त algorithm उनका जीरो के तरफ shrink (सिकुड़ा) दे हे।** Researchers लोग नीचे लिखल formula से **Soft thresholding** यूज़ कर सके हत:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Soft thresholding** output के derivative input के हिसाब से ई हे:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

ऊपर वाला formula दिखावे हे कि **Soft thresholding** के derivative या त 1 हे या फिर 0 हे। ई गुण **ReLU activation function** जइसन हे। एही से, **Soft thresholding** deep learning algorithms में gradient vanishing और gradient exploding के खतरा कम कर सके हे।

**Soft thresholding function में, threshold सेट करते टाइम दो शर्त मानल जरूरी हे। पहला, threshold positive number होना चाहिए। दुसर, threshold input signal के maximum value से ज्यादा न होना चाहिए। नहीं त, output पूरा जीरो हो जाएत।**

**साथ ही, threshold के एक तीसरा शर्त भी मानल चाहिए। हर sample के पास अपन अलग independent threshold होना चाहिए जो ओकर noise content पर depend करे।**

काहेला कि, ढेर samples में noise के मात्रा अलग-अलग होवे हे। जैसे कि, एक ही dataset में Sample A में कम noise हो सके हे और Sample B में ज्यादा noise हो सके हे। ई case में, **Soft thresholding** करते वक्त Sample A के छोटा threshold चाहिए। Sample B के बड़ा threshold चाहिए। **Deep neural networks** में, भले ही ई features और thresholds के physical मतलब साफ़ न होवे। लेकिन, basic logic वही रहे हे। मतलब कि, हर sample के अपन independent threshold होना चाहिए। उ sample में केतना noise हे, वही threshold decide करे हे।

## 3. Attention Mechanism (Attention Mechanism)

Researchers लोग computer vision field में **Attention mechanisms** के आसानी से समझ सके हत। जानवर के visual system पूरा area के जल्दी से scan कर के target के पहचान सके हे। बाद में, visual system target object पर attention focus करे हे। ई action से system और ज्यादा details निकाल सके हे। साथ ही, system बेकार information के दबा दे हे। ज्यादा जानकारी खातिर, **Attention mechanisms** वाला literature पढ़।

**Squeeze-and-Excitation Network (SENet)** एक नया deep learning method हे जो **Attention mechanisms** यूज़ करे हे। अलग-अलग samples में, अलग-अलग feature channels classification task में अलग-अलग योगदान दे हे। **SENet** एक छोटा sub-network यूज़ कर के "Learn a set of weights" करे हे। फिर, **SENet** ई weights के उ channel के features से multiply कर दे हे ("Apply weighting to each feature channel")। ई operation हर channel के features के size adjust कर दे हे। हमनी ई process के अइसन समझ सकही कि अलग-अलग feature channels पर अलग-अलग level के attention लगावल जा रहल हे।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

ई तरीका में, हर sample के पास weights के अपन set होवे हे। मतलब, कोई भी दो sample के weights अलग-अलग होवे हे। **SENet** में, weights लावे के रास्ता हे: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Deep Attention Mechanism के साथ Soft Thresholding

**Deep Residual Shrinkage Network**, **SENet** sub-network के structure यूज़ करे हे। Network ई structure के यूज़ कर के **Deep attention mechanism** के under **Soft thresholding** करे हे। Sub-network (जो लाल डब्बा में दिखल गेल हे) "Learn a set of thresholds" करे हे। फिर, नेटवर्क ई thresholds के यूज़ कर के हर feature channel पर **Soft thresholding** apply करे हे।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

ई sub-network में, system सबसे पहले input feature map के सारा features के absolute values निकाले हे। फिर, system global average pooling और averaging कर के एक feature निकाले हे, जेकरा *A* बोलल जा हे। दुसर रास्ता (Identity path) में, system global average pooling के बाद feature map के एक छोटा fully connected network में डाल दे हे। ई fully connected network आखिरी layer के रूप में **Sigmoid function** यूज़ करे हे। ई function output के 0 और 1 के बीच में normalize कर दे हे। ई process से एक coefficient मिले हे, जेकरा *α* बोलल जा हे। हमनी final threshold के *α × A* लिख सकही। एही से, threshold दो नंबर के गुणा (product) हे। एक नंबर 0 और 1 के बीच हे। दुसर नंबर feature map के absolute values के average हे। **ई तरीका पक्का करे हे कि threshold positive रहे। ई तरीका ई भी पक्का करे हे कि threshold बहुत बड़ा न हो जाए।**

**और, अलग-अलग samples से अलग-अलग threshold निकाले हे। एही से, हमनी ई तरीका के एक special Attention mechanism समझ सकही। ई mechanism उ features के पहचाने हे जो current task के काम के न हे। ई mechanism दो convolutional layers के through ई features के 0 के करीब वाला value बना दे हे। फिर, ई mechanism Soft thresholding यूज़ कर के ई features के जीरो सेट कर दे हे। या फिर, ई mechanism उ features के पहचाने हे जो current task के काम के हे। ई mechanism दो convolutional layers के through ई features के 0 से दूर वाला value बना दे हे। आखिरी में, ई mechanism ई features के बचा ले हे (preserves)।**

लास्ट में, हमनी "Stack many basic modules" करही। हमनी convolutional layers, batch normalization, activation functions, global average pooling, और fully connected output layers भी जोड़ही। ई process से पूरा **Deep Residual Shrinkage Network** बन जा हे।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization की क्षमता (Generalization Capability)

**Deep Residual Shrinkage Network** feature learning के लिए एक general method हे। काहेला कि, ढेर feature learning tasks में samples में अक्सर noise होवे हे। Samples में बेकार information भी होवे हे। ई noise और बेकार information feature learning के performance पर असर डाल सके हे। जैसे कि:

Image classification के देख। एक फोटो में एक साथ ढेर सारा दुसरा objects भी हो सके हे। हमनी ई objects के "noise" समझ सकही। **Deep Residual Shrinkage Network** शायद **Attention mechanism** यूज़ कर सके। नेटवर्क ई "noise" पर ध्यान दे हे। फिर, नेटवर्क **Soft thresholding** लगा के ई "noise" वाला features के जीरो सेट कर दे हे। ई action से image classification accuracy बढ़ सके हे।

Speech recognition के देख। खास कर के शोर-शराबा वाला जगह पर, जैसे रोड किनारे या factory workshop में बात करते टाइम। **Deep Residual Shrinkage Network** speech recognition accuracy बढ़ा सके हे। या कम से कम, नेटवर्क एक तरीका देवे हे। ई तरीका speech recognition accuracy बढ़ावे के काबिलियत रखे हे।

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## असर (Academic Impact)

ई paper के Google Scholar पर 1,400 से ज्यादा citations मिलल हे।

अधूरा statistics के हिसाब से, researchers लोग 1,000 से ज्यादा publications/studies में Deep Residual Shrinkage Network (DRSN) के apply करके देखले हत। ई सब application ढेर सारा field में फैलल हे। ई fields में mechanical engineering, electrical power, vision, healthcare, speech, text, radar, और remote sensing शामिल हे।
