---
layout: post
title: "Kusetjentiswa kwe- Deep Residual Shrinkage Network: I- Artificial Intelligence Method ye- Highly Noisy Data"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-12
tags: [Deep Learning, AI]
mathjax: true
description: "I- Deep Residual Shrinkage Network yindlela le- improved ye- Deep Residual Network. Kahle kahle, i- Deep Residual Shrinkage Network ihlanganisa tintfo letintsatfu: i- Deep Residual Network, i- attention mechanism, kanye ne- soft thresholding function."
---

**I- Deep Residual Shrinkage Network yindlela le- improved ye- Deep Residual Network. Kahle kahle, i- Deep Residual Shrinkage Network ihlanganisa tintfo letintsatfu: i- Deep Residual Network, i- attention mechanism, kanye ne- soft thresholding function.**

**Singayicondza indlela i- Deep Residual Shrinkage Network lesebenta ngayo ngalendlela lelandzelako. Kwekucala, le- network isebentisa i- attention mechanism kuncuma ema- unimportant features. Bese, le- network isebentisa i- soft thresholding function kuwenta abe ngu- zero lama- unimportant features. Ngakulokunye, le- network ibona ema- important features bese iyawagcina lama- important features. Lenchubo iqinisa emandla e- deep neural network. Loku kusita i- network kutsi ikhone ku- extract ema- useful features kuma- signals lanalo i- noise.**

## 1. Sizatfu sekwenta lolucwaningo (Research Motivation)

**Kwekucala, i- noise yintfo lengekhe siyivikele uma i- algorithm yenta i- classification yema- samples. Tibonelo talolu- noise tifaka ekhatsi i- Gaussian noise, i- pink noise, kanye ne- Laplacian noise.** Ngalokubanti, ema- samples avamise kuba ne- information lengahambisani nalo- task we- classification lesiwentako. Singayitsatsa le- information lengahambisani njenge- noise. Le- noise inganciphisa i- performance ye- classification. (I- Soft thresholding sinyatselo lesibalulekile kuma- algorithms lamanyenti we- signal denoising.)

Asibuke nasi sibonelo: cabanga ngenkhulumo leyenteka eceleni kwemgwaco. I- audio ingahle ibe nemsindvo wemahambukati etimoto kanye nemasondo. Kungenteka sifune kwenta i- speech recognition kulo- signal. Lom- background sound uzoba nemthelela kulemiphumela. Uma sibuka ngeliso le- deep learning, i- deep neural network kufanele i- eliminate ema- features lahambisana nemahambukati kanye nemasondo. Loku ku- eliminate kuvikela lama- features kutsi angaphazamisi imiphumela ye- speech recognition.

**Kwesibili, linani le- noise livame kuhluka emkhatsini wema- samples. Lolushintjo lwenteka ngisho nangekhatsi kwe- dataset yinye.** (Lolushintjo lufana kakhulu ne- attention mechanism. Asitsatfu i- image dataset njengesibonelo. Indzawo lapho kune- target object ingahluka kuleto- images. I- Attention mechanism ingakhona ku- focus kulendzawo le- specific ye- target object ku- image ngayinye.)

Sibonelo, cabanga u- traina i- classifier yenja nelikati (cat-and-dog classifier) usebentisa titfombe letisihlanu letilebulelwe njenge "dog." I- Image 1 ingaba nenja kanye neligundvwane. I- Image 2 ingaba nenja kanye nelihansi. I- Image 3 ingaba nenja kanye nenkhukhu. I- Image 4 ingaba nenja kanye nembongolo. I- Image 5 ingaba nenja kanye nelidada. Ngesikhatsi se- training, lama- object langahambisani atawuphazamisa le- classifier. Lama- object afaka ekhatsi emagundvwane, emahansi, tinkhukhu, timbongolo, kanye nemadada. Lokungenelela kubangela kwehla kwe- classification accuracy. Kodvwa, uma singakhona ku- identify lama- object langahambisani, singakhona ku- eliminate ema- features lahambisana nalama- object. Ngalendlela, singayinyusa i- accuracy yale- classifier yenja nelikati.

## 2. I- Soft Thresholding (Soft Thresholding)

**I- Soft thresholding sinyatselo lesisisekelo kuma- algorithms lamanyenti we- signal denoising. Le- algorithm i- eliminate ema- features uma i- absolute value yala- features iphasi kunalokutsite lokubitwa ngekutsi yi- threshold. Le- algorithm yenta i- shrink yema- features aye ngaku- zero uma i- absolute value yala- features iphetulu kunale- threshold.** Ebachwepheshe bangayenta i- soft thresholding basebentisa le- formula lelandzelako:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

I- derivative ye- output ye- soft thresholding mayelana ne- input iba nguloku:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Le- formula lengenhla ikhombisa kutsi i- derivative ye- soft thresholding inguba 1 noma ibe ngu 0. Le- property iyafana ncimishi ne- property ye- ReLU activation function. Ngako-ke, i- soft thresholding inganciphisa bungoti be- gradient vanishing kanye ne- gradient exploding kuma- algorithms we- deep learning.

**Kule- soft thresholding function, kusetjwa kwe- threshold kufanele kuhlangabezane nemibandzela lemibili. Kwekucala, i- threshold kufanele ibe yi- positive number. Kwesibili, i- threshold akukafaneli yengce i- maximum value ye- input signal. Uma kungenjalo, i- output itawuba ngu- zero yonkhe.**

**Ngetulu kwaloko, kuncono kutsi i- threshold ihlangabezane nembandzela wesitsatfu. I- sample ngayinye kufanele ibe ne- threshold yayo le- independent kuye ngekutsi inakanani i- noise.**

Sizatfu saloku kutsi i- content ye- noise ivame kuhluka emkhatsini wema- samples. Sibonelo, i- Sample A ingaba ne- noise lencane kantsi i- Sample B ibe ne- noise lenyenti kule- dataset yinye. Kulesimo, i- Sample A kufanele isebentise i- threshold lencane ngesikhatsi se- soft thresholding. I- Sample B kufanele isebentise i- threshold lenkhulu. Lama- features kanye nema- thresholds alahlekelwa tinhla tawo te- physical definition kule- deep neural network. Kodvwa, i- logic lesisekelo ihlala injalo. Ngalamanye emagama, i- sample ngayinye kufanele ibe ne- independent threshold. I- content ye- noise le- specific incuma le- threshold.

## 3. I- Attention Mechanism (Attention Mechanism)

Ebachwepheshe bangayicondza melula i- attention mechanism ku- field ye- computer vision. Ema- visual system etilwane angakhona kuhlukanisa ema- target ngekutsi a- scan yonkhe indzawo ngekushesha. Emvakwaloko, lama- visual system enta i- focus attention kule- target object. Lesento sivumela lama- system kutsi a- extract imininingwane leminyenti. Ngesikhatsi lesifanako, lama- system a- suppress i- irrelevant information. Kute utfole imininingwane le- specific, sicela ubuke i- literature mayelana ne- attention mechanism.

I- Squeeze-and-Excitation Network (SENet) imelela i- deep learning method lensha lesebentisa i- attention mechanism. Kuwo onkhe ema- samples lahlukene, ema- feature channels lahlukene anikela ngalokwehlukile kulo- task we- classification. I- SENet isebentisa i- sub-network lencane kutfola lokubitwa ngekutsi yi- **Learn a set of weights**. Bese, i- SENet iphindzaphindza lama- weights ngema- features ala- channels lafanele. Loku- operation ku- adjust i- magnitude yema- features ku- channel ngayinye. Singabuka lenchubo njenge- **Apply weighting to each feature channel** ngema- levels lahlukene.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Kulendlela, i- sample ngayinyeine- set yema- weights le- independent. Ngalamanye emagama, ema- weights wanoma ngumaphi ema- samples lamabili ahlukile. Ku- SENet, i- path le- specific yekutfola ema- weights yi: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. I- Soft Thresholding lenayo i- Deep Attention Mechanism

I- Deep Residual Shrinkage Network isebentisa i- structure se- SENet sub-network. Le- network isebentisa le- structure kwenta i- soft thresholding ngephansi kwe- deep attention mechanism. Le- sub-network (lekhonjiswe kuleli-bhokisi lelibovu) yenta i- **Learn a set of thresholds**. Bese, le- network yenta i- soft thresholding kuye nge- feature channel ngayinye isebentisa lama- thresholds.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Kule- sub-network, i- system icatfula ema- absolute values awo onkhe ema- features kule- input feature map. Bese, i- system yenta i- global average pooling kanye ne- averaging kutfola i- feature, lebitwa ngekutsi ngu- A. Kulenye i- path, i- system ifaka i- feature map ku- fully connected network lencane ngemuva kwe- global average pooling. Le- fully connected network isebentisa i- Sigmoid function njenge- layer yekugcina. Le- function yenta i- normalize ye- output ibe semkhatsini wa 0 na 1. Lenchubo ikhicita i- coefficient, lebitwa ngekutsi ngu- α. Singabhala i- final threshold njenge α × A. Ngako-ke, i- threshold ngumphumela wekuphindzaphindza tinombolo letimbili. Inombolo yinye isemkhatsini wa 0 na 1. Lenye inombolo yi- average yema- absolute values we- feature map. **Le- method icinisekisa kutsi i- threshold yi- positive. Le- method iphindze icinisekise kutsi i- threshold ayibi nkhulu kakhulu.**

**Ngetulu kwaloko, ema- samples lahlukene akhicita ema- thresholds lahlukene. Ngako-ke, singayicondza le- method njenge- attention mechanism le- specialized. Le- mechanism ibona ema- features langahambisani nalo- task wanyalo. Le- mechanism ishintja lama- features awente ema- values lasondzelene na zero ngekusebentisa ema- convolutional layers lamabili. Bese, le- mechanism yenta lama- features abe ngu- zero isebentisa i- soft thresholding. Ngalokunye, le- mechanism ibona ema- features lahambisana nalo- task wanyalo. Le- mechanism ishintja lama- features awente ema- values lakhashane na zero ngekusebentisa ema- convolutional layers lamabili. Ekugcineni, le- mechanism iwagcina (preserve) lama- features.**

Ekugcineni, senta i- **Stack many basic modules** ngelinani lelitsite. Siphindze sifake ema- convolutional layers, i- batch normalization, ema- activation functions, i- global average pooling, kanye ne- fully connected output layer. Lenchubo yakha i- Deep Residual Shrinkage Network lephelele.

*Note: Ku- figure yemudvwebo, uzobona imigca lebhalwe kutsi **Identity path** kanye ne- **Weighting**.*

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. I- Generalization Capability

I- Deep Residual Shrinkage Network yi- method le- general ye- feature learning. Sizatfu kutsi ema- samples avamise kuba ne- noise kuma- task lamanyenti we- feature learning. Ema- samples aphindze abe ne- irrelevant information. Le- noise kanye nale- irrelevant information kungaba nemthelela kwi- performance ye- feature learning. Sibonelo:

Cabanga nge- image classification. I- image ingaba naletinye tintfo letinyenti ngesikhatsi sinye. Singaticondza letintfo njenge "noise." I- Deep Residual Shrinkage Network ingakhona kusebentisa i- attention mechanism. Le- network iyayibona le "noise." Bese, le- network isebentisa i- soft thresholding, ukwenta ema- features lahambisana nale "noise" abe ngu- zero. Lesento singatfutfukisa i- accuracy ye- image classification.

Cabanga nge- speech recognition. Ikakhulukati, cabanga ngema- environments lanemsindvo lomunyenti, njengekukhuluma eceleni kwemgwaco noma ngaphakatsi kwe- factory workshop. I- Deep Residual Shrinkage Network ingayitfutfukisa i- accuracy ye- speech recognition. Noma okungenani, le- network iniketa i- methodology. Le- methodology inemandla ekutfutfukisa i- accuracy ye- speech recognition.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Umthelela kutesayensi (Academic Impact)

Leli-phepha (paper) selitfole ma- citations langetulu kwa 1,400 ku- Google Scholar.

Kuye ngetibalo letisengakapheleli, bachwepheshe sebasebentise i- Deep Residual Shrinkage Network (DRSN) kuma- publications noma research langetulu kwa 1,000. Lama- applications ahlanganisa imikhakha leminyenti. Lemikhakha ifaka ekhatsi i- mechanical engineering, i- electrical power, i- vision, i- healthcare, i- speech, i- text, i- radar, kanye ne- remote sensing.
