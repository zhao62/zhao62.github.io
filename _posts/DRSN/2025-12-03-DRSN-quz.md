---
layout: post
title: "Deep Residual Shrinkage Network: Huk Artificial Intelligence Method Highly Noisy Data-paq"
date: 2025-12-03
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network nisqaqa huk allinchasqa Deep Residual Network niraqmi. Chiqaptapuniqa, kayqa huñunmi Deep Residual Networkta, attention mechanismsta, hinallataq soft thresholding functionsta ima."
---

**Deep Residual Shrinkage Network nisqaqa huk allinchasqa Deep Residual Network niraqmi. Chiqaptapuniqa, kayqa huñunmi Deep Residual Networkta, attention mechanismsta, hinallataq soft thresholding functionsta ima.**

**Huk partemanta qhawaspaqa, Deep Residual Shrinkage Networkpa llamk'ayninqa khaynatam entiendekunman: attention mechanismsta llamk'achin mana ancha allin featuresta riqsinanpaq, hinaspa soft thresholding functionswan ch'usaqman (zero) tukun; hukladumantaqa, allin featuresta riqsispa waqaychan. Kay ruwaymi deep neural networkpa atiyninta kallpachan allin featuresta hurqunanpaq noiseyuq signalsmanta.**

## 1. Research Motivation

**Ñawpaqta, samplesta classify ruwachkaspaqa, noiseqa (imaynan Gaussian noise, pink noise, Laplacian noise ima) manam karunchakuyta atikunchu.** Aswan hatun rimaypi, **samples**qa sapa kutim **current classification task**wan mana tupaq willakuyta (information) apamun, chaypas **noise** hinallataqmi riqsisqa. Kay **noise**qa **classification performance**ta waqllichinman. (Soft thresholdingqa ancha allinmi achka **signal denoising algorithms**pi).

Huk ejemplo: callekunapi rimachkaptinchikqa, **audio**piqa autoq **horns**nin utaq ruedanpa qapariyninmi chaqrukunman. Kay **signals**pi **speech recognition**ta ruwaptinchikqa, **results**qa manam allinchu kanqa kay **background sounds** rayku. **Deep learning**manta qhawaspaqa, kay **horns**wan ruedakunaq **features**nintaqa **deep neural network** ukhupi chinkachina tiyan, **speech recognition results**ta mana waqllichinanpaq.

**Iskaykaq, kikin dataset ukhupipas, noiseqa sapa samplepi hukniraqmi.** (Kayqa rikch'akunmi **attention mechanisms**man; huk **image dataset**ta ejemplo hina qhawaspaqa, **target object**pa maypi kasqanqa sapa **image**pi hukniraqmi kanman; **attention mechanisms**qa atinmi **focus** ruwayta **target object**pa locationninman sapa **image**pi).

Ejemplopaq, **cat-and-dog classifier**ta trainichkaspa, pichqa **images**ta "dog" labelniyuqta qhawarisun. Ñawpaq kaq **image**piqa kanmanmi huk allqu huk ukuchawan, iskay kaqpi huk allqu huk wallatawan, kimsa kaqpi huk allqu huk wallpawan, tawa kaqpi huk allqu huk asnowan, pichqa kaqpitaq huk allqu huk patowan. Trainichkaspaqa, **classifier**qa sasachakunqapunim kay mana allin objetokunawan (ukuchakuna, wallatakuna, wallpakuna, asnokuna, patokuna), chaymi **classification accuracy**ta urmachin. Sichus kay mana allin objetokunata riqsispa (ukuchakuna, wallatakuna, wallpakuna, asnokuna, patokuna) paykunaq **features**ninta chinkachisunman chayqa, **cat-and-dog classifier**pa accuracyninqa aswan allinmi kanman.

## 2. Soft Thresholding

**Soft thresholdingqa ancha allinmi achka signal denoising algorithms ukhupi. Chayqa chinkachinmi featuresta mayqinchus absolute valuesnin aswan huch'uy huk thresholdmanta, hinallataq shrink ruwan featuresta mayqinchus absolute valuesnin aswan hatun thresholdmanta zero ladoman.** Kayqa ruwakunmanmi kay formulawan:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Soft thresholding output**pa **derivative**nin **input**man qhawaspaqa kayhinam:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Hawa formulapi rikukusqanman hina, **soft thresholding**pa **derivative**ninqa 1 utaq 0 kanman. Kayqa **ReLU activation function**wan kaqllataqmi. Chayrayku, **soft thresholding**qa yanapanmi **deep learning algorithms**ta mana **gradient vanishing** nitaq **gradient exploding** sasachakuykunapi urmananpaq.

**Soft thresholding functionpiqa, threshold churayqa iskayta hunt'ananmi: ñawpaqta, thresholdqa positive number kanan; iskaykaq, thresholdqa manam input signalpa maximum valueninta yallinanchu, mana chayqa outputqa llapanmi zero kanqa.**

**Chaymantapas, aswan allinmi kanman threshold kay kimsa kaq conditionta hunt'aptin: sapa samplemi kikin independent thresholdniyuq kanan, hayk'a noise content kasqanman hina.**

Imaraykuchus, **noise content**qa sapa **sample**pi hukniraqmi. Ejemplopaq, kikin **dataset**piqa **Sample A** pisi **noiseyuq** kanman, **Sample B**taq aswan achka **noiseyuq** kanman. Chayna kaptinqa, **signal denoising algorithm**pi **soft thresholding**ta ruwaspaqa, **Sample A**qa aswan huch'uy **threshold**ta llamk'achinan, **Sample B**taq aswan hatun **threshold**ta llamk'achinan. **Deep neural networks**pi kay **features**wan **thresholds** explicit physical definitionsninta chinkachiptinkupas, ukhupi logicankuqa kaqllam. Huk rimaypi, sapa **sample**mi kikin **independent threshold**niyuq kanan, hayk'a **noise** kasqanman hina.

## 3. Attention Mechanism

**Attention mechanisms**qa facilmi entiendey **computer vision** fieldpi. Animalpa ñawinqa (visual systems) atinmi targetsta riqsiyta llapan areata utqaylla qhawaspa, chaymantataqmi **attention**ta churan **target object**man aswan details hurqunanpaq, mana allin informationta saqispa. Aswan yachayta munaspaqa, **attention mechanisms**manta qillqasqakunata qhaway.

**Squeeze-and-Excitation Network (SENet)**qa huk musuq **deep learning method**mi **attention mechanisms**ta llamk'achiq. Hukniraq **samples**pi, sapa **feature channel**pa contributionnin **classification task**piqa hukniraqmi. **SENet**qa huk huch'uy **sub-network**ta llamk'achin huk **set of weights**ta tarinanpaq, chaymantataq kay **weights**ta mirachin sapa **channel**pa **features**ninwan, chay **features**pa magnitudninta adjustananpaq. Kay ruwayqa khaynatam qhawakunman: **Apply weighting to each feature channel** (hukniraq levels of attentionta churaspa sapa feature channelman).

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Kay ruwaypiqa, sapa **sample**mi kikin independent **set of weights**niyuq. Huk rimaypi, iskay ima **samples**pa **weights**ninqa hukniraqmi. **SENet**piqa, **weights** tarina ñanqa kaymi: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

**Deep Residual Shrinkage Network**qa hawapi rimusqanchik **SENet sub-network structure**manta yachaqakun (inspiration) **soft thresholding**ta **deep attention mechanism**pi ruwananpaq. **Sub-network**wan (puka cajapi rikukuq), huk **Learn a set of thresholds**ta yachakunman **soft thresholding**ta sapa **feature channel**man churananpaq.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Kay **sub-network**piqa, ñawpaqta llapan **features**pa **absolute values**nintam yupan **input feature map**pi. Chaymanta, **global average pooling**wan **averaging**wan, huk **feature**ta tarin, chaytaq A sutiwan riqsisqa. Huknin **Identity path**pi, **global average pooling** pasasqa **feature map**qa huk huch'uy **fully connected network**man yaykun. Kay **fully connected network**qa **Sigmoid function**ta llamk'achin final layer hina, **output**ta 0 mantawan 1 mantawan chawpipi kananpaq, chaymanta huk coefficient $\alpha$ nisqata qun. Final **threshold**qa $\alpha \times A$ nisqawanmi riqsisqa. Chayrayku, **threshold**qa kanqa: huk yupay (0 to 1) mirachisqa **feature map**pa **absolute values**ninpa averagenwan. **Kay methodqa manam threshold positive kanallanpaqchu yanapan, aswanpas mana nishuta hatun kananpaq.**

**Chaymantapas, hukniraq samplesqa hukniraq thresholdstami qun. Chayrayku, kayqa huk special attention mechanism hinam entiendekunman: riqsinmi features mana current taskwan tupaqta, hinaspa iskay convolutional layerswan kay featuresman chayan values cercaman 0, chaymantataq soft thresholdingwan zeroman tukun; utaq, riqsinmi features current taskwan tupaqta, hinaspa iskay convolutional layerswan kay featuresman chayan values karuman 0, hinaspa waqaychan.**

Tukuyninpi, **Stack many basic modules** ruwaspa, **convolutional layers**wan, **batch normalization**wan, **activation functions**wan, **global average pooling**wan, hinallataq **fully connected output layers**wan ima, hunt'asqa **Deep Residual Shrinkage Network**mi ruwakun.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

**Deep Residual Shrinkage Network**qa, chiqaptapuni, huk general **feature learning method**mi. Imaraykuchus achka **feature learning tasks**piqa, **samples**qa imaymanatam **noise**ta utaq irrelevant informationta apamun. Kay **noise**wan irrelevant informationqa **feature learning**pa performancenta waqllichinman. Ejemplopaq:

**Image classification**pi, sichus huk **image** achka hukniraq objetosta apamun chayqa, kay objetosqa "**noise**" hina riqsisqam. **Deep Residual Shrinkage Network**qa atinmanmi **attention mechanism**ta llamk'achiyta kay "**noise**" riqsinanpaq, chaymantataq **soft thresholding**wan kay "**noise**"pa **features**ninta zeroman churananpaq, chaywanmi **image classification accuracy**ta allinchanman.

**Speech recognition**pi, aswantaqa **noisy environments**pi (imaynan callekunapi utaq factory workshopspi rimay), **Deep Residual Shrinkage Network**qa **speech recognition accuracy**ta allinchanman, utaq allin ñanta (methodology) qunman **speech recognition accuracy**ta allinchananpaq.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Kay paperqa 1400 masnin citationsniyuqmi Google Scholarpi.

Mana hunt'asqa statisticasman hina, **Deep Residual Shrinkage Network (DRSN)**qa achka kutita llamk'achisqa utaq allinchasqa karqan 1000 masnin publications/studies ukhupi, imaymana fieldspi: mechanical engineering, electrical power, vision, healthcare, speech, text, radar, remote sensing ima.
