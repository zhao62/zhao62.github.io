---
layout: post
title: "Deep Residual Shrinkage Network: அதிக Noise உள்ள தரவுகளுக்கான ஒரு Artificial Intelligence முறை"
date: 2025-11-28
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network (DRSN) என்பது Deep Residual Network-ன் ஒரு மேம்படுத்தப்பட்ட வடிவமாகும். இது அடிப்படையில் Deep Residual Network, Attention Mechanism மற்றும் Soft Thresholding Function ஆகியவற்றின் ஒருங்கிணைப்பு (integration) ஆகும்.**

**ஒரு குறிப்பிட்ட அளவில், Deep Residual Shrinkage Network-ன் செயல்படும் விதத்தை பின்வருமாறு புரிந்து கொள்ளலாம்: இது Attention Mechanism மூலம் முக்கியமற்ற அம்சங்களை (unimportant features) கண்டறிந்து, Soft Thresholding Function மூலம் அவற்றை பூஜ்யமாக்குகிறது (set to zero); அல்லது, முக்கியமான அம்சங்களைக் கண்டறிந்து அவற்றைத் தக்கவைத்துக் கொள்கிறது. இதன் மூலம், Noise (இரைச்சல்) நிறைந்த சிக்னல்களில் இருந்து பயனுள்ள அம்சங்களை (useful features) பிரித்தெடுக்கும் Deep Neural Network-ன் திறனை இது அதிகரிக்கிறது.**

## 1. Research Motivation (ஆராய்ச்சி நோக்கம்)

**முதலாவதாக, மாதிரிகளை (samples) வகைப்படுத்தும் போது (classifying), அவற்றில் Gaussian Noise, Pink Noise, Laplacian Noise போன்ற Noise-கள் இருப்பது தவிர்க்க முடியாதது.** பரந்த அளவில் கூறினால், தற்போதைய Classification பணிக்குத் தொடர்பில்லாத தகவல்களும் மாதிரிகளில் இருக்கலாம், இவற்றையும் Noise என்றே கருதலாம். இந்த Noise-கள் Classification முடிவுகளை மோசமாக பாதிக்கலாம். (Soft Thresholding என்பது பல Signal Denoising அல்காரிதங்களில் ஒரு முக்கிய படியாகும்).

உதாரணமாக, சாலை ஓரத்தில் பேசும்போது, நமது குரலோடு வாகனங்களின் ஹாரன் சத்தம் மற்றும் சக்கரங்களின் சத்தம் கலந்திருக்கலாம். இந்த சிக்னல்களை வைத்து **Speech Recognition** செய்யும் போது, முடிவுகள் அந்த பின்னணி சத்தங்களால் பாதிக்கப்படும். **Deep Learning** கண்ணோட்டத்தில் பார்த்தால், இந்த ஹாரன் மற்றும் சக்கர சத்தங்களுக்குரிய Feature-கள், Deep Neural Network-க்குள் நீக்கப்பட வேண்டும். அப்போதுதான் Speech Recognition-ன் துல்லியம் பாதிக்கப்படாமல் இருக்கும்.

**இரண்டாவதாக, ஒரே Dataset-ல் இருந்தாலும், ஒவ்வொரு Sample-லும் உள்ள Noise-ன் அளவு பெரும்பாலும் மாறுபடும்.** (இது Attention Mechanism-உடன் ஒத்துப்போகிறது; ஒரு Image Dataset-ஐ உதாரணமாகக் கொண்டால், ஒவ்வொரு படத்திலும் நாம் தேடும் பொருள் (object) இருக்கும் இடம் மாறுபடலாம்; Attention Mechanism ஒவ்வொரு படத்திலும் அந்தப் பொருள் இருக்கும் இடத்தை சரியாகக் கவனிக்க உதவும்).

உதாரணமாக, ஒரு பூனை-நாய் Classifier-ஐ (Cat-Dog Classifier) நாம் train செய்வதாக வைத்துக்கொள்வோம். "நாய்" என்று லேபிள் செய்யப்பட்ட 5 படங்களை எடுத்துக்கொள்வோம். முதல் படத்தில் நாயுடன் ஒரு எலி இருக்கலாம், இரண்டாவது படத்தில் நாயுடன் ஒரு வாத்து, மூன்றாவது படத்தில் கோழி, நான்காவது படத்தில் கழுதை, ஐந்தாவது படத்தில் வாத்து என்று இருக்கலாம். நாம் நாய்-பூனை Classifier-ஐ train செய்யும்போது, எலி, வாத்து, கோழி, கழுதை போன்ற தொடர்பில்லாத பொருட்களால் (irrelevant objects) குறிக்கீடுகள் ஏற்பட்டு, Classification துல்லியம் குறைய வாய்ப்புள்ளது. இந்த தொடர்பில்லாத எலி, வாத்து, கோழி போன்றவற்றை நாம் கவனித்து, அவற்றின் Feature-களை நீக்க முடிந்தால், நாய்-பூனை Classifier-ன் துல்லியத்தை அதிகரிக்க முடியும்.

## 2. Soft Thresholding

**Soft Thresholding என்பது பல Signal Denoising அல்காரிதங்களின் முக்கிய படியாகும். இது ஒரு குறிப்பிட்ட Threshold-க்கு கீழே உள்ள Feature-களின் மதிப்பை (absolute value) நீக்குகிறது மற்றும் அந்த Threshold-க்கு மேலே உள்ள Feature-களை பூஜ்யத்தை (Zero) நோக்கி சுருக்குகிறது (shrinks).** இதை பின்வரும் சூத்திரம் மூலம் செயல்படுத்தலாம்:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Input-ஐப் பொறுத்து Soft Thresholding வெளியீட்டின் Derivative (வகைக்கெழு) பின்வருமாறு:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

மேலே உள்ளவற்றிலிருந்து, Soft Thresholding-ன் Derivative 1 அல்லது 0 ஆக மட்டுமே இருக்கும் என்பது தெளிவாகிறது. இந்த பண்பு **ReLU Activation Function**-ஐப் போன்றது. எனவே, **Gradient Vanishing** மற்றும் **Gradient Exploding** போன்ற பிரச்சனைகளை Deep Learning அல்காரிதங்கள் சந்திக்கும் அபாயத்தைக் குறைக்க Soft Thresholding உதவுகிறது.

**Soft Thresholding Function-ல், Threshold-ஐ அமைப்பது இரண்டு நிபந்தனைகளுக்கு உட்பட்டது: முதல், Threshold ஒரு பாசிட்டிவ் (positive) எண்ணாக இருக்க வேண்டும்; இரண்டு, இது உள்ளீட்டு சிக்னலின் (input signal) அதிகபட்ச மதிப்பை விட அதிகமாக இருக்கக்கூடாது, இல்லையெனில் வெளியீடு முழுவதும் பூஜ்யமாகிவிடும்.**

**அதே நேரத்தில், Threshold மூன்றாவது நிபந்தனையையும் பூர்த்தி செய்வது சிறந்தது: ஒவ்வொரு Sample-லும் உள்ள Noise-ன் அளவிற்கு ஏற்ப, அதற்கு தனித்தனியான (independent) Threshold இருக்க வேண்டும்.**

ஏனென்றால், பல மாதிரிகளில் (samples) Noise-ன் அளவு மாறுபடும். உதாரணமாக, ஒரே Dataset-க்குள் Sample A-ல் குறைவான Noise-ம், Sample B-ல் அதிகமான Noise-ம் இருப்பது வழக்கம். அப்படி இருக்கும்போது, Denoising அல்காரிதத்தில் Soft Thresholding செய்யும் போது, Sample A-க்கு சிறிய Threshold-ஐயும், Sample B-க்கு பெரிய Threshold-ஐயும் பயன்படுத்த வேண்டும். Deep Neural Network-ல் இந்த Feature-களுக்கும் Threshold-களுக்கும் நேரடியான பௌதீக அர்த்தம் (physical meaning) இல்லாவிட்டாலும், அடிப்படை தர்க்கம் ஒன்றுதான். அதாவது, ஒவ்வொரு Sample-லும் அதன் Noise அளவிற்கு ஏற்ப தனித்துவமான Threshold-ஐ கொண்டிருக்க வேண்டும்.

## 3. Attention Mechanism

**Attention Mechanism**-ஐ Computer Vision துறையில் எளிதாகப் புரிந்து கொள்ளலாம். விலங்குகளின் பார்வை அமைப்பு, ஒரு முழுப் பகுதியையும் விரைவாக ஸ்கேன் செய்து, இலக்கு பொருளைக் (target object) கண்டறிந்து, அதன் மீது கவனத்தைச் செலுத்தும். இதன் மூலம் தேவையற்ற தகவல்களைத் தவிர்த்து, இலக்கு பொருளின் கூடுதல் விவரங்களைப் பெற முடியும்.

**Squeeze-and-Excitation Network (SENet)** என்பது Attention Mechanism-ஐ பயன்படுத்தும் ஒரு நவீன Deep Learning முறையாகும். வெவ்வேறு Sample-களில், வெவ்வேறு Feature Channel-கள் Classification பணிக்கு அளிக்கும் பங்களிப்பு மாறுபடும். SENet ஒரு சிறிய Sub-network-ஐப் பயன்படுத்தி ஒரு செட் எடைகளை (weights) பெறுகிறது. பின்னர் இந்த எடைகளை அந்தந்த Channel-ன் Feature-களுடன் பெருக்கி, ஒவ்வொரு Channel-ன் முக்கியத்துவத்தையும் சரிசெய்கிறது. இதை வெவ்வேறு Feature Channel-கள் மீது வெவ்வேறு அளவிலான கவனத்தைச் (Attention) செலுத்துவதாகக் கருதலாம்.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-ta/SENET_ta_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

இந்த முறையில், ஒவ்வொரு Sample-க்கும் அதற்கென தனித்துவமான எடைகள் (weights) இருக்கும். அதாவது, ஏதேனும் இரண்டு Sample-களின் எடைகள் ஒன்றாக இருக்காது. SENet-ல், எடைகளைப் பெறுவதற்கான பாதை: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function" ஆகும்.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-ta/SENET_ta_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Deep Attention Mechanism-ன் கீழ் Soft Thresholding

**Deep Residual Shrinkage Network**, மேலே குறிப்பிட்ட SENet-ன் Sub-network கட்டமைப்பைப் பின்பற்றி, **Deep Attention Mechanism** மூலம் Soft Thresholding-ஐ செயல்படுத்துகிறது. சிவப்பு பெட்டிக்குள் (red box) உள்ள Sub-network மூலம், ஒவ்வொரு Feature Channel-க்கும் தேவையான Threshold-களை அது கற்றுக்கொள்கிறது.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-ta/DRSN_ta_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

இந்த Sub-network-ல், முதலில் Input Feature Map-ன் அனைத்து Feature-களுக்கும் Absolute Value (தனிமதிப்பு) எடுக்கப்படுகிறது. பிறகு **Global Average Pooling** மூலம் சராசரி செய்யப்பட்டு ஒரு Feature பெறப்படுகிறது, இதை A எனக் குறிப்போம். மற்றொரு பாதையில், Global Average Pooling-க்குப் பிறகு கிடைக்கும் Feature Map, ஒரு சிறிய Fully Connected Network-க்குள் செலுத்தப்படுகிறது. இந்த Network-ன் கடைசி அடுக்கில் **Sigmoid Function** பயன்படுத்தப்பட்டு, வெளியீடு 0 மற்றும் 1-க்கு இடையில் இருக்குமாறு மாற்றப்படுகிறது. இதை α (alpha) எனக் குறிப்போம். இறுதி Threshold-ஐ **α × A** எனக் குறிப்பிடலாம். எனவே, Threshold என்பது 0 முதல் 1 வரை உள்ள ஒரு எண் மற்றும் Feature Map-ன் சராசரி Absolute Value ஆகியவற்றின் பெருக்கற்பலன் ஆகும். **இந்த முறை, Threshold பாசிட்டிவ் ஆக இருப்பதை உறுதி செய்வது மட்டுமல்லாமல், அது மிக அதிகமாக இல்லாமலும் இருப்பதை உறுதி செய்கிறது.**

**மேலும், வெவ்வேறு Sample-களுக்கு வெவ்வேறு Threshold-கள் கிடைக்கும். எனவே, ஒரு குறிப்பிட்ட அளவில், இதை ஒரு சிறப்பு Attention Mechanism-ஆக புரிந்து கொள்ளலாம்: இது தற்போதைய பணிக்குத் தொடர்பில்லாத Feature-களைக் கவனித்து, இரண்டு Convolutional Layer-கள் மூலம் அவற்றை 0-க்கு நெருக்கமான மதிப்புகளாக மாற்றி, பின்பு Soft Thresholding மூலம் அவற்றை முழுமையாக பூஜ்யமாக்குகிறது. அல்லது, முக்கியமான Feature-களைக் கவனித்து, அவற்றை 0-ஐ விட்டு விலகி இருக்குமாறு செய்து தக்கவைத்துக் கொள்கிறது.**

இறுதியாக, அடிப்படைத் தொகுதிகள் (basic modules), Convolutional Layers, Batch Normalization, Activation Functions, Global Average Pooling மற்றும் Fully Connected Output Layer ஆகியவற்றை அடுக்கி, முழுமையான **Deep Residual Shrinkage Network** உருவாக்கப்படுகிறது.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-ta/DRSN_ta_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability (பொதுவான பயன்பாடு)

**Deep Residual Shrinkage Network** உண்மையில் ஒரு பொதுவான Feature Learning முறையாகும். ஏனெனில், பல Feature Learning பணிகளில், தரவுகளில் (samples) குறைந்தபட்சம் சிறிது Noise அல்லது தொடர்பில்லாத தகவல்கள் கலந்திருக்கும். இவை கற்றல் திறனைப் பாதிக்கலாம். உதாரணமாக:

Image Classification செய்யும் போது, ஒரு படத்தில் பல தேவையற்ற பொருட்கள் இருந்தால், அவற்றை "Noise" என்று புரிந்து கொள்ளலாம். Deep Residual Shrinkage Network, அதன் Attention Mechanism மூலம் இந்த "Noise"-ஐக் கவனித்து, Soft Thresholding மூலம் அவற்றுக்குரிய Feature-களை பூஜ்யமாக்கினால், Image Classification-ன் துல்லியம் அதிகரிக்க வாய்ப்புள்ளது.

அதேபோல், **Speech Recognition** செய்யும் போது, தொழிற்சாலைகள் அல்லது சாலை ஓரங்கள் போன்ற சத்தம் மிகுந்த சூழலில், Deep Residual Shrinkage Network துல்லியத்தை அதிகரிக்க உதவலாம். அல்லது குறைந்தபட்சம், சத்தமான சூழலில் துல்லியத்தை அதிகரிப்பதற்கான ஒரு புதிய சிந்தனையை இது வழங்குகிறது.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

இந்த ஆய்வுக் கட்டுரை **Google Scholar**-ல் 1400-க்கும் மேற்பட்ட முறை மேற்கோள் காட்டப்பட்டுள்ளது (citations).

கிடைக்கப்பெற்ற புள்ளிவிவரங்களின்படி, **Deep Residual Shrinkage Network**, 1000-க்கும் மேற்பட்ட ஆய்வுக் கட்டுரைகளில் நேரடியாகவோ அல்லது மேம்படுத்தப்பட்டோ பயன்படுத்தப்பட்டுள்ளது. மெக்கானிக்கல் (Mechanical), மின்சாரம் (Electric Power), விஷன் (Vision), மருத்துவம் (Medical), குரல் (Speech), உரை (Text), ரேடார் (Radar) மற்றும் ரிமோட் சென்சிங் (Remote Sensing) போன்ற பல துறைகளில் இது பயன்படுத்தப்படுகிறது.
