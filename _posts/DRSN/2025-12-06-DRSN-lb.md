---
layout: post
title: "Deep Residual Shrinkage Network: Eng Artificial Intelligence Method fir Highly Noisy Data"
date: 2025-12-06
tags: [Deep Learning, AI]
mathjax: true
description: "Den Deep Residual Shrinkage Network ass eng verbessert Variant vum Deep Residual Network. Am Wesentlechen ass et eng Integratioun aus dem Deep Residual Network, Attention Mechanisms, a Soft Thresholding Functions."
---

**Den Deep Residual Shrinkage Network ass eng verbessert Variant vum Deep Residual Network. Am Wesentlechen ass et eng Integratioun aus dem Deep Residual Network, Attention Mechanisms, a Soft Thresholding Functions.**

**Zu engem gewëssen Grad kann de Funktionsprinzip vum Deep Residual Shrinkage Network esou verstanen ginn: en benotzt Attention Mechanisms fir onwichteg Features ze identifizéieren an applizéiert Soft Thresholding Functions, fir dës op Null ze setzen; ëmgedréint identifizéiert en wichteg Features a behält dës. Dëse Prozess verbessert d'Fäegkeet vum Deep Neural Network, nëtzlech Features aus Signaler mat Noise ze extrahéieren.**

## 1. Research Motivation

**Éischtens, wann een Samples klassifizéiert, ass d'Präsenz vun Noise – wéi Gaussian Noise, Pink Noise, a Laplacian Noise – onvermeidbar.** Méi allgemeng gesinn enthalen Samples dacks Informatiounen, déi fir déi aktuell Classification Task irrelevant sinn, wat och als Noise interpretéiert ka ginn. Dësen Noise kann d'Performance vun der Klassifikatioun negativ beaflossen. (Soft Thresholding ass e kritesche Schrëtt an ville Signal Denoising Algorithmen.)

Als Beispill: Wärend enger Konversatioun um Bord vun enger Strooss kënnen d'Audio-Signaler mat de Geräischer vun Autishupen a Rieder vermëscht sinn. Wann een Speech Recognition op dësen Signaler duerchféiert, wäerten d'Resultater zwangsleefeg vun dësen Hannergrondgeräischer beaflosst ginn. Aus enger Deep Learning Perspektiv sollten d'Features, déi zu den Hupen an de Rieder gehéieren, am Deep Neural Network eliminéiert ginn, fir ze verhënneren, datt se d'Resultater vun der Speech Recognition verfälschen.

**Zweetens, och am selwechte Dataset variéiert d'Quantitéit vun Noise dacks vu Sample zu Sample.** (Dëst huet Ähnlechkeeten mat Attention Mechanisms; wann een en Image Dataset als Beispill hëlt, kann d'Positioun vum Target Object an de Biller variéieren, an Attention Mechanisms kënnen sech op déi spezifesch Positioun vum Target Object an all Bild fokusséieren.)

Zum Beispill, beim Training vun engem Kaz-an-Hond-Klassifizéierer (Cat-and-Dog Classifier), huele mir fënnef Biller mam Label "Hond". Dat éischt Bild kéint en Hond an eng Maus enthalen, dat zweet en Hond an eng Gaus, dat drëtt en Hond an en Héngchen, dat véiert en Hond an en Iesel, an dat fënneft en Hond an eng Int. Wärend dem Training wäert de Classifier zwangsleefeg duerch irrelevant Objeten wéi Mais, Gäis, Hénger, Ieselen an Inten gestéiert ginn, wat zu enger Reduktioun vun der Accuracy féiert. Wa mir dës irrelevant Objeten identifizéieren an hir entspriechend Features eliminéieren kënnen, ass et méiglech, d'Genauegkeet vum Kaz-an-Hond-Klassifizéierer ze verbesseren.

## 2. Soft Thresholding

**Soft Thresholding ass e kritesche Schrëtt an ville Signal Denoising Algorithmen. Et eliminéiert Features, deenen hiren Absolute Value méi kleng ass wéi e gewëssen Threshold, a schrumpft (shrinks) Features, deenen hiren Absolute Value méi grouss ass wéi dësen Threshold, a Richtung Null.** Et kann duerch folgend Formel implementéiert ginn:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

D'Derivative vum Soft Thresholding Output par Rapport zum Input ass:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Wéi uewen gewisen, ass d'Derivative vum Soft Thresholding entweder 1 oder 0. Dës Eegenschaft ass identesch mat där vun der ReLU Activation Function. Dofir kann Soft Thresholding och de Risiko reduzéieren, datt Deep Learning Algorithmen Problemer wéi Gradient Vanishing a Gradient Exploding begéinen.

**An der Soft Thresholding Function muss de Setup vum Threshold zwou Konditiounen erfëllen: éischtens muss den Threshold eng positiv Zuel sinn; zweetens däerf den Threshold net méi grouss sinn wéi de maximalen Wäert vum Input Signal, soss wier den Output komplett Null.**

**Zousätzlech ass et preferabel, datt den Threshold eng drëtt Konditioun erfëllt: Jiddereen Sample sollt säin eegenen onofhängegen Threshold hunn, baséiert op sengem Noise-Inhalt.**

Dëst ass well den Inhalt un Noise oft tëscht de Samples variéiert. Zum Beispill ass et am selwechten Dataset üblech, datt Sample A manner Noise enthält, wärend Sample B méi Noise enthält. An dësem Fall, wann een Soft Thresholding an engem Denoising Algorithmus applizéiert, sollt Sample A e méi klengen Threshold benotzen, wärend Sample B e méi groussen Threshold benotze sollt. Obwuel dës Features an Thresholds an Deep Neural Networks hir explizit physikalesch Definitioun verléieren, bleift d'Basislogik déi selwecht. An anere Wierder: All Sample sollt säin eegenen onofhängegen Threshold hunn (Learn a set of thresholds), bestëmmt duerch säi spezifeschen Noise-Inhalt.

## 3. Attention Mechanism

Attention Mechanisms si relativ einfach ze verstoen am Beräich vu Computer Vision. Déi visuell Systemer vun Déieren kënnen Ziler ënnerscheeden, andeems se séier de ganzen Beräich scannen an uschléissend d'Opmierksamkeet (Attention) op d'Zilobjet fokusséieren, fir méi Detailer ze extrahéieren an irrelevant Informatiounen ze ënnerdrécken. Fir Detailer referéiert w.e.g. op d'Literatur iwwer Attention Mechanisms.

Den Squeeze-and-Excitation Network (SENet) representéiert eng relativ nei Deep Learning Method, déi Attention Mechanisms benotzt. Iwwer verschidde Samples hinweg variéiert d'Kontributioun vu verschiddene Feature Channels zur Classification Task dacks. Den SENet benotzt e klengen Sub-Network fir eng Rei vu Gewiichter ze kréien (**Learn a set of weights**) an multiplizéiert dann dës Gewiichter mat de Features vun den entspriechende Channels (**Apply weighting to each feature channel**), fir d'Gréisst vun de Features an all Channel unzepassen. Dëse Prozess kann als **Weighting** ugesi ginn, oder als d'Uwenden vu verschiddenen Niveauen vun Attention op verschidde Feature Channels.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

An dëser Approche huet all Sample säin eegenen onofhängegen Set vu Weights. An anere Wierder, d'Weights fir zwou arbiträr Samples si verschidden. Am SENet ass de spezifesche Wee (Path) fir d'Weights ze kréien: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Den Deep Residual Shrinkage Network inspiréiert sech un der uewe genannter SENet Sub-Network Struktur, fir Soft Thresholding ënner engem Deep Attention Mechanism ze implementéieren. Duerch de Sub-Network (an der rouder Këscht ugedeit) kann e Set vun Thresholds geléiert ginn (**Learn a set of thresholds**), fir **Soft thresholding** op all Feature Channel unzewenden.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

An dësem Sub-Network ginn d'Absolute Values vun alle Features an der Input Feature Map fir d'éischt berechent. Dann, duerch Global Average Pooling an Averaging, gëtt e Feature kritt, bezeechent als A. Am aneren Wee (Identity path) gëtt d'Feature Map nom Global Average Pooling an e klengen Fully Connected Network aginn. Dësen Fully Connected Network benotzt d'Sigmoid Function als seng lescht Layer, fir den Output tëscht 0 an 1 ze normaliséieren, wat e Koeffizient ergëtt, bezeechent als α. De finalen Threshold kann als α × A ausgedréckt ginn. Dofir ass den Threshold de Produit vun enger Zuel tëscht 0 an 1 an dem Duerchschnëtt vun den Absolute Values vun der Feature Map. **Dës Method garantéiert, datt den Threshold net nëmme positiv ass, mee och net exzessiv grouss.**

**Doriwwer eraus resultéieren verschidde Samples a verschiddenen Thresholds. Konsequent kann dëst zu engem gewëssen Grad als e spezialiséierten Attention Mechanism interpretéiert ginn: en identifizéiert Features, déi irrelevant fir déi aktuell Task sinn, transforméiert se iwwer zwou Convolutional Layers a Wäerter no bei Null, a setzt se mat Hëllef vu Soft Thresholding op Null; alternativ identifizéiert en Features, déi relevant fir déi aktuell Task sinn, transforméiert se iwwer zwou Convolutional Layers a Wäerter wäit ewech vun Null, a behält se.**

Schlussendlech, duerch d'Stapelen vun enger gewësser Unzuel vu Basismodulen (**Stack many basic modules**) zesumme mat Convolutional Layers, Batch Normalization, Activation Functions, Global Average Pooling, a Fully Connected Output Layers, gëtt de kompletten Deep Residual Shrinkage Network konstruéiert.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Den Deep Residual Shrinkage Network ass tatsächlech eng generell Feature Learning Method. Dëst ass well an ville Feature Learning Tasks d'Samples méi oder manner e bëssen Noise souwéi irrelevant Informatiounen enthalen. Dësen Noise an dës irrelevant Informatioune kënnen d'Performance vum Feature Learning beaflossen. Zum Beispill:

An der Image Classification: Wann e Bild gläichzäiteg vill aner Objeten enthält, kënnen dës Objeten als "Noise" verstanen ginn. Den Deep Residual Shrinkage Network kéint fäeg sinn, den Attention Mechanism ze benotzen fir dësen "Noise" ze bemierken an dann Soft Thresholding anzesetzen, fir d'Features, déi zu dësem "Noise" gehéieren, op Null ze setzen, an domat potenziell d'Genauegkeet vun der Image Classification ze verbesseren.

An der Speech Recognition, spezifesch an relativ lauten Ëmfeld wéi bei enger Konversatioun um Bord vun enger Strooss oder an enger Fabrikshal, kéint den Deep Residual Shrinkage Network d'Genauegkeet vun der Speech Recognition verbesseren, oder op d'mannst eng Methodologie ubidden, déi fäeg ass, d'Genauegkeet ze verbesseren.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Dëse Pabeier huet iwwer 1.400 Zitater op Google Scholar kritt.

Baséierend op onvollstännege Statistiken, gouf den Deep Residual Shrinkage Network (DRSN) direkt applizéiert oder modifizéiert an applizéiert an iwwer 1.000 Publikatiounen/Studien iwwer eng breet Palette vu Felder, dorënner Mechanical Engineering, Electrical Power, Vision, Healthcare, Speech, Text, Radar, a Remote Sensing.
