---
layout: post
title: "Deep Residual Shrinkage Network: ඉහළ මට්ටමේ Noise සහිත Data සඳහා වූ Artificial Intelligence ක්‍රමවේදයක්"
date: 2025-12-03
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network (DRSN) යනු Deep Residual Network (ResNet) හි දියුණු කරන ලද version එකකි. සරලව කිවහොත්, මෙය Deep Residual Network, Attention mechanisms සහ Soft thresholding functions යන තුනෙහි ඒකාබද්ධ වීමකි (integration).**

**යම්තාක් දුරකට, Deep Residual Shrinkage Network ක්‍රියා කරන ආකාරය මෙසේ වටහා ගත හැක: එය attention mechanisms භාවිතා කර වැදගත් නොවන features හඳුනා ගන්නා අතර, soft thresholding functions මගින් ඒවා ශුන්‍ය (zero) කරයි; අනෙක් අතට, වැදගත් features හඳුනාගෙන ඒවා ආරක්ෂා කර ගනී. මෙම ක්‍රියාවලිය මගින් noise අඩංගු signal වලින් ප්‍රයෝජනවත් features උකහා ගැනීමට (extract) deep neural network එකට ඇති හැකියාව වැඩි දියුණු කරයි.**

## 1. Research Motivation

**පළමුව, samples වර්ගීකරණය (classify) කිරීමේදී, Gaussian noise, pink noise සහ Laplacian noise වැනි noise පැවතීම වැළැක්විය නොහැක.** පුළුල්ව කතා කළහොත්, පවතින classification task එකට අදාළ නොවන තොරතුරු බොහෝ විට sample එකක අඩංගු විය හැකි අතර, ඒවා ද noise ලෙස සැලකිය හැක. මෙම noise මගින් classification කාර්ය සාධනයට (performance) අහිතකර ලෙස බලපෑ හැකිය. (Soft thresholding යනු බොහෝ signal denoising algorithms වල මූලික පියවරකි.)

උදාහරණයක් ලෙස, මාර්ගයක් අසල කතාබස් කරන විට, එම හඬට වාහන නලා හඬ සහ රෝදවල ශබ්ද මිශ්‍ර විය හැක. මෙම signals මත speech recognition සිදු කරන විට, පසුබිම් ශබ්ද මගින් ප්‍රතිඵලයට බලපෑම් ඇති වීම වැළැක්විය නොහැක. Deep learning දෘෂ්ටි කෝණයෙන් බලන කල, speech recognition ප්‍රතිඵලවලට බලපෑම් කිරීම වැළැක්වීම සඳහා, නලා හඬට සහ රෝදවලට අදාළ features, deep neural network එක තුළදීම ඉවත් කළ යුතුය.

**දෙවනුව, එකම dataset එක තුළ වුවද, එක් එක් sample එකෙහි අඩංගු noise ප්‍රමාණය බොහෝ විට එකිනෙකට වෙනස් වේ.** (මෙය attention mechanisms හා සමානකම් දක්වයි; image dataset එකක් උදාහරණයට ගතහොත්, එක් එක් රූපයේ target object එක පිහිටා ඇති ස්ථානය වෙනස් විය හැක. Attention mechanisms මගින් එම target object එක ඇති ස්ථානයට විශේෂ අවධානයක් යොමු කළ හැක.)

උදාහරණයක් ලෙස, බළලුන් සහ බල්ලන් (cat-and-dog) classifier එකක් පුහුණු (train) කරන විට, "dog" ලෙස label කර ඇති පින්තූර 5ක් සලකමු. පළමු රූපයේ බල්ලෙක් සහ මීයෙක්, දෙවැන්නෙහි බල්ලෙක් සහ පාත්තයෙක්, තෙවැන්නෙහි බල්ලෙක් සහ කුකුළෙක්, හතරවැන්නෙහි බල්ලෙක් සහ බූරුවෙක්, සහ පස්වැන්නෙහි බල්ලෙක් සහ තාරාවෙක් සිටිය හැක. Training අතරතුර, මීයන්, පාත්තයින්, කුකුළන්, බූරුවන් සහ තාරාවන් වැනි අදාළ නොවන වස්තූන්ගෙන් classifier එකට බාධා එල්ල වීම වැළැක්විය නොහැකි අතර, එමගින් classification accuracy එක අඩු විය හැක. අපට මෙම අදාළ නොවන වස්තූන් (මීයන්, පාත්තයින්, ආදී) හඳුනාගෙන ඒවාට අනුරූප features ඉවත් කළ හැකි නම්, cat-and-dog classifier එකෙහි accuracy එක වැඩි දියුණු කළ හැක.

## 2. Soft Thresholding

**Soft thresholding යනු බොහෝ signal denoising algorithms වල මූලික පියවරකි. මෙහිදී යම් threshold අගයකට වඩා අඩු absolute values ඇති features ඉවත් කරන අතර, එම threshold අගයට වඩා වැඩි features ශුන්‍යය (zero) දෙසට "shrink" (හකුලන) කරයි.** එය පහත සූත්‍රය (formula) භාවිතයෙන් ක්‍රියාත්මක කළ හැක:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Input එකට සාපේක්ෂව soft thresholding output එකෙහි derivative එක පහත පරිදි වේ:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

ඉහත පෙන්වා ඇති පරිදි, soft thresholding හි derivative එක 1 හෝ 0 වේ. මෙම ගුණාංගය ReLU activation function එකට සමාන වේ. එබැවින්, deep learning algorithms වලදී සිදුවන gradient vanishing සහ gradient exploding ගැටළු අවම කර ගැනීමට soft thresholding උපකාරී වේ.

**Soft thresholding function එකෙහි, threshold එක සැකසීමේදී කොන්දේසි දෙකක් සම්පූර්ණ කළ යුතුය: පළමුව, threshold එක ධන අගයක් (positive number) විය යුතුය; දෙවනුව, threshold එක input signal එකේ උපරිම අගයට වඩා වැඩි නොවිය යුතුය, එසේ නොමැතිනම් output එක සම්පූර්ණයෙන්ම ශුන්‍ය (zero) වනු ඇත.**

**මීට අමතරව, threshold එක තුන්වන කොන්දේසියක් ද තෘප්තිමත් කිරීම වඩාත් සුදුසුය: එනම්, එක් එක් sample එකෙහි අඩංගු noise ප්‍රමාණය අනුව, එම sample එකට ඊටම ආවේණික වූ ස්වාධීන threshold එකක් තිබිය යුතුය.**

මන්ද, samples අතර noise ප්‍රමාණය බොහෝ විට වෙනස් වන බැවිනි. උදාහරණයක් ලෙස, එකම dataset එකක් තුළ Sample A හි අඩු noise ප්‍රමාණයක් ද, Sample B හි වැඩි noise ප්‍රමාණයක් ද තිබීම සාමාන්‍ය දෙයකි. එවැනි අවස්ථාවක, denoising algorithm එකක් තුළ soft thresholding සිදු කරන විට, Sample A සඳහා කුඩා threshold එකක් ද, Sample B සඳහා විශාල threshold එකක් ද භාවිතා කළ යුතුය. Deep neural networks තුළ මෙම features සහ thresholds වලට නිශ්චිත භෞතික අර්ථකථනයක් (explicit physical definition) අහිමි වුවද, මූලික තර්කය එලෙසම පවතී. වෙනත් වචන වලින් කිවහොත්, එක් එක් sample එකෙහි අඩංගු noise ප්‍රමාණය අනුව තීරණය වන පරිදි, ඊටම ආවේණික වූ threshold එකක් තිබිය යුතුය.

## 3. Attention Mechanism

Computer vision ක්ෂේත්‍රයේදී Attention mechanisms තේරුම් ගැනීම සාපේක්ෂව පහසුය. සතුන්ගේ දෘශ්‍ය පද්ධතියට සම්පූර්ණ ප්‍රදේශයම වේගයෙන් scan කර ඉලක්ක (targets) වෙන්කර හඳුනාගත හැකි අතර, පසුව අදාළ නොවන තොරතුරු යටපත් කරමින් (suppress), වැඩි විස්තර ලබා ගැනීම සඳහා ඉලක්කගත වස්තුව (target object) වෙත අවධානය යොමු කළ හැක. වැඩි විස්තර සඳහා attention mechanisms පිළිබඳ පර්යේෂණ පත්‍රිකා කියවන්න.

Squeeze-and-Excitation Network (SENet) යනු attention mechanisms භාවිතා කරන සාපේක්ෂව නව deep learning ක්‍රමවේදයකි. විවිධ samples හරහා, classification task එක සඳහා විවිධ feature channels මගින් ලැබෙන දායකත්වය බොහෝ විට වෙනස් වේ. SENet විසින් කුඩා sub-network එකක් භාවිතා කර **"Learn a set of weights"** (බර තැබීම් සමූහයක් ඉගෙන ගැනීම) සිදු කරන අතර, පසුව මෙම weights අදාළ channels වල features සමඟ ගුණ කිරීම මගින් එක් එක් channel එකෙහි features වල විශාලත්වය සකස් කරයි. මෙම ක්‍රියාවලිය **"Apply weighting to each feature channel"** (එක් එක් feature channel එකට බර තැබීමක් යෙදීම) ලෙස හැඳින්විය හැකි අතර, එය විවිධ feature channels වෙත විවිධ මට්ටමේ attention ලබා දීමක් ලෙස සැලකිය හැක.

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

මෙම ක්‍රමවේදයේදී, සෑම sample එකකටම ඊටම ආවේණික වූ ස්වාධීන weights සමූහයක් ඇත. වෙනත් වචන වලින් කිවහොත්, ඕනෑම අහඹු samples දෙකක weights එකිනෙකට වෙනස් වේ. SENet හි weights ලබා ගන්නා නිශ්චිත මාර්ගය (path) වන්නේ "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function" යන්නයි.

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Deep Residual Shrinkage Network ඉහත සඳහන් කළ SENet sub-network ව්‍යුහය ආශ්‍රයෙන් deep attention mechanism එකක් යටතේ soft thresholding ක්‍රියාත්මක කරයි. රතු පැහැති කොටුවෙන් ("sub-network") පෙන්වා ඇති කොටස හරහා, එක් එක් feature channel එක සඳහා soft thresholding යෙදීමට අවශ්‍ය **"Learn a set of thresholds"** (thresholds සමූහයක් ඉගෙන ගැනීම) සිදු කරනු ලබයි.

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

මෙම sub-network එක තුළ, පළමුව input feature map එකේ ඇති සියලුම features වල absolute values ගණනය කරනු ලැබේ. ඉන්පසු, global average pooling සහ averaging හරහා A ලෙස නම් කරන ලද feature එකක් ලබා ගනී. අනිත් පසින්, **"Identity path"** එකට සමාන්තරව, global average pooling වලට ලක් වූ feature map එක කුඩා fully connected network එකකට ඇතුළත් කරයි. මෙම fully connected network එකෙහි අවසාන layer එක ලෙස Sigmoid function එක භාවිතා කරන අතර, එමගින් output එක 0 සහ 1 අතර අගයකට normalize කර, α ලෙස නම් කරන ලද සංගුණකයක් (coefficient) ලබා ගනී. අවසාන threshold අගය α×A ලෙස දැක්විය හැක. එනම්, threshold අගය යනු 0 සහ 1 අතර සංඛ්‍යාවක් සහ feature map එකේ absolute values වල සාමාන්‍ය අගය (average) අතර ගුණිතයයි. **මෙම ක්‍රමය මගින් threshold එක ධන (positive) අගයක් බවත්, එය පමණට වඩා විශාල නොවන බවත් සහතික කරයි.**

**තවද, විවිධ samples සඳහා ලැබෙන්නේ විවිධ thresholds වේ. ප්‍රතිඵලයක් ලෙස, යම්තාක් දුරකට, මෙය විශේෂිත වූ attention mechanism එකක් ලෙස අර්ථ දැක්විය හැක: එනම්, වර්තමාන task එකට අදාළ නොවන features හඳුනාගෙන, convolutional layers දෙකක් හරහා ඒවා ශුන්‍යයට ආසන්න අගයන් බවට පරිවර්තනය කර, **"Soft thresholding"** භාවිතා කර ඒවා ශුන්‍ය (zero) කරයි; එසේත් නැතිනම්, වර්තමාන task එකට අදාළ වන features හඳුනාගෙන, convolutional layers දෙකක් හරහා ඒවා ශුන්‍යයෙන් ඈත අගයන් බවට පරිවර්තනය කර, ඒවා ආරක්ෂා කර ගනී (preserve).**

අවසාන වශයෙන්, convolutional layers, batch normalization, activation functions, global average pooling සහ fully connected output layers සමඟ මූලික මොඩියුලයන් කිහිපයක් ගොඩගැසීමෙන් (**"Stack many basic modules"**) සම්පූර්ණ Deep Residual Shrinkage Network එක නිර්මාණය වේ.

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

ඇත්ත වශයෙන්ම, Deep Residual Shrinkage Network යනු සාමාන්‍ය (general) feature learning ක්‍රමවේදයකි. මන්දයත්, බොහෝ feature learning tasks වලදී, samples තුළ අඩු වැඩි වශයෙන් යම් noise ප්‍රමාණයක් මෙන්ම අදාළ නොවන තොරතුරු ද අඩංගු වන බැවිනි. මෙම noise සහ අදාළ නොවන තොරතුරු feature learning කාර්ය සාධනයට බලපෑ හැකිය. උදාහරණයක් ලෙස:

Image classification වලදී, රූපයක එකවර වෙනත් වස්තූන් (objects) බොහොමයක් අඩංගු වී තිබේ නම්, මෙම වස්තූන් "noise" ලෙස වටහා ගත හැක. Deep Residual Shrinkage Network මගින් attention mechanism භාවිතා කර මෙම "noise" හඳුනා ගත හැකි අතර, පසුව soft thresholding භාවිතා කර මෙම "noise" වලට අදාළ features ශුන්‍ය (zero) කිරීම මගින්, image classification accuracy එක වැඩි දියුණු කළ හැක.

Speech recognition වලදී, විශේෂයෙන් මාර්ගයක් අසල හෝ කර්මාන්තශාලාවක් තුළ වැනි ඝෝෂාකාරී පරිසරයකදී (noisy environments), Deep Residual Shrinkage Network මගින් speech recognition accuracy එක වැඩි දියුණු කළ හැක, හෝ අවම වශයෙන් එය වැඩි දියුණු කළ හැකි ක්‍රමවේදයක් සපයයි.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

මෙම පර්යේෂණ පත්‍රිකාව Google Scholar හි 1400 වාරයකට වඩා උපුටා දක්වා (citations) ඇත.

අසම්පූර්ණ සංඛ්‍යාලේඛනවලට අනුව, Deep Residual Shrinkage Network (DRSN) මේ වන විට mechanical engineering, electrical power, vision, healthcare, speech, text, radar සහ remote sensing ඇතුළු ක්ෂේත්‍ර ගණනාවක පර්යේෂණ පත්‍රිකා/අධ්‍යයන 1000 කට අධික සංඛ්‍යාවකදී සෘජුවම භාවිතා කර හෝ වැඩිදියුණු කර භාවිතා කර ඇත.
