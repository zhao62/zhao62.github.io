---
layout: post
title: "Deep Residual Shrinkage Network: Maysa nga Artificial Intelligence Method para iti Highly Noisy Data"
date: 2025-12-03
tags: [Deep Learning, AI]
mathjax: true
---

**Ti Deep Residual Shrinkage Network ket maysa nga improved variant ti Deep Residual Network. Sa makatuwid, daytoy ket integration ti Deep Residual Network, attention mechanisms, ken soft thresholding functions.**

**Iti maysa a level, ti working principle ti Deep Residual Shrinkage Network ket mabalin a maawatan kas dagiti sumaganad: agus-usar daytoy iti attention mechanisms tapno ma-identify dagiti unimportant features ken agus-usar iti soft thresholding functions tapno ma-set dagitoy to zero; iti sabali a bangir, i-identify-na dagiti important features ken i-retain-na dagitoy. Daytoy a proseso ket mang-enhance iti abilidad ti deep neural network a mang-extract kadagiti useful features manipud kadagiti signals nga aglaon iti noise.**

## 1. Research Motivation

**Umuna, no ag-classify kadagiti samples, ti kaadda ti noise—kas koma ti Gaussian noise, pink noise, ken Laplacian noise—ket inevitable wenno saan a maliklikan.** Iti mas nalo-loang a pannakaawat, dagiti samples ket kanayon nga aglaon iti information a *irrelevant* wenno awan maitulongna iti current classification task, ket dagitoy ket mabalin met a maibilang a noise. Daytoy a noise ket mabalin a mang-affect iti classification performance. (Ti **soft thresholding** ket maysa a key step iti adu a signal denoising algorithms.)

Kas maysa a pagarigan, no adda agsarita iti igid ti kalsada, ti audio ket mabalin a mahaluan ti uni ti busina ken pilid ti lugan. No ag-perform ti speech recognition kadagitoy a signals, ti resulta ket siguradong maapektaran kadagitoy a background sounds. Manipud iti perspektibo ti **deep learning**, dagiti features a nagtaud iti busina ken pilid ket masapul a ma-eliminate iti uneg ti deep neural network tapno saan da a maapektaran ti speech recognition results.

**Maikadua, uray iti uneg ti pareho a dataset, ti kaadu ti noise ket masansan nga agduduma kada sample.** (Adda similarities daytoy iti attention mechanisms; kas pagarigan iti maysa nga image dataset, ti lokasion ti target object ket mabalin a sabali iti kada image, ken ti attention mechanisms ket mabalin nga ag-focus iti specific location ti target object iti kada image.)

Kas pagarigan, no ag-train iti maysa a cat-and-dog classifier, ikonsiderar ti lima nga images a naka-label a "dog". Ti umuna nga image ket mabalin nga addaan iti aso ken bao, ti maikadua ket aso ken gansa, ti maikatlo ket aso ken manok, ti maikapat ket aso ken donkey, ken ti maikalima ket aso ken pato. Bayat ti training, ti classifier ket siguradong maka-encounter iti interference manipud kadagiti irrelevant objects kas iti bao, gansa, manok, donkey, ken pato, a mangibunga iti panagbaba ti classification accuracy. No kabaelan tayo a ma-identify dagitoy a irrelevant objects—dagiti bao, gansa, manok, donkey, ken pato—ken ma-eliminate ti corresponding features da, posible a ma-improve ti accuracy ti cat-and-dog classifier.

## 2. Soft Thresholding

**Ti Soft thresholding ket maysa a core step iti adu a signal denoising algorithms. I-eliminate-na dagiti features a ti absolute values-da ket nababbaba ngem iti maysa a threshold, ken i-shrink-na wenno pabassitenna dagiti features a ti absolute values-da ket nangatngato ngem iti daytoy a threshold towards zero.** Mabalin daytoy a ma-implement babaen ti sumaganad a formula:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Ti derivative ti soft thresholding output with respect to the input ket:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Kas maipakita iti ngato, ti derivative ti soft thresholding ket 1 wenno 0. Daytoy a property ket kapareho ti **ReLU activation function**. Ngarud, ti soft thresholding ket makatulong met a mang-reduce iti risk nga ag-encounter ti deep learning algorithms iti **gradient vanishing** ken **gradient exploding**.

**Iti soft thresholding function, ti setting ti threshold ket masapul a mang-satisfy iti dua a conditions: umuna, ti threshold ket masapul a positive number; maikadua, ti threshold ket saan a mabalin a lumabes iti maximum value ti input signal, ta no saan, ti output ket agbalin a puro zero.**

**Kuwera dayta, mas ideal no ti threshold ket mang-satisfy iti maikatlo a condition: tunggal sample ket masapul nga addaan iti bukod ken independent a threshold base iti noise content-na.**

Daytoy ket gapu ta ti noise content ket masansan nga agduduma iti nagbaetan dagiti samples. Kas pagarigan, kadawyan iti uneg ti pareho a dataset a ti Sample A ket addaan iti bassit a noise idinto a ti Sample B ket addaan iti ad-adu a noise. Iti daytoy a kaso, no ag-perform ti soft thresholding iti denoising algorithm, ti Sample A ket rumbeng nga agusar iti smaller threshold, idinto a ti Sample B ket rumbeng nga agusar iti larger threshold. Uray no dagitoy a features ken thresholds ket maawanan iti explicit physical definitions iti uneg ti deep neural networks, ti basic underlying logic ket agtalinaed a pareho. Iti sabali a sao, tunggal sample ket rumbeng nga addaan iti bukodna a threshold a madeterminar babaen ti specific noise content-na.

## 3. Attention Mechanism

Ti **Attention mechanisms** ket medyo nalaka a maawatan iti field ti computer vision. Ti visual systems dagiti ayup ket kabaelan a mang-distinguish kadagiti targets babaen ti napartak a panag-scan iti intero nga area, ket kalpasanna ag-focus ti attention iti target object tapno mang-extract iti ad-adu a details bayat a supress-enna dagiti irrelevant information. Para iti specifics, pangaasi a kitaen dagiti literatura maipanggep iti attention mechanisms.

Ti **Squeeze-and-Excitation Network (SENet)** ket maysa a medyo baro a deep learning method nga agus-usar iti attention mechanisms. Kadagiti nadumaduma a samples, ti contribution ti nadumaduma a feature channels iti classification task ket masansan nga agduduma. Ti SENet ket agusar iti maysa a bassit a sub-network tapno makaala iti set of weights ken kalpasanna i-multiply dagitoy a weights kadagiti features ti respective channels tapno ma-adjust ti magnitude ti features iti tunggal channel. Daytoy a proseso ket mabalin a kitaen kas application ti varying levels of attention kadagiti nadumaduma a feature channels.

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Iti daytoy nga approach, tunggal sample ket addaan iti bukod ken independent a set of weights. Iti sabali a sao, ti weights para iti aniaman a dua nga arbitrary samples ket agduma. Iti SENet, ti specific path tapno maala ti weights ket "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Ti **Deep Residual Shrinkage Network** ket nakaala iti inspirasion manipud iti nadakamat a SENet sub-network structure tapno ma-implement ti soft thresholding under a deep attention mechanism. Babaen ti sub-network (naipakita iti uneg ti nalabaga a kahon), posible ti **"Learn a set of thresholds"** tapno ma-apply ti soft thresholding iti tunggal feature channel.

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Iti daytoy a sub-network, ti absolute values ti amin a features iti input feature map ket ma-calculate nga una. Kalpasanna, babaen ti global average pooling ken averaging, maala ti maysa a feature, a ma-denote kas *A*. Iti sabali a path, ti feature map kalpasan ti global average pooling ket ma-input iti maysa a bassit a fully connected network. Daytoy a fully connected network ket agusar iti Sigmoid function kas final layer tapno ma-normalize ti output between 0 ken 1, a mangyitunda iti coefficient a ma-denote kas *α*. Ti final threshold ket mabalin a mai-express kas *α × A*. Ngarud, ti threshold ket product ti maysa a numero between 0 ken 1 ken ti average ti absolute values ti feature map. **Daytoy a method ket mangsigurado a ti threshold ket saan laeng a positive ngem saan met a sobra a dakkel.**

**Maysa pay, dagiti different samples ket mangresulta iti different thresholds. Consequently, iti maysa a level, daytoy ket mabalin a ma-interpret kas maysa a specialized attention mechanism: i-identify-na dagiti features a irrelevant iti current task, i-transform-na dagitoy a values nga asideg iti zero babaen ti two convolutional layers, ken i-set-na dagitoy to zero gamit ti soft thresholding; wenno, i-identify-na dagiti features a relevant iti current task, i-transform-na dagitoy a values nga adayo iti zero babaen ti two convolutional layers, ken i-preserve-na dagitoy.**

Sa kamaudianan, babaen ti **"Stack many basic modules"** a kadua ti convolutional layers, **batch normalization**, activation functions, **global average pooling**, ken fully connected output layers, ti kompleto a Deep Residual Shrinkage Network ket ma-construct.

<p align="center">
  <img src="/assets/img/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Ti Deep Residual Shrinkage Network ket, sa kinapudno, maysa a general feature learning method. Daytoy ket gapu ta, iti adu a feature learning tasks, dagiti samples ket more or less aglaon iti noise ken irrelevant information. Daytoy a noise ken irrelevant information ket mabalin a mang-affect iti performance ti feature learning. Kas pagarigan:

Iti **image classification**, no ti image ket simultaneously aglaon iti adu a dadduma pay nga objects, dagitoy nga objects ket mabalin a maawatan kas "noise." Ti Deep Residual Shrinkage Network ket mabalin a makausar iti attention mechanism tapno mapansin daytoy a "noise" ken kalpasanna agusar iti soft thresholding tapno ma-set dagiti features a corresponding iti daytoy a "noise" to zero, ket babaen iti dayta, posible a ma-improve ti image classification accuracy.

Iti **speech recognition**, specific iti relatively noisy environments kas iti conversational settings iti igid ti kalsada wenno iti uneg ti factory workshop, ti Deep Residual Shrinkage Network ket mabalin a mang-improve iti speech recognition accuracy, wenno at the very least, mangidatag iti methodology a kabaelan a mang-improve ti speech recognition accuracy.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Daytoy a paper ket nakaawatn iti nasurok a 1,400 a citations iti Google Scholar.

Base iti incomplete statistics, ti Deep Residual Shrinkage Network (DRSN) ket direkta a na-apply wenno na-modify ken na-apply iti nasurok a 1,000 a publications/studies iti wide range of fields, pakairamanan ti mechanical engineering, electrical power, vision, healthcare, speech, text, radar, ken remote sensing.
