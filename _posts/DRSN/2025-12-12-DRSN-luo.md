---
layout: post
title: "Deep Residual Shrinkage Network: Method mar Artificial Intelligence ne Data manigi High Noise"
date: 2025-12-12
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network en version ma olos maber moloyo Deep Residual Network. Kuom adier, Deep Residual Shrinkage Network riwo kanyachiel Deep Residual Network, attention mechanisms, kod soft thresholding functions.**

**Wanyalo winjo tiend Deep Residual Shrinkage Network e yo ma luwo ni. Mokwongo, network tiyo gi attention mechanisms mondo ofweny unimportant features. Koro, network tiyo gi soft thresholding functions mondo oket unimportant features-go obed zero. To komachielo, network fwenyo important features kendo okano important features-go. Timni (process) miyo deep neural network teko medore. Timni konyo network golo useful features kuom signals manigi noise.**

## 1. Weche Mokuongo (Research Motivation)

**Mokwongo, noise en gima ok nyal geng' sama algorithm timo classify samples. Eche mag noise-ni oriwo Gaussian noise, pink noise, kod Laplacian noise.** E yo malach, **samples** pile bedoga gi **information** ma ok owinjore gi **classification task** ma itimono. Wanyalo neno **irrelevant information**-ni kaka **noise**. **Noise**-ni nyalo duoko **classification performance** piny. (**Soft thresholding** en step maduong' ahinya e **signal denoising algorithms** mang'eny.)

Kuom ranyisi, par ane wuoyo moro ma itimo e tiend ndara. **Audio** nyalo bedo gi dwele mag honni kod tairni mag mtoka. Wanyalo timo **speech recognition** kuom **signals**-go. Weche manie tok-go (background sounds) nyaka chwan **results**. Ka waneno kokalo e wang' **deep learning**, **deep neural network** onego ogol **features** ma owinjore gi honni kod tairni-go. Golo **features**-go geng'o mondo kik gichwany **speech recognition results**.

**Mar ariyo, kar romb noise pile opogore e samples mopogore opogore. Pogruokni timore kata mana e dataset achiel.** (Pogruokni nigi tudruok gi **attention mechanisms**. Kaw ane **image dataset** kaka ranyisi. Kama **target object** nitie nyalo bedo mopogore e **images** duto. **Attention mechanisms** nyalo keto chuny (focus) kuom kama **target object** nitie e **image** ka **image**.)

Kuom ranyisi, tem ane **train** **cat-and-dog classifier** gi **images** abich ma oketne **label** ni "guok" (**dog**). **Image** 1 nyalo bedo gi guok kod oyieyo. **Image** 2 nyalo bedo gi guok kod mbaka. **Image** 3 nyalo bedo gi guok kod gweno. **Image** 4 nyalo bedo gi guok kod punda. **Image** 5 nyalo bedo gi guok kod atudo. Sama itimo **training**, gik ma ok owinjore-go biro kelo kethruok ne **classifier**. Gigi oriwo oyieyo, mbaka, gweno, punda, kod atudo. Kethruokni miyo **classification accuracy** dok piny. Ka wanyalo fwenyo gigi ma ok owinjore-go. Koro, wanyalo golo **features** ma owinjore kodgi. E yo-ni, wanyalo medo **accuracy** mar **cat-and-dog classifier**-no.

## 2. Soft Thresholding

**Soft thresholding en step maduong' e signal denoising algorithms mang'eny. Algorithm golo features ka absolute values mag features-go ni piny ne threshold moro. Algorithm-no miyo features dok chien (shrinks) ir zero ka absolute values mag features-go duong' moloyo threshold-no.** Jotim nonro (**Researchers**) nyalo tiyo gi **formula** ma luwo-ni mondo gitim **soft thresholding**:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Derivative** mar **soft thresholding output** ka ipime gi **input** en:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

**Formula** maloni nyiso ni **derivative** mar **soft thresholding** en **1** kata **0**. Kido-ni (property) chalo tuc gi mar **ReLU activation function**. Omiyo, **soft thresholding** nyalo golo luoro mar **gradient vanishing** kod **gradient exploding** e **deep learning algorithms**.

**E soft thresholding function, keto threshold nyaka luw chike ariyo. Mokwongo, threshold nyaka bed positive number. Mar ariyo, threshold ok nyal kalo value maduong' mogik (maximum value) mar input signal. Nonono, output duto biro bedo zero.**

**E wi mano, threshold onego oluw chik mar adek. Sample ka sample onego obed gi threshold-ne owuon kaluwore gi noise manie sample-no.**

Gimomiyo en ni **noise** pile opogore kuom **samples**. Kuom ranyisi, **Sample** A nyalo bedo gi **noise** manok to **Sample** B nigi **noise** mang'eny e **dataset** achiel. E kinde-ni, **Sample** A onego oti gi **threshold** matin sama itimo **soft thresholding**. **Sample** B to onego oti gi **threshold** maduong'. **Features** kod **thresholds**-gi laloga tiendgi ma yot neno (physical definitions) e **deep neural networks**. Kata kamano, **logic** ma tindo-go pod osiko. Tiende ni, **sample** ka **sample** onego obed gi **threshold** mare owuon. **Noise** manie iye ema pogo **threshold**-no.

## 3. Attention Mechanism

Jotim nonro nyalo winjo **attention mechanisms** mayot e **field** mar **computer vision**. Wenge le (animals) nyalo pogo gik moko kuom ng'iyo kama duong' piyo piyo. Bang'e, wengego keto **attention** kuom **target object**. Timni miyo **systems**-go yudo **details** mang'eny. Kendo, **systems**-go golo **irrelevant information**. Mondo iyud weche momedore, rang **literature** ma wuo kuom **attention mechanisms**.

**Squeeze-and-Excitation Network (SENet)** en **deep learning method** manyien ma tiyo gi **attention mechanisms**. E **samples** mopogore opogore, **feature channels** konyo e **classification task** e yore mopogore. **SENet** tiyo gi **sub-network** matin mondo oyud (**Learn a set of weights**). Koro, **SENet** timo **multiply** **weights**-gi kod **features** mag **channels**-go. Timni (operation) loko **magnitude** mar **features** e **channel** ka **channel**. Wanyalo neno timni kaka **Apply weighting to each feature channel**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

E yo-ni, **sample** ka **sample** nigi **set of weights**-ne owuon. Tiende ni, **weights** mag **samples** ariyo moro amora opogore. E **SENet**, yo ma iyudogo **weights** en "**Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function**."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding gi Deep Attention Mechanism

**Deep Residual Shrinkage Network** tiyo gi **structure** mar **SENet sub-network**. **Network**-ni tiyo gi **structure**-ni mondo otim **soft thresholding** e bwo **deep attention mechanism**. **Sub-network**-no (ma onyis e **box** makwar) timo **Learn a set of thresholds**. Koro, **network**-no timo **soft thresholding** ne **feature channel** ka **feature channel** kotiyo gi **thresholds**-gi.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

E **sub-network**-ni, **system** mokwongo kwano **absolute values** mag **features** duto manie **input feature map**. Koro, **system** timo **global average pooling** kod **averaging** mondo oyud **feature**, miluongo ni A. E yo machielo (path), **system** keto **feature map** e **fully connected network** matin bang' **global average pooling**. **Fully connected network**-ni tiyo gi **Sigmoid function** kaka **layer** mogik. **Function**-ni timo **normalize output** e kind **0** kod **1**. Timni kelo **coefficient**, miluongo ni α. Wanyalo wacho ni **threshold** mogik en α×A. Omiyo, **threshold** en **product** mar namba ariyo. Namba achiel ni e kind **0** gi **1**. Namba machielo en **average** mar **absolute values** mag **feature map**. **Method-ni miyo wabedo sure ni threshold en positive. Method-ni bende miyo threshold ok bed maduong' ahinya.**

**E wi mano, samples mopogore kelo thresholds mopogore. Kuom mano, wanyalo winjo method-ni kaka attention mechanism ma special. Mechanism-ni fwenyo features ma ok owinjore gi task mantie sani. Mechanism-ni loko features-go obed values machiegni gi zero kokalo kuom convolutional layers ariyo. Koro, mechanism-ni keto features-go obed zero kotiyo gi soft thresholding. E yo machielo, mechanism-ni fwenyo features ma owinjore gi task mantie sani. Mechanism-ni loko features-go obed values mabor gi zero kokalo kuom convolutional layers ariyo. Gikone, mechanism-ni kano features-go.**

Gikone, wa-**Stack many basic modules** kanyachiel. Wa-**include** bende **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, kod **fully connected output layers**. Timni (process) loso **Deep Residual Shrinkage Network** ma duto. Bende, kaka inyalo neno e diagram, **Identity path** nitie mondo okony e **flow** mar **gradient**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

**Deep Residual Shrinkage Network** en **general method** mar **feature learning**. Gimomiyo en ni **samples** pile bedoga gi **noise** e **feature learning tasks** mang'eny. **Samples** bende bedoga gi **irrelevant information**. **Noise** kod **irrelevant information**-gi nyalo ketho **performance** mar **feature learning**. Kuom ranyisi:

Kaw ane **image classification**. **Image** nyalo bedo gi gik moko mang'eny e iye. Wanyalo winjo gigi kaka "**noise**." **Deep Residual Shrinkage Network** nyalo tiyo gi **attention mechanism**. **Network** neno "**noise**"-ni. Koro, **network** tiyo gi **soft thresholding** mondo oket **features** ma owinjore gi "**noise**"-ni obed **zero**. Timni (action) nyalo medo **accuracy** mar **image classification**.

Kaw ane **speech recognition**. Ahinya ahinya, par ane **environments** manigi **noise** kaka wuo e tiend ndara kata e i **factory workshop**. **Deep Residual Shrinkage Network** nyalo medo **accuracy** mar **speech recognition**. Kata koro, **network** chiwo **methodology** moro. **Methodology**-ni nigi nyalo mar medo **accuracy** mar **speech recognition**.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Weche mag Impact (Academic Impact)

**Paper**-ni oseyudo **citations** mokalo 1,400 e **Google Scholar**.

Kaluwore gi **statistics**, jotim nonro (**researchers**) osetiyo gi **Deep Residual Shrinkage Network (DRSN)** e **publications/studies** mokalo 1,000. **Applications**-gi oriwo **fields** malach. **Fields**-gi oriwo **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, kod **remote sensing**.
