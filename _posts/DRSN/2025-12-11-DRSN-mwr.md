---
layout: post
title: "Deep Residual Shrinkage Network: Highly Noisy Data वास्ते एक Artificial Intelligence Method"
date: 2025-12-11
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network** असल में **Deep Residual Network** रो ही एक सुधरायोड़ो रूप (improved variant) है। मोटी बात आ है कि, **Deep Residual Shrinkage Network** में **Deep Residual Network**, **attention mechanisms**, और **soft thresholding functions** मिलियोड़ा होवे।

आप **Deep Residual Shrinkage Network** ने इयाँ समझ सको हो कि ओ काम कियाँ करे। पहली, ओ network **attention mechanisms** use कर'र बेकार (unimportant) **features** ने पिछाणे। पछे, ओ network **soft thresholding functions** use कर'र इन बेकार **features** ने zero कर देवे। इसके उल्ट, network काम रा (important) **features** ने पिछाणे और उने संभाल'र राखे। आ चीज **deep neural network** री ताकत बढ़ा देवे। इस सूं network ने **noise** वाले **signals** में सूं काम रा **features** निकालण में मदद मिले।

## 1. **शोध रो कारण (Research Motivation)**

सबसूं पहली बात, जद algorithm **samples** ने classify करे, तो **noise** तो होवे ही है। **Noise** रा उदाहरण (examples) है **Gaussian noise**, **pink noise**, और **Laplacian noise**। थोड़ा और खुल'र कहा, तो **samples** में कई बार ऐसी जानकारी होवे जो current **classification task** वास्ते काम री कोनी होवे। आप इन बेकार जानकारी ने **noise** मान सको हो। ओ **noise** classification री performance ने कम कर सके। (**Soft thresholding** कई **signal denoising algorithms** रो एक main step होवे।)

उदाहरण के तौर पर, सड़क किनारे हो रही बात-चीत ने समझो। Audio में गाड़ी के horn और पहिया (wheels) री आवाज आ सके। मान लो कि म्हने इन **signals** पर **speech recognition** करणी है। तो background री आवाज result ने खराब कर सके। **Deep learning** रे हिसाब सूं, **deep neural network** ने horn और wheels सूं जुड़े **features** ने हटा देणो चाइजे। इयाँ करण सूं ये **features** म्हारी **speech recognition** रे result ने खराब कोनी करेला।

दूसरी बात, अलग-अलग **samples** में **noise** री मात्रा कम-ज्यादा हो सके। ये फरक एक ही **dataset** रे मायने भी हो सके। (आ चीज **attention mechanisms** सूं मिलती-जुलती है। एक **image dataset** रो उदाहरण लो। हर photo में target object अलग जगह पर हो सके। **Attention mechanisms** हर photo में target object री सही जगह पर ध्यान लगा सके।)

उदाहरण वास्ते, एक cat-and-dog classifier ने train करण री सोचो जिथे 5 photos पर "dog" रो label है। Image 1 में कुत्ता और चूहा (mouse) हो सके। Image 2 में कुत्ता और हंस (goose) हो सके। Image 3 में कुत्ता और मुर्गी (chicken) हो सके। Image 4 में कुत्ता और गधा (donkey) हो सके। Image 5 में कुत्ता और बतख (duck) हो सके। **Training** रे time, बेकार चीज classifier ने confuse करेला। जियाँ कि चूहा, हंस, मुर्गी, गधा और बतख। इस वजह सूं classification री accuracy कम हो जावे। अगर म्हने इन बेकार चीजां रो पतो चल जावे, तो म्हे उनसे जुड़े **features** ने हटा सको हाँ। इस तरह सूं, म्हे cat-and-dog classifier री accuracy बढ़ा सको हाँ।

## 2. **Soft Thresholding**

**Soft thresholding** कई **signal denoising algorithms** रो एक core step है। अगर **features** री **absolute values** एक **threshold** सूं कम होवे, तो algorithm उने उड़ा देवे (eliminates)। अगर **features** री **absolute values** इस **threshold** सूं ज्यादा होवे, तो algorithm उने zero री तरफ सिकोड़ देवे (shrinks)। Researchers नीचे दिए गए formula सूं **soft thresholding** use कर सके:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Soft thresholding** रे output रो derivative input रे हिसाब सूं ओ होवे:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

ऊपर वालो formula बतावे कि **soft thresholding** रो derivative या तो 1 होवे या 0। ओ **ReLU activation function** जिसो ही है। इस वास्ते, **soft thresholding** deep learning algorithms में **gradient vanishing** और **gradient exploding** रो risk कम कर सके।

**Soft thresholding function** में, **threshold** set करते time दो शर्तां (conditions) माननी पड़े। पहली, **threshold** positive number होणो चाइजे। दूसरी, **threshold** input **signal** री maximum value सूं ज्यादा कोनी होणो चाइजे। नहीं तो, सारा output zero हो जावेला।

साथ ही, **threshold** ने तीसरी शर्त भी माननी चाइजे। हर **sample** रो खुद रो अलग **threshold** होणो चाइजे, जो उस **sample** रे **noise** रे हिसाब सूं होवे।

इसरो कारण ओ है कि **samples** में **noise** अलग-अलग होवे। जैसे, एक ही **dataset** में Sample A में कम **noise** हो सके और Sample B में ज्यादा **noise**। इस case में, **soft thresholding** करते time Sample A वास्ते छोटो **threshold** होणो चाइजे और Sample B वास्ते बड़ो **threshold**। हालाँकि **deep neural networks** में इन **features** और **thresholds** रो physical मतलब साफ़ कोनी होवे, पण logic वही रहे। सीधी बात आ है कि, हर **sample** रो एक independent **threshold** होणो चाइजे। कितनो **noise** है, उस हिसाब सूं **threshold** decide होवे।

## 3. **Attention Mechanism**

Researchers **computer vision** field में **attention mechanisms** ने आसानी सूं समझ सके। जानवरां (animals) री आँखां (visual systems) पहली सारे इलाके ने जल्दी सूं scan करे और target ने पिछाणे। पछे, आँखां target object पर ध्यान (attention) लगावे। इस सूं system ने और ज्यादा details मिले। साथ ही, system बेकार जानकारी ने दबा देवे (suppress)। ज्यादा जानकारी वास्ते, आप **attention mechanisms** वालो literature पढ़ सको हो।

**Squeeze-and-Excitation Network (SENet)** एक नया **deep learning** method है जो **attention mechanisms** use करे। अलग-अलग **samples** में, अलग-अलग **feature channels** classification task में अलग तरह सूं योगदान देवे। **SENet** एक छोटे **sub-network** रो use कर'र **Learn a set of weights** (weights रो एक set पता करे)। पछे, **SENet** इन weights ने उनके **channels** रे **features** सूं multiply करे। ओ operation हर **channel** रे **features** री value ने adjust करे। आप इस process ने इयाँ समझ सको हो कि अलग-अलग **feature channels** पर अलग-अलग level रो **Apply weighting to each feature channel** (weighting लगायी है)।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

इस तरीके में, हर **sample** वास्ते **weights** रो एक independent set होवे। मतलब, कोई भी दो **samples** रे **weights** अलग-अलग होवे। **SENet** में, weights लाने रो रास्तो ओ है: "**Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function**."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. **Deep Attention Mechanism रे साथ Soft Thresholding**

**Deep Residual Shrinkage Network**, **SENet sub-network** रे structure ने use करे। Network इस structure सूं **deep attention mechanism** रे under **soft thresholding** करे। ओ **sub-network** (जो लाल डिब्बे/red box में है) **Learn a set of thresholds** (thresholds रो एक set सीखे)। पछे, network इन thresholds ने use कर'र हर **feature channel** पर **soft thresholding** लगावे।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

इस **sub-network** में, system पहली input **feature map** रे सारे **features** री absolute values निकाले। पछे, system **global average pooling** और averaging करे ताकि एक **feature** मिले, जिने *A* कहा। दूसरे रास्ते (path) में, **global average pooling** रे बाद system **feature map** ने एक छोटे **fully connected network** में डाले। ओ **fully connected network** आखरी layer में **Sigmoid function** use करे। ओ function output ने 0 और 1 रे बीच में normalize करे। इस process सूं एक coefficient मिले, जिने *α* कहा। आप final **threshold** ने *α × A* मान सको हो। इसलिये, **threshold** दो नंबरां रो गुणा (product) है। एक नंबर 0 और 1 रे बीच में है। दूसरा नंबर **feature map** री absolute values रो average है। **ओ तरीको ध्यान राखे कि threshold हमेशा positive होवे। ओ तरीको ये भी ध्यान राखे कि threshold घणा बड़ो न हो जावे।**

इसके अलावा, अलग-अलग samples सूं अलग-अलग thresholds बने। इस वास्ते, आप इस तरीके ने एक special **attention mechanism** मान सको हो। ओ mechanism current task वास्ते बेकार **features** ने पिछाणे। ओ mechanism दो **convolutional layers** use कर'र इन **features** ने 0 रे पास ले आवे। पछे, ओ mechanism **soft thresholding** use कर'र इन **features** ने zero कर देवे। या फेर इयाँ कहो, ओ mechanism current task वास्ते काम रा **features** ने पिछाणे। ओ mechanism दो **convolutional layers** use कर'र इन **features** ने 0 सूं दूर ले जावे। आखिर में, ओ mechanism इन **features** ने बचा के राखे (preserves)।

आखिर में, म्हे कई सारे **basic modules** ने एक रे ऊपर एक रख सको (**Stack many basic modules**)। म्हे इसमें **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, और **fully connected output layers** भी जोड़ा। इयाँ पूरो **Deep Residual Shrinkage Network** बन जावे।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. **Generalization Capability**

**Deep Residual Shrinkage Network** **feature learning** वास्ते एक general method है। क्यूँकि, घणे सारे **feature learning** tasks में **samples** में **noise** होवे ही है। **Samples** में बेकार जानकारी भी होवे। ये **noise** और बेकार जानकारी **feature learning** री performance खराब कर सके। उदाहरण वास्ते:

**Image classification** ने देखो। एक photo में और भी घणी चीजां हो सके। आप इन चीजां ने "**noise**" मान सको हो। **Deep Residual Shrinkage Network** शायद **attention mechanism** use कर सके। Network इस "**noise**" ने note करे। पछे, network **soft thresholding** सूं इस "**noise**" वाले **features** ने zero कर देवे। इस सूं **image classification** री accuracy बढ़ सके।

**Speech recognition** ने देखो। खास कर शोर-शराबे वाले माहौल में, जियाँ सड़क किनारे या factory में बात-चीत। **Deep Residual Shrinkage Network** **speech recognition** री accuracy बढ़ा सके। या कम सूं कम, network एक तरीको तो देवे ही है। ओ तरीको **speech recognition** री accuracy बढ़ा सके है।

## **संदर्भ (Reference)**

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## **Academic Impact**

इस paper ने **Google Scholar** पर 1400 सूं ज्यादा citations मिल्या है।

अधूरे आंकड़ों (incomplete statistics) रे हिसाब सूं, researchers ने **Deep Residual Shrinkage Network (DRSN)** ने 1000 सूं ज्यादा publications/studies में काम में लियो है। ये applications घणे fields में फैलोड़ी है। इन fields में **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, और **remote sensing** शामिल है।
