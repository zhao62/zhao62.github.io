---
layout: post
title: "Deep Residual Shrinkage Network: Метод с изкуствен интелект за силно зашумени данни"
date: 2025-11-29
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network (DRSN) е подобрен вариант на дълбоките остатъчни мрежи (Deep Residual Networks). По същество, тя представлява интеграция на дълбоки остатъчни мрежи, механизми за внимание (attention mechanisms) и функции за меко прагуване (soft thresholding).**

**В известна степен, принципът на действие на Deep Residual Shrinkage Network може да се разбере по следния начин: чрез механизмите за внимание се забелязват и идентифицират незначителните характеристики (features), които след това се нулират чрез функциите за меко прагуване; или казано по друг начин – механизмите за внимание откриват важните характеристики и ги запазват. Този процес усилва способността на дълбоката невронна мрежа да извлича полезна информация от сигнали, съдържащи шум.**

## 1. Мотивация на изследването

**Първо, при класификацията на образци (samples), наличието на шум – като гаусов шум, розов шум, лапласов шум и др. – е неизбежно.** В по-широк смисъл, образците често съдържат информация, която не е свързана с текущата задача за класификация, което също може да се тълкува като „шум“. Този шум може да окаже негативно влияние върху ефективността на класификацията. (Мекото прагуване е ключова стъпка в много алгоритми за потискане на шума от сигнали).

Например, при разговор на улицата, човешкият глас може да се смеси със звуци от автомобилни клаксони, въртене на гуми и други. Когато се извършва разпознаване на реч върху такива сигнали, резултатът неизбежно ще бъде повлиян от тези фонови звуци. От гледна точка на дълбокото обучение (deep learning), характеристиките, съответстващи на клаксоните и колелата, трябва да бъдат премахнати вътре в самата дълбока невронна мрежа, за да се предотврати влиянието им върху разпознаването на речта.

**Второ, дори в рамките на един и същ набор от данни (dataset), количеството шум често варира при отделните образци.** (Това има допирни точки с механизмите за внимание; ако вземем за пример набор от изображения, позицията на целевия обект може да е различна в различните снимки, а механизмът за внимание може да се фокусира конкретно върху мястото на обекта във всяка отделна снимка).

Например, когато обучаваме класификатор за кучета и котки, нека разгледаме 5 изображения с етикет „куче“. Първото изображение може да съдържа куче и мишка, второто – куче и гъска, третото – куче и кокошка, четвъртото – куче и магаре, а петото – куче и патица. При обучението на класификатора, ние неизбежно ще бъдем подложени на смущения от ирелевантни обекти като мишки, гъски, кокошки, магарета и патици, което води до спад в точността на класификацията. Ако успеем да „забележим“ тези ирелевантни обекти и да премахнем съответстващите им характеристики, е възможно да повишим точността на класификатора за кучета и котки.

## 2. Меко прагуване (Soft Thresholding)

**Мекото прагуване е основна стъпка в много алгоритми за потискане на шума в сигналите. То премахва характеристики, чиято абсолютна стойност е по-малка от определен праг, и „свива“ (shrinks) към нулата тези характеристики, чиято абсолютна стойност е по-голяма от този праг.** Това може да се реализира чрез следната формула:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Производната на изхода на мекото прагуване спрямо входа е:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Както се вижда по-горе, производната на мекото прагуване е или 1, или 0. Това свойство е идентично с това на активационната функция ReLU. Следователно, мекото прагуване също може да намали риска от изчезване (gradient vanishing) или експлодиране (gradient exploding) на градиента, с който често се сблъскват алгоритмите за дълбоко обучение.

**Във функцията за меко прагуване, задаването на прага трябва да отговаря на две условия: първо, прагът трябва да е положително число; второ, прагът не може да бъде по-голям от максималната стойност на входния сигнал, в противен случай изходът ще бъде изцяло нула.**

**Също така е желателно прагът да отговаря и на трето условие: всеки образец трябва да има свой собствен, независим праг, съобразен със съдържанието на шум в него.**

Това се налага, защото съдържанието на шум често е различно при различните образци. Например, често се случва в един и същ набор от данни, образец А да съдържа по-малко шум, докато образец Б да съдържа повече шум. В такъв случай, ако прилагаме меко прагуване в алгоритъм за потискане на шума, образец А трябва да използва по-малък праг, докато образец Б трябва да използва по-голям праг. В дълбоките невронни мрежи, въпреки че тези характеристики и прагове губят своя ясен физически смисъл, основната логика остава същата. Тоест, всеки образец трябва да има свой собствен независим праг, базиран на собственото му ниво на шум.

## 3. Механизъм за внимание (Attention Mechanism)

Механизмите за внимание са сравнително лесни за разбиране в областта на компютърното зрение (Computer Vision). Зрителната система на животните може бързо да сканира цялата област, да открие целевия обект и след това да фокусира вниманието си върху него, за да извлече повече детайли, като същевременно потиска несвързаната информация. За повече подробности, моля, направете справка с литературата относно механизмите за внимание.

Squeeze-and-Excitation Network (SENet) е сравнително нов метод за дълбоко обучение, използващ механизми за внимание. При различните образци, приносът на различните канали от характеристики (feature channels) към задачата за класификация често е различен. SENet използва малка подмрежа, за да получи набор от тегла, и след това умножава тези тегла по характеристиките на съответните канали, за да коригира големината на характеристиките във всеки канал. Този процес може да се разглежда като прилагане на различно ниво на внимание към различните канали от характеристики.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-29-DRSN-sr-Latn/SENET_sr_Latn_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

При този подход всеки образец притежава свой собствен независим набор от тегла. С други думи, теглата за всеки два произволни образеца са различни. В SENet конкретният път за получаване на теглата е: „Глобален пулнинг (Global Pooling) → Напълно свързан слой (Fully Connected Layer) → Функция ReLU → Напълно свързан слой → Функция Sigmoid“.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-29-DRSN-sr-Latn/SENET_sr_Latn_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Меко прагуване с дълбок механизъм за внимание

Deep Residual Shrinkage Network заема гореспоменатата структура на подмрежата от SENet, за да реализира меко прагуване, управлявано от дълбок механизъм за внимание. Чрез подмрежата (обозначена в червеното поле на диаграмите на алгоритъма), може да се научи набор от прагове, чрез които да се извърши меко прагуване на всеки канал от характеристиките.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-29-DRSN-sr-Latn/DRSN_sr_Latn_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

В тази подмрежа първо се изчисляват абсолютните стойности на всички характеристики във входната карта на характеристиките (feature map). След това, чрез глобален пулнинг на средна стойност (global average pooling) и усредняване, се получава една характеристика, която обозначаваме с A. В другия път, картата на характеристиките след глобалния пулнинг се подава към малка напълно свързана мрежа (fully connected network). Послелният слой на тази мрежа използва функцията Sigmoid, за да нормализира изхода в диапазона между 0 и 1, получавайки коефициент, който обозначаваме с α. Крайният праг може да се изрази като α × A. Следователно, прагът представлява число между 0 и 1, умножено по средната стойност на абсолютните стойности на картата на характеристиките. **Този метод гарантира, че прагът е не само положителeн, но и че няма да бъде прекалено голям.**

**Освен това, различните образци получават различни прагове. Следователно, в известна степен това може да се разбира като специален вид механизъм за внимание: забелязват се характеристики, които не са свързани с текущата задача, те се трансформират чрез два конволюционни слоя (convolutional layers) в стойности, близки до 0, и чрез меко прагуване се нулират; или казано иначе – забелязват се характеристиките, свързани с текущата задача, те се трансформират чрез два конволюционни слоя в стойности, далеч от 0, и се запазват.**

Накрая, чрез натрупване (stacking) на определен брой основни модули, както и конволюционни слоеве, Batch Normalization, активационни функции, глобален пулнинг на средна стойност и напълно свързан изходен слой, се получава пълната Deep Residual Shrinkage Network.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-29-DRSN-sr-Latn/DRSN_sr_Latn_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Универсалност

Deep Residual Shrinkage Network всъщност е универсален метод за изучаване на характеристики (feature learning). Това е така, защото в много задачи за изучаване на характеристики, образците съдържат повече или по-малко шум, както и ирелевантна информация. Този шум и несвързана информация могат да повлияят на ефективността на обучението. Например:

При класификация на изображения, ако изображението съдържа много други обекти едновременно, тези обекти могат да бъдат разбрани като „шум“; Deep Residual Shrinkage Network може би ще успее, чрез механизма за внимание, да забележи този „шум“ и след това, чрез меко прагуване, да нулира характеристиките, съответстващи на този „шум“, което може да повиши точността на класификацията на изображения.

При разпознаване на реч, особено в по-шумна среда – например при разговор до пътя или във фабричен цех – Deep Residual Shrinkage Network може да повиши точността на разпознаването или най-малкото предоставя идея и методология за подобряване на точността на разпознаване на реч.

## Библиография (Reference)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Влияние и отзвук

Броят на цитиранията на тази статия в Google Scholar надхвърля 1400.

Според непълна статистика, Deep Residual Shrinkage Network е била използвана директно или подобрена и приложена в над 1000 публикации в множество области, включително механика, електроенергетика, компютърно зрение, медицина, обработка на реч, текст, радарни системи, дистанционно наблюдение и други.
