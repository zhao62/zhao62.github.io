---
layout: post
title: "Deep Residual Shrinkage Network: Artificial Intelligence tʋʋm-kãngã sẽn tʋmd ne Highly Noisy Data"
date: 2025-12-11
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network** yaa **Deep Residual Network** sẽn maneg n yɩɩda. Sã n yaa a võore, **Deep Residual Shrinkage Network** wã lagimda **Deep Residual Network**, **attention mechanisms**, la **soft thresholding functions** n tʋmdẽ.

D tõe n bãnga **Deep Residual Shrinkage Network** tʋʋmde woto: Pipi, **network** wã tũnuga ne **attention mechanisms** n bãng **features** nins sẽn pa tar yõodo. Rẽ poore, **network** wã tũnuga ne **soft thresholding functions** n rɩk **unimportant features** kãens n lebg zero. La sã n yaa **important features** wã, **network** wã bãngda bãmba, n bas-b tɩ b looge. Tʋʋm-kãngã kenga **deep neural network** wã pãnga. A sõngda **network** wã t'a yiis **useful features** nins sẽn be **signals** wã pʋgẽ, baa ne **noise** sẽn be a pʋgẽ wã.

## 1. Research Motivation

**Pipi**, **noise** wã yaa bũmb d sẽn pa tõe n gĩ ne, sã n yaa tɩ **algorithm** wã maand **classify** ne **samples**. **Noise** kãngã buud yaa wala **Gaussian noise**, **pink noise**, la **Laplacian noise**. Sã n yaa ne pãng sẽn yɩɩd woto, **samples** wã nod n tara kibay sẽn pa tar yõod ne **classification task** wã. D tõe n gesa kibay kãens wa **noise**. **Noise** kãngã tõe n kɩtame tɩ **classification performance** wã lʋɩ tẽng wʋsgo. (**Soft thresholding** yaa tʋʋm-kãng sẽn tar yõod wʋsg **signal denoising algorithms** wʋsg pʋgẽ.)

Bɩ d rɩk makre: d sã n sosd sor noli. Koɛɛgã tõe n tara mobili-dãmb wiis la b rũms goama. D sã n dat n maan **speech recognition** ne **signals** kãense, bũmb nins sẽn be poorẽ wã na n kɩtame tɩ tʋʋmdã pa yɩ sõma ye. D sã n ges **deep learning** nifẽ, **deep neural network** wã segd n yisa **features** nins sẽn yaa mobili-dãmb wiis la b rũmsã. Yaa woto la d tõe n gidi **features** kãens tɩ b ra paam pãng **speech recognition** wã zutu ye.

**Yiib-n-soaba**, **noise** wã sõor pa yembre ne **samples** wã fãa ye. Bũmb kãngã maanda woto baa **dataset** a ye pʋgẽ. (Yɛl kãens tara wʋsg n naag ne **attention mechanisms**. Bɩ d rɩk **image dataset** n maan makre. Bõn-ning d sẽn dat n gese, a zĩig tõe n yaa toor ne **images** wã fãa. **Attention mechanisms** wã tõe n kenga a nif ne zĩig ninga d sẽn dat n ges **image** fãa pʋgẽ.)

Bɩ d rɩk makre: d sã n train **cat-and-dog classifier** ne **images** a nu sẽn tar "dog" label. **Image** 1 tõe n tara baa ne dayuug-bila. **Image** 2 tõe n tara baa ne no-yɛɛga. **Image** 3 tõe n tara baa ne noaaka. **Image** 4 tõe n tara baa ne bõnga. **Image** 5 tõe n tara baa ne larde. **Training** wã sasa, bõn-kãens sẽn pa tar yõod wã na n kɩtame tɩ **classifier** wã pa tʋm sõma ye. Bõn-kãens yaa dayuug-bila, no-yɛɛga, noaaka, bõnga, la larde. Woto kɩtdame tɩ **classification accuracy** wã lʋɩ tẽnga. D sã n tõog n bãng bõn-kãens nins sẽn pa tar yõodo, d tõe n yiisa **features** nins sẽn yaa b rẽndã. Woto, d tõe n kenga **cat-and-dog classifier** wã **accuracy** t'a yɩ sõma wʋsgo.

## 2. Soft Thresholding

**Soft thresholding** yaa tʋʋm-kãng sẽn tar yõod wʋsg **signal denoising algorithms** wʋsg pʋgẽ. **Algorithm** wã yiisda **features** wã sã n yaa tɩ **absolute values** wã pa ta **threshold** wã. **Algorithm** wã maanda **shrinks features** wã n dɩk n kẽng zero nifẽ, sã n yaa tɩ **absolute values** wã yɩɩda **threshold** wã. **Researchers** wã tõe n tũnuga ne formula kãngã n maan **soft thresholding**:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Soft thresholding** output wã **derivative** ne a input wã yaa:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Formula kãngã sẽn be yĩngrã wilgdame tɩ **soft thresholding** wã **derivative** yaa 1 bɩ 0. Bũmb kãngã yaa woto wa **ReLU activation function** wã me. Woto yĩnga, **soft thresholding** wã tõe n kɩtame tɩ **gradient vanishing** la **gradient exploding** ra yɩ yɛl-kãseng **deep learning algorithms** wã pʋgẽ ye.

**Soft thresholding function** wã pʋgẽ, d sã n dɩk **threshold** wã, a segd n tũu conditions a yiibu. Pipi, **threshold** wã segd n yaa **positive number**. Yiib-n-soaba, **threshold** wã pa segd n yɩɩd **input signal** wã **maximum value** ye. Sã n pa woto, **output** wã fãa na n lebga zero.

Sẽn paase, **threshold** wã segd n tũu condition a tãab-n-soaba. **Sample** fãa segd n tara a **threshold** a to, n tũ ne **noise** wã sõor sẽn be **sample** kãngã pʋgẽ.

A võor yaa tɩ **noise** wã sõor pa yembre ne **samples** wã fãa ye. Wala makre, **Sample A** tõe n tara **noise** bilfu, tɩ **Sample B** tar **noise** wʋsg **dataset** a ye pʋgẽ. Zĩig kãngã, **Sample A** segd n tũnuga ne **threshold** sẽn yaa bilfu **soft thresholding** wã sasa. **Sample B** segd n tũnuga ne **threshold** sẽn yaa kãsenga. **Deep neural networks** wã pʋgẽ, baa ne **features** la **thresholds** wã sẽn pa tar **physical definitions** sẽn yaa vẽeneg wã, a tʋʋmdã võor yaa yembre. Woto yĩnga, **sample** fãa segd n tara **independent threshold**. **Noise** wã sõor n na n wilg tɩ **threshold** wã yaa kãseng bɩ a yaa bilfu.

## 3. Attention Mechanism

**Researchers** wã tõe n bãnga **attention mechanisms** wã võor nana-nana **computer vision** tʋʋmdã pʋgẽ. Rũmsã **visual systems** tõe n bãnga bõn-nins b sẽn dat n gesã, n gese zĩigã fãa tao-tao. Rẽ poore, **visual systems** wã ningda **attention** bõn-kãngã zugu. Tʋʋm-kãngã kɩtdame tɩ **systems** wã paam kibay wʋsg n paase. Sasa kãngã me, **systems** wã gida kibay nins sẽn pa tar yõod wã. Sã n yaa ne a kɛlgre, bɩ y ges **literature** nins sẽn gomd **attention mechanisms** wã yelle.

**Squeeze-and-Excitation Network (SENet)** yaa **deep learning method** sẽn yaa paalgo, n tũnugd ne **attention mechanisms**. **Samples** toor-toor pʋgẽ, **feature channels** toor-toor sõngda **classification task** wã ne sore toor-toore. **SENet** tũnugda ne **sub-network** bilf n paam **a set of weights**. Rẽ poore, **SENet** wã maanda **Apply weighting to each feature channel** (paasda features wã pãng ne weights wã). Tʋʋm-kãngã teenda **features** wã pãnga **channel** fãa pʋgẽ. D tõe n gesa tʋʋm-kãngã wa d sẽn ningd **attention** sẽn yaa toor-toor ne **feature channels** toor-toor zutu.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Sore kãngã pʋgẽ, **sample** fãa tara **independent set of weights**. Woto yĩnga, **samples** a yiib **weights** pa yembre ye. **SENet** pʋgẽ, sore ning b sẽn tũnugd n paamd **weights** wã yaa: “**Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function**”.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

**Deep Residual Shrinkage Network** wã tũnugda ne **SENet sub-network** wã structure. **Network** wã tũnugda ne structure kãngã n maan **soft thresholding** ne **deep attention mechanism**. **Sub-network** wã (sẽn be red box pʋgẽ wã) maanda **Learn a set of thresholds**. Rẽ poore, **network** wã maanda **soft thresholding** ne **feature channel** fãa, n tũnugd ne **thresholds** kãense.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

**Sub-network** kãngã pʋgẽ, **system** wã rengd n maanda **calculate** ne **absolute values** nins fãa sẽn be **input feature map** wã pʋgẽ. Rẽ poore, **system** wã maanda **global average pooling** la **averaging** n paam **feature** a ye, tɩ d boond-a tɩ A. Sore a to wã pʋgẽ, **system** wã maanda **input feature map** wã ne **fully connected network** bilfu, **global average pooling** wã poore. **Fully Connected network** kãngã tũnugda ne **Sigmoid function** wa a layer sẽn baasdã. **Function** kãngã maanda **normalizes the output** 0 la 1 suka. Tʋʋm-kãngã wata ne **coefficient** a ye, tɩ d boond-a tɩ α. D tõe n wilga **final threshold** wã wa α × A. Woto yĩnga, **threshold** wã yaa sõor a yiib la b lagem n "multiply". Sõor a ye be 0 la 1 suka. Sõor a to wã yaa **average** ne **absolute values** nins sẽn be **feature map** wã pʋgẽ. **Sore kãngã kɩtdame tɩ threshold wã yaa positive. Sore kãngã me kɩtdame tɩ threshold wã pa kãseng n loog ye.**

Sẽn paase, **samples** toor-toor kɩtdame tɩ **thresholds** wã yaa toor-toore. Woto yĩnga, d tõe n bãnga **method** kãngã wa **attention mechanism** sẽn yaa a toor. **Mechanism** kãngã bãngda **features** nins sẽn pa tar yõod ne **task** wã. **Mechanism** wã teenda **features** kãens n lebg sõor sẽn pẽ ne zero, n tũnug ne **convolutional layers** a yiibu. Rẽ poore, **mechanism** wã maanda **soft thresholding** n rɩk **features** kãens n lebg zero. Bɩ d tõe n yeel tɩ **mechanism** wã bãngda **features** nins sẽn tar yõod ne **task** wã. **Mechanism** wã teenda **features** kãens n lebg sõor sẽn zãr ne zero, n tũnug ne **convolutional layers** a yiibu. Wa a sẽn baasdã, **mechanism** wã kɩtdame tɩ **features** kãens looge.

A baasgẽ, d maanda **Stack many basic modules** (ningd basic modules wã b taab zutu). D le paasda **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, la **fully connected output layers**. Tʋʋm-kãngã meeda **Deep Residual Shrinkage Network** wã t'a zems zãng fasɩ.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

**Deep Residual Shrinkage Network** yaa **general method** ne **feature learning**. A võor yaa tɩ **feature learning tasks** wʋsg pʋgẽ, **samples** wã mod n tara **noise**. **Samples** wã me tara kibay sẽn pa tar yõodo. **Noise** la kibay kãens sẽn pa tar yõod wã tõe n kɩtame tɩ **feature learning** wã pa tʋm sõma ye. Wala makre:

Bɩ d ges **image classification**. **Image** a ye tõe n tara bõn-naands a taab wʋsg a pʋgẽ. D tõe n gesa bõn-kãens wa "**noise**". **Deep Residual Shrinkage Network** wã tõe n tũnuga ne **attention mechanism**. **Network** wã yãta "**noise**" kãngã. Rẽ poore, **network** wã tũnugda ne **soft thresholding** n rɩk **features** nins sẽn yaa "**noise**" rẽnda n lebg zero. Tʋʋm-kãngã tõe n kenga **image classification accuracy** wã.

Bɩ d ges **speech recognition**. Sẽn yɩɩda fãa sã n yaa zĩig sẽn tar **noise** wʋsgo, wala sor noli bɩ **factory** pʋgẽ. **Deep Residual Shrinkage Network** wã tõe n kenga **speech recognition accuracy** wã. Bɩ, **network** wã kõta **methodology**. **Methodology** kãngã tara pãng n na n keng **speech recognition accuracy** wã.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

**Paper** kãngã paama **citations** sẽn yɩɩd 1400 **Google Scholar** zugu.

D sã n ges statistics nins sẽn pa zems zãngã, **researchers** wã tʋma ne **Deep Residual Shrinkage Network (DRSN)** **publications/studies** sẽn yɩɩd 1000 pʋgẽ. Tʋʋm-kãens kẽeda **fields** wʋsg pʋgẽ. **Fields** kãens naaga **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, la **remote sensing**.
