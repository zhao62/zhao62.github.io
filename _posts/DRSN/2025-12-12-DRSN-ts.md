---
layout: post
title: "Deep Residual Shrinkage Network: Endlelo ra Artificial Intelligence ra ti-Data leti nga na Noise yikulu"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-12
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network i variant yo antswisiwa ya Deep Residual Network. Kahle-kahle, Deep Residual Shrinkage Network yi hlanganisa Deep Residual Network, attention mechanisms, na soft thresholding functions."
---

**Deep Residual Shrinkage Network** i variant yo antswisiwa ya **Deep Residual Network**. Kahle-kahle, **Deep Residual Shrinkage Network** yi hlanganisa **Deep Residual Network**, **attention mechanisms**, na **soft thresholding functions**.

Hi nga twisisa matirhelo ya **Deep Residual Shrinkage Network** hi ndlela leyi landzelaka. Xo sungula, **network** yi tirhisa **attention mechanisms** ku vona **unimportant features** (ti-features leti nga riki ta nkoka). Endzhaku, **network** yi tirhisa **soft thresholding functions** ku endla ti-features leti, leti nga riki ta nkoka, ti va zero. Ku hambana na sweswo, **network** yi vona **important features** (ti-features ta nkoka) kutani yi hlayisa ti-features leti. Endlelo leri ri tiyisa matimba ya **deep neural network**. Endlelo leri ri pfuna **network** ku **extract** ti-features leti pfunaka ku suka eka ti-signals leti nga na **noise**.

## 1. Research Motivation

**Xo sungula, noise a yi papalateki loko algorithm yi endla classify ti-samples. Swikombiso swa noise leyi swi katsa Gaussian noise, pink noise, na Laplacian noise.** Hi ku angarhela, ti-samples hakanyingi ti na vuxokoxoko byo karhi lebyi nga yelaniki na **classification task** ya sweswi. Hi nga teka vuxokoxoko lebyi byo ka byi nga yelani tanihi **noise**. **Noise** leyi yi nga hunguta matirhelo ya **classification**. (**Soft thresholding** i goza ra nkoka eka ti-algorithms to tala ta **signal denoising**.)

Hi xikombiso, a hi ehleketeni hi bulo etlhelo ka gondzo. Audio yi nga ha va na mipfumawulo ya ti-honi ta mimovha na mavhilwa. Hi nga ha endla **speech recognition** eka ti-signals leti. Mipfumawulo ya le ndzhaku (background sounds) yi ta khumba vuyelo (results). Ku ya hi mavonelo ya **deep learning**, **deep neural network** yi fanele ku susa ti-features leti yelanaka na ti-honi na mavhilwa. Ku susa loku ku sivela ti-features leti ku khumba vuyelo bya **speech recognition**.

**Xa vumbirhi, mpimo wa noise hakanyingi wa hambana exikarhi ka ti-samples. Ku hambana loku ku humelela hambi ku ri endzeni ka dataset yin’we.** (Ku hambana loku ku na ku fana na ti-**attention mechanisms**. A hi tekeni xikombiso xa **image dataset**. Ndhawu ya **target object** yi nga ha hambana eka ti-images. **Attention mechanisms** ti nga langutisa (focus) eka ndhawu yo karhi ya **target object** eka **image** yin’wana na yin’wana.)

Hi xikombiso, a hi ehleketeni hi ku tren’a (training) **cat-and-dog classifier** hi ti-images ta ntlhanu leti nga na lebuli ya "dog." Image 1 yi nga ha va na mbyana na kondlo. Image 2 yi nga ha va na mbyana na gandaganda. Image 3 yi nga ha va na mbyana na huku. Image 4 yi nga ha va na mbyana na mbhongolo. Image 5 yi nga ha va na mbyana na dhadha. Hi nkarhi wa **training**, swilo leswi nga yelaniki swi ta pfilunganya **classifier**. Swilo leswi swi katsa makondlo, magandaganda, tihuku, tiimbhongolo, na madhadha. Ku pfilunganya loku ku vanga ku ehla ka **classification accuracy**. Loko hi kota ku vona swilo leswi nga yelaniki, hi nga susa ti-features leti yelanaka na swilo leswi. Hi ndlela leyi, hi nga antswisa **accuracy** ya **cat-and-dog classifier**.

## 2. Soft Thresholding

**Soft thresholding i goza ra nkoka eka ti-algorithms to tala ta signal denoising. Algorithm yi susa ti-features loko ti-absolute values ta ti-features teto ti ri hansi ka threshold yo karhi. Algorithm yi hunguta (shrinks) ti-features ti ya eka zero loko ti-absolute values ta ti-features teto ti ri henhla ka threshold leyi.** Valavisisi va nga tirhisa **soft thresholding** hi ku landzelela fomu leyi:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Derivative** ya **output** ya **soft thresholding** mayelana na **input** i:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Fomu leyi nga laha henhla yi kombisa leswaku **derivative** ya **soft thresholding** i 1 kumbe 0. Xihlawulekisi lexi xa fana na xihlawulekisi xa **ReLU activation function**. Hikokwalaho, **soft thresholding** yi nga hunguta khombo ra **gradient vanishing** na **gradient exploding** eka ti-algorithms ta **deep learning**.

**Eka soft thresholding function, ku vekiwa ka threshold ku fanele ku enerisa swipimelo swimbirhi. Xo sungula, threshold yi fanele ku va nomboro ya positive. Xa vumbirhi, threshold a yi fanelanga ku tlula mpimo wa le henhla (maximum value) wa input signal. Loko swi nga ri tano, output yi ta va zero hinkwayo.**

**Ku engetela kwalaho, swi nga va kahle loko threshold yi enerisa xipimelo xa vunharhu. Sample yin’wana na yin’wana yi fanele ku va na threshold ya yona yi ri yoxe (independent) ku ya hi mpimo wa noise eka sample yoleyo.**

Xivangelo hileswaku mpimo wa **noise** hakanyingi wa hambana exikarhi ka ti-samples. Hi xikombiso, Sample A yi nga ha va na **noise** yitsongo kasi Sample B yi na **noise** yinyingi endzeni ka **dataset** yin’we. Eka xiyimo lexi, Sample A yi fanele ku tirhisa **threshold** yitsongo hi nkarhi wa **soft thresholding**. Sample B yi fanele ku tirhisa **threshold** leyikulu. Ti-features leti na ti-thresholds leti ti lahlekeriwa hi nhlamuselo ya tona ya ntiyiso (physical definition) eka **deep neural networks**. Hambiswiritano, logic ya masungulo ya tshama yi ri tano. Hi marito man’wana, **sample** yin’wana na yin’wana yi fanele ku va na **threshold** leyi nga yoxe (independent). Mpimo wa **noise** hi wona wu lawulaka **threshold** leyi.

## 3. Attention Mechanism

Valavisisi va nga twisisa **attention mechanisms** hi ku olova eka nsimu ya **computer vision**. Ti-visual systems ta swiharhi ti kota ku hambanisa swilo (targets) hi ku scan-a hi ku hatlisa ndhawu hinkwayo. Endzhaku ka sweswo, ti-visual systems ti veka **attention** eka **target object**. Xiendlo lexi xi pfuna ti-systems ku **extract** vuxokoxoko byo tala (more details). Hi nkarhi lowu fanaka, ti-systems ti kandziyela (suppress) vuxokoxoko lebyi nga yelaniki. Eka vuxokoxoko, hi kombela mi languta tibuku leti vulavulaka hi **attention mechanisms**.

**Squeeze-and-Excitation Network (SENet)** yi yimela ndlela yintshwa ya **deep learning** leyi tirhisaka **attention mechanisms**. Eka ti-samples to hambana, ti-**feature channels** to hambana ti na xiave xo hambana eka **classification task**. **SENet** yi tirhisa **sub-network** yitsongo ku kuma **a set of weights**. Kutani, **SENet** yi andzisa (multiplies) ti-weights leti na ti-features ta ti-channels teto. Xiendlo lexi xi lulamisa mpimo wa ti-features eka **channel** yin’wana na yin’wana. Hi nga teka endlelo leri tanihi **Apply weighting to each feature channel** hi ku hambana.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Hi ndlela leyi, **sample** yin’wana na yin’wana yi na **a set of weights** leyi nga yoxe (independent). Hi marito man’wana, ti-weights ta ti-samples timbirhi tihi na tihi ti hambanile. Eka **SENet**, ndlela yo kuma ti-weights i "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

**Deep Residual Shrinkage Network** yi tirhisa xivumbeko xa **SENet sub-network**. **Network** yi tirhisa xivumbeko lexi ku endla **soft thresholding** ehansi ka **deep attention mechanism**. **Sub-network** (leyi kombisiweke eka bokisi ro tshwuka) i **Learn a set of thresholds**. Endzhaku, **network** yi tirhisa **soft thresholding** eka **feature channel** yin’wana na yin’wana hi ku tirhisa ti-thresholds leti.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Eka **sub-network** leyi, system yi rhanga hi ku hlayela ti-absolute values ta ti-features hinkwato eka **input feature map**. Endzhaku, system yi endla **global average pooling** na **averaging** ku kuma **feature**, leyi hi yi vitanaka A. Eka ndlela leyin’wana, system yi nghenisa **feature map** eka **fully connected network** yitsongo endzhaku ka **global average pooling**. **Fully connected network** leyi yi tirhisa **Sigmoid function** tanihi **layer** yo hetelela. **Function** leyi yi endla **normalize** ya **output** yi va exikarhi ka 0 na 1. Endlelo leri ri humesa coefficient, leyi hi yi vitanaka α. Hi nga tsala **threshold** ya makumu tanihi α × A. Hikokwalaho, **threshold** i vuyelo bya ku andzisiwa ka tinomboro timbirhi. Nomboro yin’wana yi le xikarhi ka 0 na 1. Nomboro leyin’wana i average ya ti-absolute values ta **feature map**. **Endlelo leri ri tiyisekisa leswaku threshold i positive. Endlelo leri ri tlhela ri tiyisekisa leswaku threshold a yi kuli ku tlula mpimo.**

**Ku engetela kwalaho, ti-samples to hambana ti humesa ti-thresholds to hambana. Hikwalaho, hi nga twisisa endlelo leri tanihi attention mechanism yo hlawuleka. Mechanism leyi yi vona ti-features leti nga yelaniki na task ya sweswi. Mechanism leyi yi cinca ti-features leti ti va values leti nga ekusuhi na zero hi ku tirhisa ti-convolutional layers timbirhi. Kutani, mechanism yi endla ti-features leti ti va zero hi ku tirhisa soft thresholding. Kumbe hi nga ku, mechanism yi vona ti-features leti yelanaka na task ya sweswi. Mechanism yi cinca ti-features leti ti va values leti nga ekule na zero hi ku tirhisa ti-convolutional layers timbirhi. Xo hetelela, mechanism yi hlayisa ti-features leti.**

Xo hetelela, hi endla **Stack many basic modules** hi xithandza xo karhi. Hi tlhela hi nghenisa ti-convolutional layers, batch normalization, activation functions, global average pooling, na ti-fully connected output layers. Endlelo leri ri aka **Deep Residual Shrinkage Network** leyi heleleke.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

**Deep Residual Shrinkage Network** i endlelo ra **general** ra **feature learning**. Xivangelo hileswaku, eka mintirho yo tala ya **feature learning**, ti-samples hakanyingi ti na **noise**. Ti-samples ti tlhela ti va na vuxokoxoko byo ka byi nga yelani. **Noise** leyi na vuxokoxoko byo ka byi nga yelani swi nga ha khumba matirhelo ya **feature learning**. Hi xikombiso:

A hi languteni **image classification**. **Image** yi nga ha va na swilo swin’wana swo tala hi nkarhi wun’we. Hi nga twisisa swilo leswi tanihi "**noise**." **Deep Residual Shrinkage Network** yi nga ha kota ku tirhisa **attention mechanism**. **Network** yi xiya "**noise**" leyi. Endzhaku, **network** yi tirhisa **soft thresholding** ku endla ti-features leti yelanaka na "**noise**" leyi ti va zero. Xiendlo lexi xi nga ha antswisa **accuracy** ya **image classification**.

A hi languteni **speech recognition**. Ngopfu-ngopfu, a hi languteni swiyimo swa **noise** leyi nga yiki, yo fana na mabulo etlhelo ka gondzo kumbe endzeni ka workshop ya fektri. **Deep Residual Shrinkage Network** yi nga ha antswisa **accuracy** ya **speech recognition**. Kumbe hi katsongo, **network** yi nyika endlelo (methodology). Endlelo leri ri kota ku antswisa **accuracy** ya **speech recognition**.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Phepha leri (Paper) ri kumile ku tlula 1,400 wa ti-citations eka Google Scholar.

Hi ku ya hi tinhlayo leti nga helelangiki, valavisisi va tirhisile **Deep Residual Shrinkage Network (DRSN)** eka nkandziyiso/dyondzo (publications/studies) wo tlula 1,000. Matirhiselo lawa ya katsa tindhawu to tala. Tindhawu leti ti katsa **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, na **remote sensing**.
