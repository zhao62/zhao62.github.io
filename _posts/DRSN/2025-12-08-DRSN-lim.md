---
layout: post
title: "Deep Residual Shrinkage Network: 'n Artificial Intelligence Methode veur Highly Noisy Data"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-08
tags: [Deep Learning, AI]
mathjax: true
---

**'t **Deep Residual Shrinkage Network** is 'n verbeterde variant van 't **Deep Residual Network**. In essentie integreert 't **Deep Residual Shrinkage Network** 't **Deep Residual Network**, **attention mechanisms**, en **soft thresholding functions**.**

**V'r kinne 't werkingsprincipe van 't **Deep Residual Shrinkage Network** op de volgende maneer begriepe. Ieës gebroek 't netwerk **attention mechanisms** um onbelangrieke **features** te identificere. Dan gebroek 't netwerk **soft thresholding functions** um dees onbelangrieke **features** op nul te zitte. Aan de angere kant identificeert 't netwerk belangrieke **features** en behauwt dees. Dit proces versterk de capaciteit van 't **deep neural network**. Dit proces help 't netwerk um nuttige **features** te extrahere oet signale die **noise** bevatte.**

## 1. Ongerzeuksmotivatie (Research Motivation)

**Ten ieësjte is **noise** neet te vermieje wienie 't algoritme **samples** classificeert. Veurbeelder van dees **noise** zeen **Gaussian noise**, **pink noise**, en **Laplacian noise**.** Mieë algemein gezag, bevatte **samples** dök informatie die irrelevant is veur de huidige **classification task**. V'r kinne dees irrelevante informatie interpreteren as **noise**. Dees **noise** kin de **classification performance** verminjere. (**Soft thresholding** is 'n sleutelsjtap in väöl **signal denoising algorithms**.)

Nump ens e gesprek langs de weeg as veurbeeld. De audio kin geluide van **car horns** en **wheels** bevatte. V'r zouwe mesjien **speech recognition** wille oetveure op dees signale. De achtergrondgeluide zulle onvermijdelijk de resultate beïnvleuje. Oet 't perspectief van **deep learning**, zou 't **deep neural network** de **features** motte eliminere die euvereinkomme mit de toeters en wiele. Dees eliminatie veurkump tot de **features** de **speech recognition** resultate beïnvleuje.

**Ten tweïde varieert de hoeveelheid **noise** dök tösse de **samples**. Dees variatie gebeurt zelfs binne dezelfde **dataset**.** (Dees variatie deilt geliekenisse mit **attention mechanisms**. Nump 'n **image dataset** as veurbeeld. De locatie van 't doelobject kin versjille per plaatje. **Attention mechanisms** kinne focuse op de specifieke locatie van 't doelobject in eder plaatje.)

Sjtel uuch veur, v'r **trainen** 'n **cat-and-dog classifier** mit vief plaatjes die gelabeld zeen as "hond". **Image** 1 bevat mesjien 'nen hond en 'n moes. **Image** 2 bevat mesjien 'nen hond en 'n gans. **Image** 3 bevat mesjien 'nen hond en 'n hin. **Image** 4 bevat mesjien 'nen hond en 'nen ezel. **Image** 5 bevat mesjien 'nen hond en 'n eend. Tiedes 't **training** proces zulle irrelevante objecte de **classifier** sjtuure. Dees objecte omvatten muus, ganze, hinne, ezele, en eende. Dees sjtuuring resulteert in 'n aafname van de **classification accuracy**. Sjtèl tot v'r dees irrelevante objecte kinne identificere. Dan kinne v'r de **features** eliminere die mit dees objecte euvereinkomme. Op dees maneer kinne v'r de **accuracy** van de **cat-and-dog classifier** verbetere.

## 2. Soft Thresholding (Soft Thresholding)

**Soft thresholding is 'n kernsjtap in väöl **signal denoising algorithms**. 't Algoritme elimineert **features** es de absolute waardes van de **features** lieger zeen as 'n bepaalde **threshold**. 't Algoritme krimp (**shrinks**) **features** nao nul toe es de absolute waardes van de **features** hoeger zeen as dees **threshold**.** Ongerzeukers kinne **soft thresholding** implementere mit de volgende formule:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

De afgeleide (**derivative**) van de **soft thresholding** output ten opzichte van de input is:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

De formule hiejbove toent aan tot de afgeleide van **soft thresholding** ofwaal 1 of 0 is. Dees eigenschap is identiek aan de eigenschap van de **ReLU activation function**. Daorum kin **soft thresholding** 't risico op **gradient vanishing** en **gradient exploding** in **deep learning algorithms** verminjere.

**In de **soft thresholding function**, mot de insjtelling van de **threshold** aan twieë veurwaardes voldoon. Ten ieësjte mot de **threshold** 'n positief getal zeen. Ten tweïde maag de **threshold** neet groeter zeen as de maximale waarde van 't input signaal. Angesj zal de output volledig nul zeen.**

**Daoneve zou de **threshold** bij veurkeur aan 'n derde veurwaarde motte voldoon. Eder **sample** zou zien eige onafhankelijke **threshold** motte höbbe, gebaseerd op de **noise content** van dat **sample**.**

De reie is tot de **noise content** dök varieert tösse **samples**. Beveurbeeld, **Sample** A kin minder **noise** bevatte, terwiel **Sample** B mieë **noise** bevat binne dezelfde **dataset**. In dit geval zou **Sample** A 'n kleindere **threshold** motte gebroeke tiedes **soft thresholding**. **Sample** B zou 'n groetere **threshold** motte gebroeke. Dees **features** en **thresholds** verleze hun expliciete fysieke definities in **deep neural networks**. Aevel, de basislogica blief 't zelfde. Mit angere wäörd, eder **sample** zou 'n onafhankelijke **threshold** motte höbbe. De specifieke **noise content** bepaalt dees **threshold**.

## 3. Attention Mechanism (Attention Mechanism)

Ongerzeukers kinne **attention mechanisms** gemekkelik begriepe in 't veld van **computer vision**. De visuele systeme van dere kinne doelen ondersjeie door sjnel 't ganse gebied te scanne. Vervolges focuse de visuele systeme hun **attention** op 't doelobject. Dees actie stelt de systeme in sjtaot um mieë details te extrahere. Tegeliekertied ongerdrukke de systeme irrelevante informatie. Veur details, verwieze v'r nao literatuur euver **attention mechanisms**.

't **Squeeze-and-Excitation Network (SENet)** vertegenwoordigt 'n relatief nuuj **deep learning** methode die **attention mechanisms** gebroek. Euver versjillende **samples** heen, drage versjillende **feature channels** versjillend bij aan de **classification task**. **SENet** gebroek 'n klein sub-netwerk um 'n set van gewichte (**weights**) te kriege. Dan vermenigvuldigt **SENet** dees gewichte mit de **features** van de respectievelijke **channels**. Dees operatie pas de grootte van de **features** in eder **channel** aan. V'r kinne dit proces zeen as 't toepasse van variërende niveaus van **attention** op versjillende **feature channels**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

In dees aanpak bezit eder **sample** 'n onafhankelijke set van gewichte. Mit angere wäörd, de gewichte veur willekeurig welke twieë **samples** zeen versjillend. In **SENet** is 't specifieke paad veur 't kriege van gewichte: "**Global Pooling** → **Fully Connected Layer** → **ReLU Function** → **Fully Connected Layer** → **Sigmoid Function**".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding mit Deep Attention Mechanism (Soft Thresholding with Deep Attention Mechanism)

't **Deep Residual Shrinkage Network** gebroek de structuur van 't **SENet** sub-netwerk. 't Netwerk gebroek dees structuur um **soft thresholding** te implementere onger 'n **deep attention mechanism**. 't Sub-netwerk (aangeduid binne de roeje does) liert 'n set van drempelwaardes, ofwaal **Learn a set of thresholds**. Dan past 't netwerk **soft thresholding** toe op eder **feature channel** mit behulp van dees **thresholds**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

In dit sub-netwerk berekent 't systeem ieës de absolute waardes van alle **features** in de **input feature map**. Dan veurt 't systeem **global average pooling** en middeling oet um 'n **feature** te kriege, genoteerd as $A$. In 't angere paad (de **Identity path**), veurt 't systeem de **feature map** in in 'n klein **fully connected network** nao **global average pooling**. Dit **fully connected network** gebroek de **Sigmoid function** as de lètste laog. Dees functie normaliseert de output tösse 0 en 1. Dit proces lievert 'n coëfficiënt op, genoteerd as $\alpha$. V'r kinne de uiteindelijke **threshold** oetdrukke as $\alpha \times A$. Daorum is de **threshold** 't product van twieë getalle. Ein getal lik tösse 0 en 1. 't Anger getal is 't gemiddelde van de absolute waardes van de **feature map**. **Dees methode verzekert tot de **threshold** positief is. Dees methode verzekert ouch tot de **threshold** neet excessief groet is.**

**Bovendeen resultere versjillende **samples** in versjillende **thresholds**. Gevolgellik kinne v'r dees methode interpreteren as 'n gespecialiseerd **attention mechanism**. 't Mechanisme identificeert **features** die irrelevant zeen veur de huidige taak. 't Mechanisme transformeert dees **features** nao waardes dich bij nul via twieë **convolutional layers**. Dan zèt 't mechanisme dees **features** op nul mit behulp van **soft thresholding**. Of angesjom, 't mechanisme identificeert **features** die relevant zeen veur de huidige taak. 't Mechanisme transformeert dees **features** nao waardes wied van nul via twieë **convolutional layers**. Uiteindelijk behauwt 't mechanisme dees **features**.**

Tot sjlot, **Stack many basic modules** (sjtapelen v'r 'n aantal basismodules). V'r vuge ouch **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, en **fully connected output layers** toe. Dit proces construeert 't complete **Deep Residual Shrinkage Network**. Zoewie in de figure getoend, gebroeke v'r **Apply weighting to each feature channel** en **Learn a set of weights** as ongerdeil van 't proces.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalisatiecapaciteit (Generalization Capability)

't **Deep Residual Shrinkage Network** is 'n algemein methode veur **feature learning**. De reie is tot **samples** dök **noise** bevatte in väöl **feature learning tasks**. **Samples** bevatte ouch irrelevante informatie. Dees **noise** en irrelevante informatie kinne de prestaties van **feature learning** beïnvleuje. Beveurbeeld:

Dink aan **image classification**. 'n Plaatje kin tegeliekertied väöl angere objecte bevatte. V'r kinne dees objecte begriepe as "**noise**". 't **Deep Residual Shrinkage Network** is mesjien in sjtaot um 't **attention mechanism** te gebroeke. 't Netwerk merk dees "**noise**" op. Dan gebroek 't netwerk **soft thresholding** um de **features** die euvereinkomme mit dees "**noise**" op nul te zitte. Dees actie verbetert meugelik de **image classification accuracy**.

Dink aan **speech recognition**. Specifiek, dink aan relatief ruzige omgevinge zoewie gesprekssettinge langs de weeg of binne in 'n febriekshal. 't **Deep Residual Shrinkage Network** kin de **speech recognition accuracy** verbetere. Of op z'n minste, 't netwerk biejt 'n methodologie. Dees methodologie is in sjtaot um **speech recognition accuracy** te verbetere.

## Referentie (Reference)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academische Impact (Academic Impact)

Dit artikel haet mieë as 1.400 citaties ontvange op Google Scholar.

Gebaseerd op onvolledige statistieke, höbbe ongerzeukers 't **Deep Residual Shrinkage Network (DRSN)** toegepas in mieë as 1.000 publicaties/studies. Dees toepassinge dekke 'n breed scala aan velden. Dees velden omvatten **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, en **remote sensing**.
