---
layout: post
title: "Deep Residual Shrinkage Network: 'n Artificial Intelligence Methode veur Highly Noisy Data"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-08
tags: [Deep Learning, AI]
mathjax: true
---

<strong>'t <strong>Deep Residual Shrinkage Network</strong> is 'n verbeterde variant van 't <strong>Deep Residual Network</strong>. In essentie integreert 't <strong>Deep Residual Shrinkage Network</strong> 't <strong>Deep Residual Network</strong>, <strong>attention mechanisms</strong>, en <strong>soft thresholding functions</strong>.</strong>

<strong>V'r kinne 't werkingsprincipe van 't <strong>Deep Residual Shrinkage Network</strong> op de volgende maneer begriepe. Ieës gebroek 't netwerk <strong>attention mechanisms</strong> um onbelangrieke <strong>features</strong> te identificere. Dan gebroek 't netwerk <strong>soft thresholding functions</strong> um dees onbelangrieke <strong>features</strong> op nul te zitte. Aan de angere kant identificeert 't netwerk belangrieke <strong>features</strong> en behauwt dees. Dit proces versterk de capaciteit van 't <strong>deep neural network</strong>. Dit proces help 't netwerk um nuttige <strong>features</strong> te extrahere oet signale die <strong>noise</strong> bevatte.</strong>

## 1. Ongerzeuksmotivatie (Research Motivation)

<strong>Ten ieësjte is <strong>noise</strong> neet te vermieje wienie 't algoritme <strong>samples</strong> classificeert. Veurbeelder van dees <strong>noise</strong> zeen <strong>Gaussian noise</strong>, <strong>pink noise</strong>, en <strong>Laplacian noise</strong>.</strong> Mieë algemein gezag, bevatte <strong>samples</strong> dök informatie die irrelevant is veur de huidige <strong>classification task</strong>. V'r kinne dees irrelevante informatie interpreteren as <strong>noise</strong>. Dees <strong>noise</strong> kin de <strong>classification performance</strong> verminjere. (<strong>Soft thresholding</strong> is 'n sleutelsjtap in väöl <strong>signal denoising algorithms</strong>.)

Nump ens e gesprek langs de weeg as veurbeeld. De audio kin geluide van <strong>car horns</strong> en <strong>wheels</strong> bevatte. V'r zouwe mesjien <strong>speech recognition</strong> wille oetveure op dees signale. De achtergrondgeluide zulle onvermijdelijk de resultate beïnvleuje. Oet 't perspectief van <strong>deep learning</strong>, zou 't <strong>deep neural network</strong> de <strong>features</strong> motte eliminere die euvereinkomme mit de toeters en wiele. Dees eliminatie veurkump tot de <strong>features</strong> de <strong>speech recognition</strong> resultate beïnvleuje.

<strong>Ten tweïde varieert de hoeveelheid <strong>noise</strong> dök tösse de <strong>samples</strong>. Dees variatie gebeurt zelfs binne dezelfde <strong>dataset</strong>.</strong> (Dees variatie deilt geliekenisse mit <strong>attention mechanisms</strong>. Nump 'n <strong>image dataset</strong> as veurbeeld. De locatie van 't doelobject kin versjille per plaatje. <strong>attention mechanisms</strong> kinne focuse op de specifieke locatie van 't doelobject in eder plaatje.)

Sjtel uuch veur, v'r <strong>trainen</strong> 'n <strong>cat-and-dog classifier</strong> mit vief plaatjes die gelabeld zeen as "hond". <strong>Image</strong> 1 bevat mesjien 'nen hond en 'n moes. <strong>Image</strong> 2 bevat mesjien 'nen hond en 'n gans. <strong>Image</strong> 3 bevat mesjien 'nen hond en 'n hin. <strong>Image</strong> 4 bevat mesjien 'nen hond en 'nen ezel. <strong>Image</strong> 5 bevat mesjien 'nen hond en 'n eend. Tiedes 't <strong>training</strong> proces zulle irrelevante objecte de <strong>classifier</strong> sjtuure. Dees objecte omvatten muus, ganze, hinne, ezele, en eende. Dees sjtuuring resulteert in 'n aafname van de <strong>classification accuracy</strong>. Sjtèl tot v'r dees irrelevante objecte kinne identificere. Dan kinne v'r de <strong>features</strong> eliminere die mit dees objecte euvereinkomme. Op dees maneer kinne v'r de <strong>accuracy</strong> van de <strong>cat-and-dog classifier</strong> verbetere.

## 2. Soft Thresholding (Soft Thresholding)

<strong>Soft thresholding is 'n kernsjtap in väöl <strong>signal denoising algorithms</strong>. 't Algoritme elimineert <strong>features</strong> es de absolute waardes van de <strong>features</strong> lieger zeen as 'n bepaalde <strong>threshold</strong>. 't Algoritme krimp (<strong>shrinks</strong>) <strong>features</strong> nao nul toe es de absolute waardes van de <strong>features</strong> hoeger zeen as dees <strong>threshold</strong>.</strong> Ongerzeukers kinne <strong>soft thresholding</strong> implementere mit de volgende formule:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

De afgeleide (**derivative**) van de **soft thresholding** output ten opzichte van de input is:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

De formule hiejbove toent aan tot de afgeleide van **soft thresholding** ofwaal 1 of 0 is. Dees eigenschap is identiek aan de eigenschap van de **ReLU activation function**. Daorum kin **soft thresholding** 't risico op **gradient vanishing** en **gradient exploding** in **deep learning algorithms** verminjere.

**In de **soft thresholding function**, mot de insjtelling van de **threshold** aan twieë veurwaardes voldoon. Ten ieësjte mot de **threshold** 'n positief getal zeen. Ten tweïde maag de **threshold** neet groeter zeen as de maximale waarde van 't input signaal. Angesj zal de output volledig nul zeen.**

**Daoneve zou de **threshold** bij veurkeur aan 'n derde veurwaarde motte voldoon. Eder **sample** zou zien eige onafhankelijke **threshold** motte höbbe, gebaseerd op de **noise content** van dat **sample**.**

De reie is tot de **noise content** dök varieert tösse **samples**. Beveurbeeld, **Sample** A kin minder **noise** bevatte, terwiel **Sample** B mieë **noise** bevat binne dezelfde **dataset**. In dit geval zou **Sample** A 'n kleindere **threshold** motte gebroeke tiedes **soft thresholding**. **Sample** B zou 'n groetere **threshold** motte gebroeke. Dees **features** en **thresholds** verleze hun expliciete fysieke definities in **deep neural networks**. Aevel, de basislogica blief 't zelfde. Mit angere wäörd, eder **sample** zou 'n onafhankelijke **threshold** motte höbbe. De specifieke **noise content** bepaalt dees **threshold**.

## 3. Attention Mechanism (Attention Mechanism)

Ongerzeukers kinne **attention mechanisms** gemekkelik begriepe in 't veld van **computer vision**. De visuele systeme van dere kinne doelen ondersjeie door sjnel 't ganse gebied te scanne. Vervolges focuse de visuele systeme hun **attention** op 't doelobject. Dees actie stelt de systeme in sjtaot um mieë details te extrahere. Tegeliekertied ongerdrukke de systeme irrelevante informatie. Veur details, verwieze v'r nao literatuur euver **attention mechanisms**.

't **Squeeze-and-Excitation Network (SENet)** vertegenwoordigt 'n relatief nuuj **deep learning** methode die **attention mechanisms** gebroek. Euver versjillende **samples** heen, drage versjillende **feature channels** versjillend bij aan de **classification task**. **SENet** gebroek 'n klein sub-netwerk um 'n set van gewichte (**weights**) te kriege. Dan vermenigvuldigt **SENet** dees gewichte mit de **features** van de respectievelijke **channels**. Dees operatie pas de grootte van de **features** in eder **channel** aan. V'r kinne dit proces zeen as 't toepasse van variërende niveaus van **attention** op versjillende **feature channels**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

In dees aanpak bezit eder **sample** 'n onafhankelijke set van gewichte. Mit angere wäörd, de gewichte veur willekeurig welke twieë **samples** zeen versjillend. In **SENet** is 't specifieke paad veur 't kriege van gewichte: "**Global Pooling** → **Fully Connected Layer** → **ReLU Function** → **Fully Connected Layer** → **Sigmoid Function**".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding mit Deep Attention Mechanism (Soft Thresholding with Deep Attention Mechanism)

't **Deep Residual Shrinkage Network** gebroek de structuur van 't **SENet** sub-netwerk. 't Netwerk gebroek dees structuur um **soft thresholding** te implementere onger 'n **deep attention mechanism**. 't Sub-netwerk (aangeduid binne de roeje does) liert 'n set van drempelwaardes, ofwaal **Learn a set of thresholds**. Dan past 't netwerk **soft thresholding** toe op eder **feature channel** mit behulp van dees **thresholds**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

In dit sub-netwerk berekent 't systeem ieës de absolute waardes van alle **features** in de **input feature map**. Dan veurt 't systeem **global average pooling** en middeling oet um 'n **feature** te kriege, genoteerd as *A*. In 't angere paad (de **Identity path**), veurt 't systeem de **feature map** in in 'n klein **fully connected network** nao **global average pooling**. Dit **fully connected network** gebroek de **Sigmoid function** as de lètste laog. Dees functie normaliseert de output tösse 0 en 1. Dit proces lievert 'n coëfficiënt op, genoteerd as *α*. V'r kinne de uiteindelijke **threshold** oetdrukke as *α × A*. Daorum is de **threshold** 't product van twieë getalle. Ein getal lik tösse 0 en 1. 't Anger getal is 't gemiddelde van de absolute waardes van de **feature map**. **Dees methode verzekert tot de **threshold** positief is. Dees methode verzekert ouch tot de **threshold** neet excessief groet is.**

**Bovendeen resultere versjillende **samples** in versjillende **thresholds**. Gevolgellik kinne v'r dees methode interpreteren as 'n gespecialiseerd **attention mechanism**. 't Mechanisme identificeert **features** die irrelevant zeen veur de huidige taak. 't Mechanisme transformeert dees **features** nao waardes dich bij nul via twieë **convolutional layers**. Dan zèt 't mechanisme dees **features** op nul mit behulp van **soft thresholding**. Of angesjom, 't mechanisme identificeert **features** die relevant zeen veur de huidige taak. 't Mechanisme transformeert dees **features** nao waardes wied van nul via twieë **convolutional layers**. Uiteindelijk behauwt 't mechanisme dees **features**.**

Tot sjlot, **Stack many basic modules** (sjtapelen v'r 'n aantal basismodules). V'r vuge ouch **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, en **fully connected output layers** toe. Dit proces construeert 't complete **Deep Residual Shrinkage Network**. Zoewie in de figure getoend, gebroeke v'r **Apply weighting to each feature channel** en **Learn a set of weights** as ongerdeil van 't proces.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalisatiecapaciteit (Generalization Capability)

't **Deep Residual Shrinkage Network** is 'n algemein methode veur **feature learning**. De reie is tot **samples** dök **noise** bevatte in väöl **feature learning tasks**. **Samples** bevatte ouch irrelevante informatie. Dees **noise** en irrelevante informatie kinne de prestaties van **feature learning** beïnvleuje. Beveurbeeld:

Dink aan **image classification**. 'n Plaatje kin tegeliekertied väöl angere objecte bevatte. V'r kinne dees objecte begriepe as "**noise**". 't **Deep Residual Shrinkage Network** is mesjien in sjtaot um 't **attention mechanism** te gebroeke. 't Netwerk merk dees "**noise**" op. Dan gebroek 't netwerk **soft thresholding** um de **features** die euvereinkomme mit dees "**noise**" op nul te zitte. Dees actie verbetert meugelik de **image classification accuracy**.

Dink aan **speech recognition**. Specifiek, dink aan relatief ruzige omgevinge zoewie gesprekssettinge langs de weeg of binne in 'n febriekshal. 't **Deep Residual Shrinkage Network** kin de **speech recognition accuracy** verbetere. Of op z'n minste, 't netwerk biejt 'n methodologie. Dees methodologie is in sjtaot um **speech recognition accuracy** te verbetere.

## Referentie (Reference)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academische Impact (Academic Impact)

Dit artikel haet mieë as 1.400 citaties ontvange op Google Scholar.

Gebaseerd op onvolledige statistieke, höbbe ongerzeukers 't **Deep Residual Shrinkage Network (DRSN)** toegepas in mieë as 1.000 publicaties/studies. Dees toepassinge dekke 'n breed scala aan velden. Dees velden omvatten **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, en **remote sensing**.
