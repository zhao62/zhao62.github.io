---
layout: post
title: "Deep Residual Shrinkage Network: 'n Artificial Intelligence Methode veur Highly Noisy Data"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-08
tags: [Deep Learning, AI]
mathjax: true
---

<strong>'t <strong>Deep Residual Shrinkage Network</strong> is 'n verbeterde variant van 't <strong>Deep Residual Network</strong>. In essentie integreert 't <strong>Deep Residual Shrinkage Network</strong> 't <strong>Deep Residual Network</strong>, <strong>attention mechanisms</strong>, en <strong>soft thresholding functions</strong>.</strong>

<strong>V'r kinne 't werkingsprincipe van 't <strong>Deep Residual Shrinkage Network</strong> op de volgende maneer begriepe. Ieës gebroek 't netwerk <strong>attention mechanisms</strong> um onbelangrieke <strong>features</strong> te identificere. Dan gebroek 't netwerk <strong>soft thresholding functions</strong> um dees onbelangrieke <strong>features</strong> op nul te zitte. Aan de angere kant identificeert 't netwerk belangrieke <strong>features</strong> en behauwt dees. Dit proces versterk de capaciteit van 't <strong>deep neural network</strong>. Dit proces help 't netwerk um nuttige <strong>features</strong> te extrahere oet signale die <strong>noise</strong> bevatte.</strong>

## 1. Ongerzeuksmotivatie (Research Motivation)

<strong>Ten ieësjte is <strong>noise</strong> neet te vermieje wienie 't algoritme <strong>samples</strong> classificeert. Veurbeelder van dees <strong>noise</strong> zeen <strong>Gaussian noise</strong>, <strong>pink noise</strong>, en <strong>Laplacian noise</strong>.</strong> Mieë algemein gezag, bevatte <strong>samples</strong> dök informatie die irrelevant is veur de huidige <strong>classification task</strong>. V'r kinne dees irrelevante informatie interpreteren as <strong>noise</strong>. Dees <strong>noise</strong> kin de <strong>classification performance</strong> verminjere. (<strong>Soft thresholding</strong> is 'n sleutelsjtap in väöl <strong>signal denoising algorithms</strong>.)

Nump ens e gesprek langs de weeg as veurbeeld. De audio kin geluide van <strong>car horns</strong> en <strong>wheels</strong> bevatte. V'r zouwe mesjien <strong>speech recognition</strong> wille oetveure op dees signale. De achtergrondgeluide zulle onvermijdelijk de resultate beïnvleuje. Oet 't perspectief van <strong>deep learning</strong>, zou 't <strong>deep neural network</strong> de <strong>features</strong> motte eliminere die euvereinkomme mit de toeters en wiele. Dees eliminatie veurkump tot de <strong>features</strong> de <strong>speech recognition</strong> resultate beïnvleuje.

<strong>Ten tweïde varieert de hoeveelheid <strong>noise</strong> dök tösse de <strong>samples</strong>. Dees variatie gebeurt zelfs binne dezelfde <strong>dataset</strong>.</strong> (Dees variatie deilt geliekenisse mit <strong>attention mechanisms</strong>. Nump 'n <strong>image dataset</strong> as veurbeeld. De locatie van 't doelobject kin versjille per plaatje. <strong>attention mechanisms</strong> kinne focuse op de specifieke locatie van 't doelobject in eder plaatje.)

Sjtel uuch veur, v'r <strong>trainen</strong> 'n <strong>cat-and-dog classifier</strong> mit vief plaatjes die gelabeld zeen as "hond". <strong>Image</strong> 1 bevat mesjien 'nen hond en 'n moes. <strong>Image</strong> 2 bevat mesjien 'nen hond en 'n gans. <strong>Image</strong> 3 bevat mesjien 'nen hond en 'n hin. <strong>Image</strong> 4 bevat mesjien 'nen hond en 'nen ezel. <strong>Image</strong> 5 bevat mesjien 'nen hond en 'n eend. Tiedes 't <strong>training</strong> proces zulle irrelevante objecte de <strong>classifier</strong> sjtuure. Dees objecte omvatten muus, ganze, hinne, ezele, en eende. Dees sjtuuring resulteert in 'n aafname van de <strong>classification accuracy</strong>. Sjtèl tot v'r dees irrelevante objecte kinne identificere. Dan kinne v'r de <strong>features</strong> eliminere die mit dees objecte euvereinkomme. Op dees maneer kinne v'r de <strong>accuracy</strong> van de <strong>cat-and-dog classifier</strong> verbetere.

## 2. Soft Thresholding (Soft Thresholding)

<strong>Soft thresholding is 'n kernsjtap in väöl <strong>signal denoising algorithms</strong>. 't Algoritme elimineert <strong>features</strong> es de absolute waardes van de <strong>features</strong> lieger zeen as 'n bepaalde <strong>threshold</strong>. 't Algoritme krimp (<strong>shrinks</strong>) <strong>features</strong> nao nul toe es de absolute waardes van de <strong>features</strong> hoeger zeen as dees <strong>threshold</strong>.</strong> Ongerzeukers kinne <strong>soft thresholding</strong> implementere mit de volgende formule:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

De afgeleide (<strong>derivative</strong>) van de <strong>soft thresholding</strong> output ten opzichte van de input is:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

De formule hiejbove toent aan tot de afgeleide van <strong>soft thresholding</strong> ofwaal 1 of 0 is. Dees eigenschap is identiek aan de eigenschap van de <strong>ReLU activation function</strong>. Daorum kin <strong>soft thresholding</strong> 't risico op <strong>gradient vanishing</strong> en <strong>gradient exploding</strong> in <strong>deep learning algorithms</strong> verminjere.

<strong>In de <strong>soft thresholding function</strong>, mot de insjtelling van de <strong>threshold</strong> aan twieë veurwaardes voldoon. Ten ieësjte mot de <strong>threshold</strong> 'n positief getal zeen. Ten tweïde maag de <strong>threshold</strong> neet groeter zeen as de maximale waarde van 't input signaal. Angesj zal de output volledig nul zeen.</strong>

<strong>Daoneve zou de <strong>threshold</strong> bij veurkeur aan 'n derde veurwaarde motte voldoon. Eder <strong>sample</strong> zou zien eige onafhankelijke <strong>threshold</strong> motte höbbe, gebaseerd op de <strong>noise content</strong> van dat <strong>sample</strong>.</strong>

De reie is tot de <strong>noise content</strong> dök varieert tösse <strong>samples</strong>. Beveurbeeld, <strong>Sample</strong> A kin minder <strong>noise</strong> bevatte, terwiel <strong>Sample</strong> B mieë <strong>noise</strong> bevat binne dezelfde <strong>dataset</strong>. In dit geval zou <strong>Sample</strong> A 'n kleindere <strong>threshold</strong> motte gebroeke tiedes <strong>soft thresholding</strong>. <strong>Sample</strong> B zou 'n groetere <strong>threshold</strong> motte gebroeke. Dees <strong>features</strong> en <strong>thresholds</strong> verleze hun expliciete fysieke definities in <strong>deep neural networks</strong>. Aevel, de basislogica blief 't zelfde. Mit angere wäörd, eder <strong>sample</strong> zou 'n onafhankelijke <strong>threshold</strong> motte höbbe. De specifieke <strong>noise content</strong> bepaalt dees <strong>threshold</strong>.

## 3. Attention Mechanism (Attention Mechanism)

Ongerzeukers kinne <strong>attention mechanisms</strong> gemekkelik begriepe in 't veld van <strong>computer vision</strong>. De visuele systeme van dere kinne doelen ondersjeie door sjnel 't ganse gebied te scanne. Vervolges focuse de visuele systeme hun <strong>attention</strong> op 't doelobject. Dees actie stelt de systeme in sjtaot um mieë details te extrahere. Tegeliekertied ongerdrukke de systeme irrelevante informatie. Veur details, verwieze v'r nao literatuur euver <strong>attention mechanisms</strong>.

't <strong>Squeeze-and-Excitation Network (SENet)</strong> vertegenwoordigt 'n relatief nuuj <strong>deep learning</strong> methode die <strong>attention mechanisms</strong> gebroek. Euver versjillende <strong>samples</strong> heen, drage versjillende <strong>feature channels</strong> versjillend bij aan de <strong>classification task</strong>. <strong>SENet</strong> gebroek 'n klein sub-netwerk um 'n set van gewichte (<strong>weights</strong>) te kriege. Dan vermenigvuldigt <strong>SENet</strong> dees gewichte mit de <strong>features</strong> van de respectievelijke <strong>channels</strong>. Dees operatie pas de grootte van de <strong>features</strong> in eder <strong>channel</strong> aan. V'r kinne dit proces zeen as 't toepasse van variërende niveaus van <strong>attention</strong> op versjillende <strong>feature channels</strong>.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

In dees aanpak bezit eder <strong>sample</strong> 'n onafhankelijke set van gewichte. Mit angere wäörd, de gewichte veur willekeurig welke twieë <strong>samples</strong> zeen versjillend. In <strong>SENet</strong> is 't specifieke paad veur 't kriege van gewichte: "<strong>Global Pooling</strong> → <strong>Fully Connected Layer</strong> → <strong>ReLU Function</strong> → <strong>Fully Connected Layer</strong> → <strong>Sigmoid Function</strong>".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding mit Deep Attention Mechanism (Soft Thresholding with Deep Attention Mechanism)

't <strong>Deep Residual Shrinkage Network</strong> gebroek de structuur van 't <strong>SENet</strong> sub-netwerk. 't Netwerk gebroek dees structuur um <strong>soft thresholding</strong> te implementere onger 'n <strong>deep attention mechanism</strong>. 't Sub-netwerk (aangeduid binne de roeje does) liert 'n set van drempelwaardes, ofwaal <strong>Learn a set of thresholds</strong>. Dan past 't netwerk <strong>soft thresholding</strong> toe op eder <strong>feature channel</strong> mit behulp van dees <strong>thresholds</strong>.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

In dit sub-netwerk berekent 't systeem ieës de absolute waardes van alle <strong>features</strong> in de <strong>input feature map</strong>. Dan veurt 't systeem <strong>global average pooling</strong> en middeling oet um 'n <strong>feature</strong> te kriege, genoteerd as *A*. In 't angere paad (de <strong>Identity path</strong>), veurt 't systeem de <strong>feature map</strong> in in 'n klein <strong>fully connected network</strong> nao <strong>global average pooling</strong>. Dit <strong>fully connected network</strong> gebroek de <strong>Sigmoid function</strong> as de lètste laog. Dees functie normaliseert de output tösse 0 en 1. Dit proces lievert 'n coëfficiënt op, genoteerd as *α*. V'r kinne de uiteindelijke <strong>threshold</strong> oetdrukke as *α × A*. Daorum is de <strong>threshold</strong> 't product van twieë getalle. Ein getal lik tösse 0 en 1. 't Anger getal is 't gemiddelde van de absolute waardes van de <strong>feature map</strong>. <strong>Dees methode verzekert tot de <strong>threshold</strong> positief is. Dees methode verzekert ouch tot de <strong>threshold</strong> neet excessief groet is.</strong>

<strong>Bovendeen resultere versjillende <strong>samples</strong> in versjillende <strong>thresholds</strong>. Gevolgellik kinne v'r dees methode interpreteren as 'n gespecialiseerd <strong>attention mechanism</strong>. 't Mechanisme identificeert <strong>features</strong> die irrelevant zeen veur de huidige taak. 't Mechanisme transformeert dees <strong>features</strong> nao waardes dich bij nul via twieë <strong>convolutional layers</strong>. Dan zèt 't mechanisme dees <strong>features</strong> op nul mit behulp van <strong>soft thresholding</strong>. Of angesjom, 't mechanisme identificeert <strong>features</strong> die relevant zeen veur de huidige taak. 't Mechanisme transformeert dees <strong>features</strong> nao waardes wied van nul via twieë <strong>convolutional layers</strong>. Uiteindelijk behauwt 't mechanisme dees <strong>features</strong>.</strong>

Tot sjlot, <strong>Stack many basic modules</strong> (sjtapelen v'r 'n aantal basismodules). V'r vuge ouch <strong>convolutional layers</strong>, <strong>batch normalization</strong>, <strong>activation functions</strong>, <strong>global average pooling</strong>, en <strong>fully connected output layers</strong> toe. Dit proces construeert 't complete <strong>Deep Residual Shrinkage Network</strong>. Zoewie in de figure getoend, gebroeke v'r <strong>Apply weighting to each feature channel</strong> en <strong>Learn a set of weights</strong> as ongerdeil van 't proces.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalisatiecapaciteit (Generalization Capability)

't <strong>Deep Residual Shrinkage Network</strong> is 'n algemein methode veur <strong>feature learning</strong>. De reie is tot <strong>samples</strong> dök <strong>noise</strong> bevatte in väöl <strong>feature learning tasks</strong>. <strong>Samples</strong> bevatte ouch irrelevante informatie. Dees <strong>noise</strong> en irrelevante informatie kinne de prestaties van <strong>feature learning</strong> beïnvleuje. Beveurbeeld:

Dink aan <strong>image classification</strong>. 'n Plaatje kin tegeliekertied väöl angere objecte bevatte. V'r kinne dees objecte begriepe as "<strong>noise</strong>". 't <strong>Deep Residual Shrinkage Network</strong> is mesjien in sjtaot um 't <strong>attention mechanism</strong> te gebroeke. 't Netwerk merk dees "<strong>noise</strong>" op. Dan gebroek 't netwerk <strong>soft thresholding</strong> um de <strong>features</strong> die euvereinkomme mit dees "<strong>noise</strong>" op nul te zitte. Dees actie verbetert meugelik de <strong>image classification accuracy</strong>.

Dink aan <strong>speech recognition</strong>. Specifiek, dink aan relatief ruzige omgevinge zoewie gesprekssettinge langs de weeg of binne in 'n febriekshal. 't <strong>Deep Residual Shrinkage Network</strong> kin de <strong>speech recognition accuracy</strong> verbetere. Of op z'n minste, 't netwerk biejt 'n methodologie. Dees methodologie is in sjtaot um <strong>speech recognition accuracy</strong> te verbetere.

## Referentie (Reference)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academische Impact (Academic Impact)

Dit artikel haet mieë as 1.400 citaties ontvange op Google Scholar.

Gebaseerd op onvolledige statistieke, höbbe ongerzeukers 't <strong>Deep Residual Shrinkage Network (DRSN)</strong> toegepas in mieë as 1.000 publicaties/studies. Dees toepassinge dekke 'n breed scala aan velden. Dees velden omvatten <strong>mechanical engineering</strong>, <strong>electrical power</strong>, <strong>vision</strong>, <strong>healthcare</strong>, <strong>speech</strong>, <strong>text</strong>, <strong>radar</strong>, en <strong>remote sensing</strong>.
