---
layout: post
title: "Deep Residual Shrinkage Network: बहुत ज्यादा Noisy Data बर एक Artificial Intelligence Method"
date: 2025-12-10
tags: [Deep Learning, AI]
mathjax: true
description: ""
---

**Deep Residual Shrinkage Network** ह **Deep Residual Network** के एक सुधरे हुए रूप (**improved variant**) हरय। असल म, **Deep Residual Shrinkage Network** ह **Deep Residual Network**, **attention mechanisms**, अउ **soft thresholding functions** ल एक संग मिलाथे।

हमन **Deep Residual Shrinkage Network** के काम करे के तरीका ल अइसने समझ सकथन। सबले पहली, यह **network** ह **attention mechanisms** के उपयोग करके बेकार (**unimportant**) **features** ल पहिचानथे। फिर, यह **network** ह **soft thresholding functions** लगा के इन बेकार **features** ल **zero** सेट कर देथे। वइसने ही, **network** ह जरूरी (**important**) **features** ल पहिचानथे अउ इन जरूरी **features** ल बचा के राखथे। यह **process** ह **deep neural network** के क्षमता ल बढ़ा देथे। यह **process** ह **network** ल **noise** वाले **signals** से काम के **features** निकाले म मदद करथे।

## 1. **Research Motivation** के बारे म

**सबले पहली बात, जब algorithm ह samples ल classify करथे, त noise तो रहिबेच करथे (is inevitable)। Noise के उदाहरण म Gaussian noise, pink noise, अउ Laplacian noise शामिल हे।** थोड़ा अउ विस्तार से बोले त, **samples** म अक्सर अइसन जानकारी होथे जेहर अभी के **classification task** बर काम के नहीं होय। हमन ई बेकार जानकारी ल **noise** मान सकथन। यह **noise** ह **classification performance** ल कम कर सकथे। (**Soft thresholding** ह बहुत अक **signal denoising algorithms** म एक मुख्य **step** होथे।)

जैसे कि, सड़क किनारे के बातचीत ल सोचा। **Audio** म गाड़ी के **horn** अउ चक्का के आवाज आ सकथे। हमन शायद इन **signals** पे **speech recognition** करबो। ये पीछे के आवाज (**background sounds**) ह **results** ल जरूर खराब करही। **Deep learning** के नजरिये से देखे त, **deep neural network** ल **horn** अउ चक्का वाले **features** ल हटा देना चाही। अइसन करे से यह **features** ह **speech recognition results** ल खराब नई कर पाये।

**दूसरी बात, अलग-अलग samples म noise के मात्रा (amount) अक्सर अलग-अलग होथे। यह अंतर एक ही dataset के भीतर भी हो सकथे।** (यह चीज **attention mechanisms** संग मिलती-जुलती हे। एक **image dataset** के उदाहरण ले। **Target object** के जगह अलग-अलग **images** म अलग-अलग हो सकथे। **Attention mechanisms** ह हर **image** म **target object** के सही जगह पर ध्यान लगा सकथे।)

उदाहरण के लिए, मान ले कि हमन एक कुत्ता-बिल्ली (**cat-and-dog**) **classifier** ल **train** करत हन जिमा 5 **images** हे जिनकर **label** "dog" हे। **Image 1** म एक कुत्ता अउ एक मुसवा (**mouse**) हो सकथे। **Image 2** म एक कुत्ता अउ एक हंस (**goose**) हो सकथे। **Image 3** म एक कुत्ता अउ एक कुकड़ा (**chicken**) हो सकथे। **Image 4** म एक कुत्ता अउ एक गधा (**donkey**) हो सकथे। **Image 5** म एक कुत्ता अउ एक बतख (**duck**) हो सकथे। **Training** के समय, बेकार चीजे **classifier** ल **disturb** करही। इन चीजन म मुसवा, हंस, कुकड़ा, गधा, अउ बतख शामिल हे। इस गड़बड़ी के कारण **classification accuracy** कम हो जाही। अगर हमन इन बेकार चीजन ल पहिचान ले, त फिर हमन इन चीजन वाले **features** ल हटा सकथन। इस तरीके से, हमन कुत्ता-बिल्ली **classifier** के **accuracy** ल सुधार सकथन।

## 2. **Soft Thresholding**

**Soft thresholding ह बहुत अक signal denoising algorithms के core step हरय। अगर features के absolute values एक निश्चित threshold से कम हे, त algorithm इन features ल हटा देथे (eliminates)। अगर features के absolute values इस threshold से ज्यादा हे, त algorithm इन features ल zero के तरफ सिकोड़ (shrinks) देथे।** **Researchers** मन नीचे दिए गए **formula** से **soft thresholding** ल लागू कर सकथे:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Input** के हिसाब से **soft thresholding output** के **derivative** हे:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

ऊपर वाला **formula** दिखथे कि **soft thresholding** के **derivative** या तो 1 हे या फिर 0 हे। यह गुण (**property**) बिल्कुल **ReLU activation function** के जैसे हे। इसलिए, **soft thresholding** ह **deep learning algorithms** म **gradient vanishing** अउ **gradient exploding** के खतरा ल कम कर सकथे।

**Soft thresholding function म, threshold सेट करते समय दुइ ठन शर्त मानना जरूरी हे। पहली, threshold ह positive number होना चाही। दूसरी, threshold ह input signal के maximum value से ज्यादा नई होना चाही। नई ते, पूरा output ह zero हो जाही।**

**संगी हो, threshold बर एक तीसरा शर्त भी होना चाही। हर sample के पास, ओ sample के noise content के आधार पे, अपन खुद के independent threshold होना चाही।**

काहे कि, अलग-अलग **samples** म **noise** के मात्रा अक्सर अलग होथे। जैसे कि, एक ही **dataset** म **Sample A** म कम **noise** हो सकथे जबकि **Sample B** म ज्यादा **noise** हो सकथे। अइसन स्थिति म, **soft thresholding** करते समय **Sample A** बर छोटा **threshold** **use** करना चाही। **Sample B** बर बड़ा **threshold** **use** करना चाही। **Deep neural networks** म, भले ही इन **features** अउ **thresholds** के कोई स्पष्ट भौतिक (**physical**) मतलब न बचे, लेकिन मूल **logic** वइसने ही रहथे। मतलब कि, हर **sample** बर एक **independent threshold** होना चाही। **Noise content** ही इस **threshold** ल तय करथे।

## 3. **Attention Mechanism**

**Computer vision** के क्षेत्र म **Researchers** मन **attention mechanisms** ल आसानी से समझ सकथे। जानवर मन के **visual systems** ह पूरा इलाका ल जल्दी से **scan** करके **targets** ल पहिचान सकथे। उसके बाद, **visual systems** ह **target object** पर ध्यान (**attention**) लगाथे। यह हरकत **systems** ल अउ ज्यादा बारीकी (**details**) निकाले म मदद करथे। साथ ही साथ, **systems** ह बेकार जानकारी ल दबा देथे। इसके बारे म और जानने बर कृपया **attention mechanisms** वाले **literature** ल पढ़ो।

**Squeeze-and-Excitation Network (SENet)** एक नवा **deep learning method** हे जोन **attention mechanisms** के उपयोग करथे। अलग-अलग **samples** म, अलग-अलग **feature channels** ह **classification task** म अलग-अलग योगदान देथे। **SENet** एक छोटा **sub-network** **use** करथे ताकि वो **Learn a set of weights** कर सके। फिर, **SENet** इन **weights** ल उनके **channels** के **features** से गुणा (**multiplies**) कर देथे। यह काम हर **channel** म **features** के **size** ल **adjust** कर देथे। हमन इस **process** ल अलग-अलग **feature channels** पर अलग-अलग **attention** लगाने के रूप म देख सकथन (**Apply weighting to each feature channel**)।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

इस तरीके म, हर **sample** के पास **weights** के एक **independent set** होथे। मतलब कि, कोनो भी दुइ **samples** के **weights** अलग-अलग होथे। **SENet** म, **weights** पाये के रस्ता हे: "**Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function**."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. **Deep Attention Mechanism** के साथ **Soft Thresholding**

**Deep Residual Shrinkage Network** ह **SENet sub-network** के **structure** **use** करथे। **Network** इस **structure** के उपयोग करके **deep attention mechanism** के अंतर्गत **soft thresholding** ल लागू करथे। यह **sub-network** (लाल डिब्बे म दिखाये गये) एक **set of thresholds** सीखथे (**Learn a set of thresholds**)। फिर, **network** इन **thresholds** के उपयोग करके हर **feature channel** पर **soft thresholding** लगाथे।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

इस **sub-network** म, **system** सबले पहली **input feature map** के सभी **features** के **absolute values calculate** करथे। फिर, **system** ह **global average pooling** अउ **averaging** करथे ताकि एक **feature** मिल सके, जेला *A* माना गे हे। दूसरी तरफ, **system** ह **global average pooling** के बाद **feature map** ल एक छोटे **fully connected network** म डाल देथे। यह **fully connected network** आखिरी **layer** के रूप म **Sigmoid function** **use** करथे। यह **function** ह **output** ल 0 अउ 1 के बीच म **normalize** कर देथे। इस **process** से एक **coefficient** मिलथे, जेला *α* माना गे हे। हमन **final threshold** ल *α × A* बता सकथन। इसलिए, **threshold** ह दुइ **number** के गुणा (**product**) हरय। एक **number** 0 अउ 1 के बीच हे। दूसरा **number** **feature map** के **absolute values** के **average** हे। **यह तरीका सुनिश्चित करथे कि threshold positive रहय। यह तरीका यह भी पक्का करथे कि threshold बहुत ज्यादा बड़ा न होय।**

**अउ तो अउ, अलग-अलग samples से अलग-अलग thresholds बनथे। नतीजन, हमन इस तरीके ल एक विशेष attention mechanism समझ सकथन। यह mechanism ह अभी के task बर बेकार features ल पहिचानथे। यह mechanism दुइ convolutional layers के जरिये इन features ल 0 के करीब बदल देथे। फिर, यह mechanism ह soft thresholding के उपयोग से इन features ल zero सेट कर देथे। या फिर, यह mechanism अभी के task बर जरूरी features ल पहिचानथे। यह mechanism दुइ convolutional layers के जरिये इन features ल 0 से दूर बदल देथे। आखिरी म, यह mechanism इन features ल बचा के राखथे।**

सबले आखिरी म, हमन कुछ **Stack many basic modules** करथन। हमन **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, अउ **fully connected output layers** ल भी शामिल करथन। यह **process** ह पूरा **Deep Residual Shrinkage Network** बनाथे।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. **Generalization Capability**

**Deep Residual Shrinkage Network** ह **feature learning** बर एक **general** तरीका हे। कारण यह हे कि बहुत अक **feature learning tasks** म **samples** म अक्सर **noise** होथे। **samples** म बेकार जानकारी भी होथे। यह **noise** अउ बेकार जानकारी ह **feature learning** के **performance** ल खराब कर सकथे। उदाहरण के लिए:

**Image classification** ल सोचा। एक **image** म एक साथ कई दूसरे **objects** हो सकथे। हमन इन **objects** ल "**noise**" समझ सकथन। **Deep Residual Shrinkage Network** शायद **attention mechanism** के उपयोग कर सके। **Network** इस "**noise**" ल ध्यान देथे। फिर, **network** ह **soft thresholding** **use** करके इस "**noise**" वाले **features** ल **zero** सेट कर देथे। यह हरकत **image classification accuracy** ल बढ़ा सकथे।

**Speech recognition** ल सोचा। खास तौर पे जब माहौल बहुत शोर-शराबे वाला हो, जैसे सड़क किनारे या **factory** के भीतर बातचीत। **Deep Residual Shrinkage Network** ह **speech recognition accuracy** ल सुधार सकथे। या कम से कम, **network** एक तरीका (**methodology**) प्रदान करथे। यह तरीका **speech recognition accuracy** ल बढ़ाये म सक्षम हे।

## **Reference**

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## **Academic Impact** (प्रभाव)

यह **paper** ल **Google Scholar** पर 1400 से ज्यादा **citations** मिल चुके हे।

अधूरे आंकड़ों (**statistics**) के मुताबिक, **researchers** मन 1000 से ज्यादा **publications/studies** म **Deep Residual Shrinkage Network (DRSN)** के उपयोग करे हे। यह **applications** बहुत सारे क्षेत्रों (**fields**) म फैला हे। इन क्षेत्रों म **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, अउ **remote sensing** शामिल हे।
