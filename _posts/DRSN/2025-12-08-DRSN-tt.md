---
layout: post
title: "Tirän Qaldıq Qısqartu Çeltäre (Deep Residual Shrinkage Network): Köçle Şaw-şulı Data öçen Yasal intellekt Isulu"
date: 2025-12-08
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network – ul Deep Residual Network arxitekturasınıñ yaxşırtılğan variantı. Töp ayırması şunda: Deep Residual Shrinkage Network üz eçenä Deep Residual Network, Attention mechanism häm Soft thresholding funksiälären berläşterä."
---

**Deep Residual Shrinkage Network – ul Deep Residual Network arxitekturasınıñ yaxşırtılğan variantı. Töp ayırması şunda: Deep Residual Shrinkage Network üz eçenä Deep Residual Network, Attention mechanism häm Soft thresholding funksiälären berläşterä.**

**Äydägez, Deep Residual Shrinkage Network niçek eşlägänen qarap kitik. Bere kileş, çeltär Attention mechanism yärdämendä möhim bulmağan üzençäleklärne (features) anıqlıy. Annarı, Soft thresholding funksiäse qullanıp, bu kiräksez üzençäleklärne nolga (zero) äyländerä. Kiresençä, çeltär möhim üzençäleklärne anıqlıy häm alarnı saqlap qala. Bu protsess Deep Neural Network mömkinleklären arttıra häm şaw-şulı (noisy) signallardan faydalı mäğlümätne ayırıp alırğa yärdäm itä.**

## 1. Tikşerenü motivatsiäse

**Berencedän, algoritm ürnäklärne (samples) klassifikatsiälägändä, şaw-şu (noise) qotılgısız närsä. Mäsälän: Gaussian noise, Pink noise häm Laplacian noise.** Kiñräk planda alğanda, ürnäklärdä yış qına xäzerge klassifikatsiä mäsyäläse öçen kiräksez mäğlümät bula. Bez bu kiräksez mäğlümätne "şaw-şu" (noise) dip atıy alabız. Bu şaw-şu klassifikatsiä sıyfatın kimetergä mömkin. (**Soft thresholding** – küpçelek signal çistartu algoritmı öçen bik möhim adım.)

Mäsälän, uram çitendäge söyläşüne küz aldına kiteregez. Audio yazmada maşina signalları häm tägärmäç tawışları bulırğa mömkin. Bez bu signallarğa **speech recognition** yasarğa tırışabız di. Bu fon tawışları näticägä naçar täesir itäçäk. **Deep learning** küzlegennän qarağanda, **Deep Neural Network** maşina häm tägärmäç tawışlarına turı kilgän üzençäleklärne (features) yuq itärgä tiyeş. Bu yuq itü, şul üzençäleklärneñ **speech recognition** näticäsenä täesir itüenä yul quymıy.

**Ikençedän, här ürnäktäge şaw-şu küläme yış qına törleçä bula. Bu üzgäreş xätta ber ük dataset eçendä dä küzätelä.** (Bu üzgäreş **Attention mechanism** belän oxşaşlıqqa iä. Ber **image dataset** misalın alıyq. Törle räsemnärdä maqsatlı obyekt törle urında urnaşırğa mömkin. **Attention mechanism** här räsemdäge maqsatlı obyekt urnaşqan konkret urınğa iğtibar itä ala.)

Mäsälän, "it" (dog) dip bilgelängän biş räsem belän pesi-it klassifikatorın öyrätü protsessın qarıyq.
*   Räsem 1: it häm tıçqan.
*   Räsem 2: it häm qaz.
*   Räsem 3: it häm tawıq.
*   Räsem 4: it häm işäk.
*   Räsem 5: it häm ürdäk.

Öyrätü (training) waqıtında, kiräksez obyektlar klassifikatorğa qomaçawlıy. Bu obyektlar: tıçqan, qaz, tawıq, işäk häm ürdäk. Bu qomaçawlaw klassifikatsiä tögällegen kimetä. Ägär bez bu kiräksez obyektlarnı anıqlıy alsaq, bez şul obyektlarğa turı kilgän üzençäleklärne (features) yuq itä alabız. Şulay itep, bez pesi-it klassifikatorınıñ tögällegen arttıra alabız.

## 2. Soft Thresholding

**Soft thresholding – signalnı şaw-şudan çistartu algoritmınıñ töp öleşe (core step). Ägär üzençäleklärneñ absolut zurlığı bilgele ber buyisadan (threshold) keçeräk bulsa, algoritm alarnı yuq itä. Ägär üzençäleklärneñ absolut zurlığı bu thresholdtan zurraq bulsa, algoritm alarnı nolga taba "qısqarta" (shrinks).** Tikşerenüçelär **Soft thresholding** ğämälen tübändäge formula yärdämendä başqara ala:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

**Soft thresholding** çığışınıñ kerügä qarata çığarılması (derivative):

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Yuğarıdağı formula kürsätkänçä, **Soft thresholding** funksiäseneñ çığarılması 1 yäki 0 bula. Bu üzençälek **ReLU activation function** üzençälege belän täñgäl kilä. Şuña kürä, **Soft thresholding** qullanu **deep learning** algoritmnarında **gradient vanishing** häm **gradient exploding** riskın kimetä ala.

**Soft thresholding funksiäsendä thresholdnı (tau) bilgeläw ike şartqa turı kilergä tiyeş. Berencedän, threshold uñay san (positive number) bulırğa tiyeş. Ikençedän, threshold kerü signalınıñ maksimal qimmätennän artırğa tiyeş tügel. Yuğisä, çığış (output) tulısınça nol bulaçaq.**

**Şulay uq, threshold öçençe şartqa da turı kilsä yaxşıraq. Här ürnäk (sample), üzeneñ şaw-şu eçtälegenä qarap, şäxsi bäysez thresholdqa iä bulırğa tiyeş.**

Moneñ säbäbe şunda: ürnäklärneñ şaw-şu küläme yış qına törleçä bula. Mäsälän, ber ük dataset eçendä A ürnägendä azraq şaw-şu, ä B ürnägendä kübräk şaw-şu bulırğa mömkin. Bu oçraqta, **Soft thresholding** waqıtında A ürnäge keçeräk threshold qullanırğa tiyeş, ä B ürnäge zurraq threshold qullanırğa tiyeş. **Deep Neural Network** eçendä bu üzençäleklär häm thresholdlar üzläreneñ açıq fizik mäğnäsen yuğaltalar. Läkin, töp mantıq (logic) üzgäreşsez qala. Yäğni, här ürnäk bäysez thresholdqa iä bulırğa tiyeş. Häm bu thresholdnı konkret şaw-şu eçtälege bilgeliy.

## 3. Attention Mechanism

Tikşerenüçelär **computer vision** ölkäsendä **Attention mechanism** prinsipın ciñel añlıy ala. Xaywannarnıñ kürü sisteması barlıq ölkäne tiz küzätep çığu yärdämendä maqsatlarnı ayıra ala. Annarı, kürü sisteması iğtibarnı (attention) maqsatlı obyektqa yünältä. Bu ğämäl sistemaga tağın da kübräk detallarnı alırğa mömkinlek birä. Şul uq waqıtta, sistema kiräksez mäğlümätne basıla (suppress).

**Squeeze-and-Excitation Network (SENet)** – **Attention mechanism** qullanğan çağıştırmaça yaña **deep learning** ısulu. Törle ürnäklärdä (samples), klassifikatsiä mäsyäläsenä törle **feature channel**lar törleçä öleş kertä. **SENet** keçe ber **sub-network** yärdämendä ber törkem wazınnar (weights) ala. Annarı **SENet** bu wazınnarnı tiyeşle kanallarnıñ üzençäleklärenä tapqırlıy. Bu operatsiä här kanaldağı üzençäleklärneñ zurlığın köyli. Bez bu protsessnı törle **feature channel**larğa törle däräcädäge **attention** qullanu dip qarıy alabız.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Bu ısulda här ürnäk bäysez wazınnar (weights) cıyılmasına iä. Başqaça äytkändä, teläsä qaysı ike ürnäkneñ wazınnarı törleçä. **SENet** eçendä wazınnarnı alu yulı mondıy: "**Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function**".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Deep Attention Mechanism yärdämendä Soft Thresholding

**Deep Residual Shrinkage Network** çeltäre **SENet sub-network** strukturasın qullana. Çeltär bu strukturanı **Deep Attention mechanism** şartlarında **Soft thresholding** ğämälgä aşıru öçen faydalana. Bu **sub-network** (qızıl ramka eçendä kürsätelgän) ber törkem thresholdlar öyränä (**Learn a set of thresholds**). Annarı, çeltär bu thresholdlarnı qullanıp, här **feature channel** öçen **Soft thresholding** yasıy.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Bu **sub-network** eçendä, sistema başqa kerü **feature map**ındağı barlıq üzençäleklärneñ absolut qimmätlären isäpli. Annarı, sistema **Global Average Pooling** häm urtaça qimmätne alu yulı belän A dip bilgelängän üzençälekne ala. Başqa yulda (path), sistema **Global Average Pooling**dan soñ **feature map**nı keçe **fully connected network**qa kertä. Bu **fully connected network** soñğı qatlam (layer) bularaq **Sigmoid function** qullana. Bu funksiä çığışnı (output) 0 häm 1 arasına **normalize** itä. Bu protsess α dip bilgelängän koeffitsiyentnı birä. Bez soñğı thresholdnı α×A dip bilgeli alabız. Şulay itep, threshold ike sannıñ tapqırçığışına tigez. Ber san 0 häm 1 arasında. Ikençe san – **feature map**nıñ absolut qimmätläreneñ urtaçası. **Bu ısul thresholdnıñ uñay (positive) san buluın täemin itä. Şulay uq, bu ısul thresholdnıñ artıq zur bulmawın da garantíli.**

**Möxime şunda: törle ürnäklär (samples) törle thresholdlar birä. Şuña kürä, bez bu ısulnı maxsus Attention mechanism dip añlıy alabız. Bu mexanizm xäzerge mäsyälä öçen kiräksez üzençäleklärne anıqlıy. Mexanizm ike convolutional layer yärdämendä bu üzençäleklärne 0-gä yaqın qimmätlärgä äyländerä. Annarı, mexanizm Soft thresholding qullanıp, bu üzençäleklärne tulısınça nolga tigezli. Yäki kiresençä, mexanizm xäzerge mäsyälä öçen möhim üzençäleklärne anıqlıy. Mexanizm ike convolutional layer yärdämendä bu üzençäleklärne 0-dän yıraq qimmätlärgä äyländerä. Häm soñğı çiktä, mexanizm bu üzençäleklärne saqlap qala.**

Axırda, bez bilgele ber sandagı töp modullarnı öyäbez (**Stack many basic modules**). Bez şulay uq **convolutional layers**, **Batch Normalization**, **activation functions**, **Global Average Pooling** häm **fully connected output layers** qullanabız. Bu protsess tulı **Deep Residual Shrinkage Network** strukturasın tözä.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Ğomumiläterü mömkinlege (Generalization Capability)

**Deep Residual Shrinkage Network** – ul **feature learning** öçen universal (general) ısul. Çönki küpçelek oçraqta **feature learning** mäsyälärendä ürnäklär (samples) şaw-şu (noise) üz eçenä ala. Ürnäklär şulay uq kiräksez mäğlümätne dä üz eçenä ala. Bu şaw-şu häm kiräksez mäğlümät **feature learning** näticäsenä naçar täesir itergä mömkin. Mäsälän:

**Image classification** (räsemnärne klassifikatsiäläw) oçrağın qarıyq. Ber räsemdä ber ük waqıtta küp kenä başqa obyektlar bulırğa mömkin. Bez bu obyektlarnı "şaw-şu" dip añlıy alabız. **Deep Residual Shrinkage Network**, **Attention mechanism** qullanıp, bu "şaw-şu"nı kürep ala. Annarı çeltär **Soft thresholding** yärdämendä şul "şaw-şu"ğa turı kilgän üzençäleklärne nolga äyländerä. Bu ğämäl **image classification** tögällegen arttırırğa mömkin.

**Speech recognition** (söyläwçene tanu) oçrağın qarıyq. Ayıruça uram çitendä yäki zavod sexı eçendäge söyläşü kebek şaw-şulı moxitnı küzdä totıq. **Deep Residual Shrinkage Network** **speech recognition** tögällegen arttırırğa mömkin. Yäki, kim digändä, çeltär yaña metodologiä täqdim itä. Bu metodologiä **speech recognition** tögällegen arttıru sälätendä iä.

## Ädäbiät (Reference)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Akademik yoğıntı (Academic Impact)

Bu xezmät Google Scholar platformasında 1400-dän artıq sitata (citations) cıydı.

Tulı bulmağan statistika buyınça, tikşerenüçelär **Deep Residual Shrinkage Network (DRSN)** algoritın 1000-nän artıq basma/tikşerenüdä qullandı. Bu qullanışlar bik kiñ ölkälärne qaplıy. Bu ölkälärgä mexanik injeneriä, elektr energiäse, computer vision, sälamätlek saqlaw, söyläw (speech), tekst, radar häm remote sensing kerä.
