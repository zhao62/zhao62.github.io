---
layout: post
title: "<div style='text-align: center; direction: rtl;'>Deep Residual Shrinkage Network: Highly Noisy Data خٲطرٕ اکھ Artificial Intelligence Method</div>"
subtitle: "<div style='text-align: center; direction: ltr;'>An Artificial Intelligence Method for Highly Noisy Data</div>"
date: 2025-12-04
tags: [Deep Learning, AI, DRSN]
mathjax: true
lang: ks
dir: rtl
---

<!-- CSS Styling -->
<style>
  /* 1. Force Headers Right */
  h1, h2, h3, h4, h5, h6 { text-align: right !important; }
  
  /* 2. Force Lists Right */
  ul, ol { text-align: right !important; direction: rtl !important; padding-right: 40px; padding-left: 0; }

  /* 3. Force Code LTR */
  pre, code { text-align: left !important; direction: ltr !important; }
  
  /* 4. Force References LTR */
  .references-ltr { text-align: left !important; direction: ltr !important; }
</style>

<!-- Main Content Wrapper -->
<div dir="rtl" markdown="1" style="text-align: right; direction: rtl; font-family: 'Noto Nastaliq Urdu', 'Sakkal Majalla', 'Traditional Arabic', serif; font-size: 1.1em; line-height: 1.8;">

<strong>Deep Residual Shrinkage Network چھُ دراصل Deep Residual Network ہُک اکھ بہتر Variant۔ اصلس منز چھُ یہِ Deep Residual Network، Attention Mechanisms تہٕ Soft Thresholding Functions ہُک مجموعہ (Integration)۔</strong>

<strong>کینہہ حد تک، Deep Residual Shrinkage Network ہٕچ کٲم کرنٕچ صلاحیت ہیکو أسۍ یہِ سمجھتھ کہ: یہِ چھُ Attention Mechanisms ہُک استعمال کرتھ غیر ضروری (unimportant) Features شناخت کران تہٕ Soft Thresholding Functions ذریعے تمن Zero سیٹ (set) کران؛ یا بییہِ طرفہٕ، یہِ چھُ ضروری Features شناخت کران تہٕ تمن برقرار (retain) تھاوان۔ یہِ عمل چھُ Deep Neural Network کِس Noise والین Signals منزٕ مفید Features نیرہٕ کڈنس منز مدد کران۔</strong>

## 1. Research Motivation

<strong>گۄڈٕ، ییلہِ أسۍ Samples کِہ Classifying چھِ کران، Noise مثلاً Gaussian Noise، Pink Noise تہٕ Laplacian Noise آسُن چھُ لازمی۔</strong> وسیع معنن منز، Samples منز ہیکن اکثر یِژھ معلومات آسِتھ یوس موجودہ Classification Task سٟتۍ متعلق چھنہٕ آسان، تہٕ اتھ تہِ ہیکو أسۍ Noise ونِتھ۔ یہِ Noise ہیکہِ Classification Performance پؠٹھ غلط اثر تراوِتھ۔ (Soft thresholding چھُ واریاہن Signal Denoising Algorithms منز اکھ اہم قدم۔)

مثالے، سڑکِ کنارے کتھ باتھ کرنہٕ وزِ ہیکہِ آوازِ منز گاڑین ہٕندِ ہارن یا ٹایرن ہٕنز آواز مِلِتھ گژھتھ۔ ییلہِ یمن Signals ہٕنز Speech Recognition کرنہٕ ییہِ، تہٕ نتیجہِ گژھہِ ضرور خراب۔ Deep Learning کِس نظریہ سٟتۍ، ہارن تہٕ ٹایرن ہٕندۍ Features گژھن Deep Neural Network منز ختم (Eliminate) کرنہٕ ینہٕ تاکہ تم Speech Recognition کِس نتیجس خراب نہٕ کرِن۔

<strong>دۄیم، اکی Dataset منز تہِ چھُ الگ الگ Samples منز Noise amount اکثر مختلف آسان۔</strong> (یہِ چھُ Attention Mechanisms سٟتۍ رلان؛ اگر اکھ Image Dataset وچھو، مختلف تصویرن منز ہیکہِ Target Object مختلف جاین پؠٹھ آسِتھ، تہٕ Attention Mechanisms ہیکن پرؠتھ تصویرِ منز Target Object کِس خاص لوکیشنس (Location) پؠٹھ توجہ دِتھ۔)

مثالے، اگر اسۍ کیٹ-اینڈ-ڈاگ (Cat-and-Dog) Classifier ٹرین (train) کرو، تہٕ "Dog" لیبل (Label) والی 5 تصویر چھِ۔ گۄڈنِچہِ تصویرِ منز ہیکہِ ہُن تہٕ گگُر آسِتھ، دۄیمس منز ہُن تہٕ آنٛز، تریمس منز ہُن تہٕ کوکر، څوٗرمس منز ہُن تہٕ خر، تہٕ پنجمس منز ہُن تہٕ بتُکھ۔ Training دوران گژھہِ Classifier یمن غیر متعلق چیزن (Irrelevant Objects) مثلاً گگُر، آنٛز، کوکر، خر تہٕ بتُکھ سٟتۍ Distub، تہٕ Classification Accuracy گژھہِ کم۔ اگر اسۍ یمن غیر ضروری چیزن—گگُر، آنٛز، کوکر، خر تہٕ بتُکھ—شناخت کٔرِتھ تمن ہٕندۍ Features ختم کٔرو، تیلہِ ہیکہِ Cat-and-Dog Classifier ہٕچ Accuracy بہتر گژھتھ۔

## 2. Soft Thresholding

<strong>Soft thresholding چھُ واریاہن Signal Denoising Algorithms ہُک اکھ اہم حصہ۔ یہِ چھُ تمن Features ختم کران یمن ہُنٛد Absolute Value اکہِ خاص Threshold کھوتہٕ کم چھُ آسان، تہٕ یمن ہُنٛد Absolute Value اتھ Threshold کھوتہٕ زیادہ چھُ آسان تمن چھُ یہِ Zero کُن Shrink کران۔</strong> یہِ ہیکو أسۍ یتھ Formula سٟتۍ لاگو کٔرِتھ:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Soft thresholding output ہُک Derivative، Input کِس حسابہٕ چھُ یہِ:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

ہیری دِینہٕ آمُت، Soft thresholding ہُک Derivative چھُ یا تہِ 1 آسان یا 0۔ یہِ خصلت چھِ ReLU Activation Function ہیو۔ لہذا، Soft thresholding سٟتۍ چھُ Deep Learning Algorithms منز Gradient Vanishing تہٕ Gradient Exploding ہُک خطرہ کم گژھان۔

<strong>Soft thresholding function منز، Threshold سیٹ (set) کرنہٕ وزِ گژھن زٕ شرطہٕ پورٕ گژھنۍ: گۄڈٕ، Threshold گژھہِ Positive (مثبت) آسُن؛ دۄیم، Threshold گژھہِ نہٕ Input Signal کِس Maximum Value کھوتہٕ زیادہ آسُن، ورنہٕ Output ییہِ سوروئی Zero۔</strong>

<strong>بییہِ، Threshold گژھہِ ترِیم شرط تہِ پورٕ کرُن: پرؠتھ Sample ہُک گژھہِ پننِس Noise Content مُطابق پنُن الگ Independent Threshold آسُن۔</strong>

یہِ چھُ امیوجہ کہ Samples منز چھُ Noise content اکثر مختلف آسان۔ مثالے، اکثر چھُ یہِ گژھان کہ اکی Dataset منز Sample A منز چھُ کم Noise آسان مگر Sample B منز چھُ زیادہ Noise آسان۔ یتھ حالتس منز، Denoising algorithm منز Soft thresholding کرنہٕ وزِ، Sample A خٲطرٕ گژھہِ کم Threshold تہٕ Sample B خٲطرٕ گژھہِ زیادہ Threshold استعمال کرُن۔ اگرچہ Deep Neural Networks منز یمن Features تہٕ Thresholds ہٕنز واضح Physical definitions چھِ نہٕ آسان، مگر بنیادی منطق (Logic) چھِ سہی۔ یعنی، پرؠتھ Sample ہُک گژھہِ پننِس Noise content مُطابق پنُن الگ Independent Threshold آسُن۔

## 3. Attention Mechanism

Attention Mechanisms چھِ Computer Vision فیلڈس منز سمجھنۍ آسان۔ جانورن ہُنٛد Visual System چھُ ساری علاقہ تیزی سان Scan کران تہٕ Target Object شناخت کران، پتہٕ چھُ صرف اتھ Target Object پؠٹھ Attention دیوان تاکہ مزید تفصیلات (Details) حاصل کرِ تہٕ غیر ضروری معلومات (Irrelevant information) کرِ نظر انداز۔ تفصیلات خٲطرٕ وچھِیو Attention Mechanisms متعلق لٹریچر۔

Squeeze-and-Excitation Network (SENet) چھُ Attention Mechanisms استعمال کرن وول اکھ نوُ Deep Learning Method۔ مختلف Samples منز، مختلف Feature Channels ہُنٛد Classification Task منز حصہ (Contribution) چھُ مختلف آسان۔ SENet چھُ اکھ لۄکُٹ Sub-network استعمال کٔرِتھ "Learn a set of weights" (Weights ہٕچ اکھ سیٹ حاصل کران) تہٕ پتہٕ یمن Weights چھُ متعلقہ Channels ہٕندین Features سٟتۍ ضرب (multiply) دِوان تاکہ پرؠتھ Channel کِس Feature size ایڈجسٹ (Adjust) کرِ۔ یتھ عملس ہیکو أسۍ "Apply weighting to each feature channel" ونِتھ۔

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

یتھ طریقہِ منز، پرؠتھ Sample ہٕچ چھِ پنٕنۍ Independent Weights آسان۔ SENet منز، Weights حاصل کرنُک راستہ چھُ: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function"۔

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Deep Residual Shrinkage Network چھُ Soft thresholding یُس Deep Attention Mechanism تحت چھُ، حاصل کرنہٕ خٲطرٕ SENet Sub-network سٹرکچر (Structure) پیٹھہٕ اِنسپریشن (Inspiration) ہیزان۔ Red box منز دِینہٕ آمُت Sub-network ذریعے ہیکو أسۍ "Learn a set of thresholds" (Thresholds ہٕچ اکھ سیٹ حاصل کٔرِتھ) پرؠتھ Feature Channel پؠٹھ Soft thresholding لاگو کٔرِتھ۔

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

اتھ Sub-network منز، گۄڈٕ چھُ Input Feature Map کِس سارِنی Features ہُک Absolute Value کڈنہٕ یِوان۔ پتہٕ Global Average Pooling تہٕ Averaging ذریعہ چھُ اکھ Feature حاصل گژھان، ییتھ $A$ وننہٕ چھُ یِوان۔ دۄیمہِ راستہ (Identity path) منز، Global Average Pooling پتہٕ یُس Feature Map چھُ میلان، سہ چھُ اکہِ لۄکٹِس Fully Connected Network منز دِینہٕ یِوان۔ یہِ Fully Connected Network چھُ Sigmoid Function آخری Layer کِس طور استعمال کران تاکہ Output ییہِ 0 تہٕ 1 درمیان Normalize کرنہٕ، تہٕ اکھ Coefficient چھُ میلان ییتھ $\alpha$ وننہٕ چھُ یِوان۔ لہذا، Final Threshold ہیکو أسۍ $\alpha \times A$ سٟتۍ ظاہر کٔرِتھ۔ یعنی، Threshold چھُ 0 تہٕ 1 درمیان اکھ نمبر × Feature Map کِس Absolute Values ہُک Average۔ <strong>یہِ طریقہ چھُ یقین دِلاوان زِ Threshold چھُ نہ صرف Positive بلکہ یہِ گژھہِ نہٕ حد کھوتہٕ زیادہ بڈٕ۔</strong>

<strong>مزيد برآں، مختلف Samples ہٕندۍ چھِ مختلف Thresholds آسان۔ نتیجتاً، کیہہ حد تک ہیکو أسۍ اتھ اکھ خاص Attention Mechanism سمجھِتھ: یہِ چھُ تمن Features شناخت کران یم موجودہ Task سٟتۍ متعلق چھِ نہٕ آسان، تہٕ تمن چھُ زٕ (2) Convolutional Layers ذریعے 0 کِس قریب کران تہٕ Soft thresholding ذریعے تمن بالکل Zero سیٹ کران؛ یا بییہِ طرفہٕ، یہِ چھُ Task سٟتۍ متعلق Features شناخت کران، تمن 0 نش دوٗر کران تہٕ تمن برقرار (Preserve) تھاوان۔</strong>

آخرس منز، کینہہ خاص تعداد منز Basic Modules ("Stack many basic modules")، Convolutional Layers، Batch Normalization، Activation Functions، Global Average Pooling تہٕ Fully Connected Output Layers سٟتۍ مِلٲوِتھ چھُ مکمل Deep Residual Shrinkage Network بنان۔

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Deep Residual Shrinkage Network چھُ دراصل اکھ عام Feature Learning Method۔ یہِ امیوجہ کہ واریاہن Feature Learning Tasks منز چھِ Samples منز کم یا زیادہ Noise یا غیر متعلقہ معلومات آسان۔ یہِ Noise ہیکہِ Feature Learning کِس کارکردگی (Performance) پؠٹھ اثر تراوِتھ۔ مثالے:

Image Classification منز، اگر تصویرِ منز باقی چیز تہِ آسَن، تمن ہیکو أسۍ "Noise" سمجھِتھ۔ Deep Residual Shrinkage Network ہیکہِ Attention Mechanism سٟتۍ یمن "Noise" چیزن نوٹس (Notice) کٔرِتھ Soft thresholding ذریعے تمن Zero سیٹ کٔرِتھ Image Classification Accuracy بہتر کٔرِتھ۔

Speech Recognition منز، خاص کر شور واجِنی جایہِ مثلاً سڑکِ کنارے یا فیکٹری منز، Deep Residual Shrinkage Network ہیکہِ Speech Recognition Accuracy بہتر کٔرِتھ، یا کم از کم اکھ یُتھ طریقہِ کار فراہم کٔرِتھ یُس Accuracy بہتر کرنس منز مدد دیِہ۔

## Academic Impact

اتھ پیپرز (Paper) چھِ Google Scholar پؠٹھ 1400 کھوتہٕ زیادہ Citations۔

نامکمل اعداد و شمار مُطابق، Deep Residual Shrinkage Network (DRSN) چھُ 1000 کھوتہٕ زیادہ Publications/Studies منز براہ راست استعمال یا Modify کٔرِتھ استعمال کرنہٕ آمُت، یمن منز Mechanical Engineering، Electrical Power، Vision، Healthcare، Speech، Text، Radar تہٕ Remote Sensing شامل چھِ۔

<!-- 使用 HTML h2 标签替代 Markdown 的 ## -->
<h2>حوالہ جات (References)</h2>

<!-- 强制左对齐的容器 -->
<div class="references-ltr" dir="ltr" style="text-align: left; direction: ltr; margin-bottom: 20px;">
  <p>
    Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.
  </p>
  <p>
    <!-- 使用 HTML a 标签替代 Markdown 链接 -->
    <a href="https://ieeexplore.ieee.org/document/8850096">https://ieeexplore.ieee.org/document/8850096</a>
  </p>
</div>

<h2>BibTeX</h2>

<div dir="ltr" style="text-align: left; direction: ltr;">
<pre style="background-color: #f6f8fa; padding: 16px; border-radius: 6px; overflow: auto;"><code>@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}</code></pre>
</div>
