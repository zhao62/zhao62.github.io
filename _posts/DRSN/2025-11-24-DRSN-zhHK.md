---
layout: post
title: "深度殘差收縮網絡 (Deep Residual Shrinkage Network)：一種適用於強雜訊數據的 AI 方法"
date: 2025-11-24
tags: [Deep Learning, AI]
mathjax: true
---

**深度殘差收縮網絡 (Deep Residual Shrinkage Network, DRSN) 是 Deep Residual Network (ResNet) 的改良變體，本質上是 ResNet、Attention Mechanism (注意力機制) 與 Soft Thresholding (軟閾值函數) 的整合。**

**DRSN 的運作原理可概括為：利用 Attention Mechanism 識別出不重要的特徵，並通過 Soft Thresholding 將其歸零 (Zeroing)；同時，保留並強化重要的特徵。這種機制顯著提升了 Deep Neural Network 從含雜訊訊號 (Noisy Signals) 中提取有效特徵的能力。**

## 1. 研究動機
**首先，在對樣本進行分類時，數據中不可避免地會包含各類雜訊，諸如 Gaussian Noise (高斯雜訊)、Pink Noise (粉紅雜訊) 或 Laplacian Noise (拉普拉斯雜訊)**。廣義而言，數據中任何與當前分類任務無關的資訊 (Information)，均可被視為雜訊。這些雜訊往往會對分類表現 (Classification Performance) 構成負面影響。（註：Soft Thresholding 正是許多訊號降噪演算法中的關鍵步驟。）

舉例來說，在馬路邊進行對話時，錄音中可能會混雜車輛的響號聲 (Horn Sound)、路噪 (Road Noise)等。當我們對這些訊號進行語音識別 (Speech Recognition) 時，識別準確率難免會受這些背景雜訊干擾。從 Deep Learning 的角度視之，這些響號聲和路噪對應的特徵，理應在 Neural Network 內部被剔除 (Eliminated)，以免影響識別結果。

**其次，即使在同一個 Dataset (數據集) 中，各樣本的雜訊含量 (Noise Level) 也往往存在差異。**（這一點與 Attention Mechanism 的理念相通：以圖像 Dataset 為例，目標物體在每張圖片中的位置不盡相同；Attention Mechanism 能夠針對每一張圖片，動態聚焦於目標物體所在區域。）

例如，在訓練一個「貓狗分類器」時，對於 Label 為「狗」的 5 張圖像：第 1 張可能包含狗和老鼠，第 2 張包含狗和鵝……以此類推。訓練過程中，Model 不可避免地會受到老鼠、鵝、雞、驢等無關物體的干擾 (Interference)，導致準確率下降。若我們能成功識別並剔除這些無關物體的特徵，貓狗分類器的準確率將有望提升。

## 2. 軟閾值化 (Soft Thresholding)
**Soft Thresholding 是許多訊號降噪演算法的核心，其作用是將 Absolute Value (絕對值) 小於某個 Threshold (閾值) 的特徵剔除，並將 Absolute Value 大於該 Threshold 的特徵向零收縮 (Shrinkage)**。其數學公式如下：

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Soft Thresholding 的輸出對輸入的Derivative (導數)為：

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

由此可見，Soft Thresholding 的 Derivative 要麼是 1，要麼是 0。這一性質與 ReLU 完全一致。因此，引入 Soft Thresholding 亦有助於減低 Deep Learning 演算法遭遇 Gradient Vanishing (梯度消失) 和 Gradient Exploding (梯度爆炸) 的風險。

**在設計 Soft Thresholding 函數時，Threshold 的設定必須滿足兩個條件：第一，Threshold 必須為正數；第二，Threshold 不能大於 Input Signal 的最大值，否則輸出將全部歸零。**

**此外，理想的 Threshold 還應滿足第三個條件：每個樣本應根據自身的雜訊含量，擁有獨立的 Threshold。**

這是因為不同樣本的雜訊強度往往不同。例如在同一個 Dataset 中，樣本 A 可能雜訊較少，而樣本 B 雜訊較多。在降噪過程中，樣本 A 應採用較小的 Threshold，而樣本 B 則適用較大的 Threshold。在 Deep Neural Network 中，雖然特徵與閾值不再具有顯式的物理意義，但背後的邏輯是一致的：每個樣本應根據其自身的特徵分佈，自適應地 (Adaptively) 確定獨立的 Threshold。

## 3. 注意力機制 (Attention Mechanism)
Attention Mechanism 在 Computer Vision (電腦視覺) 領域已被廣泛應用。生物的視覺系統能夠快速掃描全局，鎖定目標物體，並將注意力 分配 (Allocate) 於該區域以提取細節，同時抑制周邊無關資訊。詳情可參考 Attention Mechanism 相關文獻。

Squeeze-and-Excitation Network (SENet) 是一種代表性的基於 Attention 的深度學習方法。在分類任務中，不同的 Feature Channels (特徵通道) 對結果的貢獻度往往不同。SENet 通過一個小型的 Sub-network 學習一組 Weights (權重)，並將其與各通道的特徵相乘，從而調整各通道的特徵強度。此過程可視為對不同特徵通道施加不同程度的 Attention。

<p align="center">
  <img src="/assets/img/2025-11-24-DRSN-zhHK/SENET_zhHK_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

在這種架構下，每個樣本都會生成一組獨立的 Weights。換言之，任意兩個樣本的權重分佈都是獨一無二的。在 SENet 中，生成權重的路徑為：「Global Pooling → Fully Connected Layer → ReLU → Fully Connected Layer → Sigmoid」。

<p align="center">
  <img src="/assets/img/2025-11-24-DRSN-zhHK/SENET_zhHK_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. 深度注意力機制下的軟閾值化
DRSN 參考了上述 SENet 的結構，實現了基於深度注意力機制的 Soft Thresholding。通過紅框所示的 Sub-network，Model 能夠學習出一組閾值，對各個 Feature Channel 執行 Soft Thresholding。

<p align="center">
  <img src="/assets/img/2025-11-24-DRSN-zhHK/DRSN_zhHK_1.png" alt="深度殘差收縮網絡" width="75%">
</p>

在該 Sub-network 中，首先計算輸入 Feature Map 中所有特徵的 Absolute Value。隨後，經由 Global Average Pooling 取平均值，得到特徵 A。在另一路徑中，經過 Global Average Pooling 的 Feature Map 被輸入至一個小型的 Fully Connected Network。該網絡以 Sigmoid 函數作為最後一層，將輸出 Normalize (正規化) 至 0 和 1 之間，得到係數 α。最終的 Threshold 可表示為 α × A。因此，Threshold 本質上是一個 0 至 1 之間的係數與特徵圖絕對值平均數的乘積。**這種設計不僅確保了 Threshold 為正數，且數值不會過大。**

**更重要的是，這實現了「不同樣本對應不同 Threshold」的機制。這在一定程度上可理解為一種特殊的 Attention Mechanism：Model 能識別與當前任務無關的特徵，通過 Convolutional Layers 將其變換為接近 0 的數值，再經 Soft Thresholding 徹底歸零；反之，與任務相關的特徵則會被轉換為遠離 0 的數值並得以保留。**

最後，通過堆疊 (Stacking) 一定數量的 Basic Modules，配合 Convolutional Layers、Batch Normalization、Activation Function、Global Average Pooling 以及 Fully Connected Output Layer，便構建出完整的深度殘差收縮網絡。

<p align="center">
  <img src="/assets/img/2025-11-24-DRSN-zhHK/DRSN_zhHK_2.png" alt="深度殘差收縮網絡" width="55%">
</p>

## 5. 通用性 (Generalization)
事實上，DRSN 是一種通用的 Feature Learning (特徵學習) 方法。因為在大多數特徵學習任務中，樣本或多或少都包含雜訊或不相關資訊，這些因素往往會削弱學習效果。

*   **圖像分類 (Image Classification)**：若圖像背景雜亂，包含許多非目標物體，這些物體即可視為「雜訊」。DRSN 有望借助 Attention Mechanism 鎖定這些「雜訊」，並利用 Soft Thresholding 將其對應特徵歸零，從而提升分類準確率。
*   **語音識別 (Speech Recognition)**：在嘈雜環境（如馬路邊或工業環境）進行對話時，DRSN 可能有效提升識別準確率，或至少為解決高噪聲環境下的識別問題提供了一個極具潛力的研究方向。

## 參考文獻

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2942898}
}
```

## 影響力情況

該論文在 Google Scholar 上的引用次數已超過 1400 次。

據保守估計，DRSN 已被超過 1000 篇文獻引用，或經改良後應用於機械工程、電力系統、Computer Vision、醫療診斷、語音處理、NLP、雷達及遙感等多個領域。
