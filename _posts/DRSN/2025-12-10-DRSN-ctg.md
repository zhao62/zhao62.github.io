---
layout: post
title: "Deep Residual Shrinkage Network: Highly Noisy Data-র লাই একখান Artificial Intelligence Method"
date: 2025-12-10
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network অইলো Deep Residual Network এর একখান improved variant. আসলে, Deep Residual Shrinkage Network এ Deep Residual Network, attention mechanisms, আর soft thresholding functions এক সাথে কাজ গরে।**

**আঁরা Deep Residual Shrinkage Network এর কাজ গরিবার তরিকা নিম্ন-লিখিত ভাবে বুঝিত পারি। প্রথমে, network attention mechanisms use গরি unimportant features খুঁজি বাইর গরে। তারপর, network soft thresholding functions use গরি এই unimportant features-রে zero বানাই দেয়। উল্টা ভাবে অইলে, network important features চিনি লয় আর এই important features-রে ধরি রাখে। এই process deep neural network এর ক্ষমতা বাড়ায়। এই process network-রে noise থাকা signal ওত্তুন useful features extract গরিতে সাহায্য গরে।**

## 1. Research এর উদ্দেশ্য (Research Motivation)

**প্রথমে, algorithm যখন sample classify গরে, তখন noise থাকাটাই স্বাভাবিক। এই noise এর উদাহরণ অইলো Gaussian noise, pink noise, আর Laplacian noise.** আরো বড় ভাবে অইলে, sample-এ প্রায় সময় এমন information থাকে যেইটা বর্তমান classification task এর লগে মিল না থাকে। আঁরা এই irrelevant information-রে noise হিসাবে ধরি লইত পারি। এই noise classification performance হমাই দিতে পারে। (Soft thresholding অনেক signal denoising algorithms এর একখান আসল ধাপ।)

উদাহরণ হিসাবে, রাস্তার কিনারে হথা অইবার হথা চিন্তা গরন। Audio-তে গাড়ি horn আর চাকা-র আওয়াজ থাকিলত পারে। আঁরা এই signal এর উফরে speech recognition গরিতে পারি। এই background sounds result-রে নিশ্চিত ভাবে affect গরিবু। Deep learning এর দিক দি চিন্তা গরিলে, deep neural network এর উচিত horn আর চাকা-র লগে মিল থাকা features eliminate গরি দেয়া। এই elimination এই features-রে speech recognition result-এ প্রভাব ফেলা ওত্তুন আটকায়।

**দ্বিতীয় হথা অইলো, প্রায় সময় বিভিন্ন sample-এ noise এর পরিমাণ আলাদা হয়। এমনকি একই dataset এর ভিতরে ও এমন হয়।** (এই variation attention mechanisms এর লগে মিল আছে। একখান image dataset এর উদাহরণ লন। প্রত্যেক image-এ target object এর location আলাদা অইত পারে। Attention mechanisms প্রত্যেক image-এ target object এর নির্দিষ্ট location-এ focus গরিতে পারে।)

মনে গরন, আঁরা একখান cat-and-dog classifier train গরির, আর আঁরদের কাছে ৫ খান "dog" label করা image আছে। Image 1-এ হয়তো কুকুর আর ইঁদুর আছে। Image 2-এ হয়তো কুকুর আর হাঁস আছে। Image 3-এ হয়তো কুকুর আর মুরগি আছে। Image 4-এ হয়তো কুকুর আর গাধা আছে। Image 5-এ হয়তো কুকুর আর পাতি-হাঁস আছে। Training এর সময়, irrelevant objects classifier-রে disturb গরিবু। এই object গুলিন অইলো ইঁদুর, হাঁস, মুরগি, গাধা আর পাতি-হাঁস। এই interference এর কারণে classification accuracy হমি যায়। যদি আঁরা এই irrelevant objects চিনি লইত পারি, তাইলে আঁরা এই objects এর লগে থাকা features eliminate গরিতে পারিয়ুম। এই ভাবে, আঁরা cat-and-dog classifier এর accuracy বাড়াইতে পারি।

## 2. Soft Thresholding

**Soft thresholding অনেক signal denoising algorithms এর একখান মূল ধাপ। যদি features এর absolute values একখান নির্দিষ্ট threshold এর নিচে হয়, তাইলে algorithm এই features eliminate গরি দেয়। যদি features এর absolute values এই threshold এর বেশি হয়, তাইলে algorithm এই features-রে zero-র দিকে shrink গরে (বা হমাই আনে)।** Researchers রা নিচের formula দিয়ে soft thresholding implement গরিতে পারে:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Input এর সাপেক্ষে soft thresholding output এর derivative অইলো:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

উফরর formula-য় দেহা যার যে, soft thresholding এর derivative হয় 1 নাইলে 0. এই বিষয়টা ReLU activation function এর মতোই। তাই, soft thresholding deep learning algorithms এ gradient vanishing আর gradient exploding এর risk হমাই দেয়।

**Soft thresholding function-এ, threshold set গরিবার সময় দুই খান শর্ত মানিত অইবো। এক, threshold অবশ্যই positive number অইত অইবো। দুই, threshold input signal এর maximum value ওত্তুন বড় অইত ন পারিবু। নইলে, পুরা output zero অই যাইবু।**

**তারপরও, threshold আরেকখান তৃতীয় শর্ত মানিলে ভালো হয়। প্রত্যেক sample এর নিজের noise content অনুসারে নিজের independent threshold থাকা উচিত।**

এর কারণ অইলো, প্রায় সময় বিভিন্ন sample-এ noise এর পরিমাণ আলাদা হয়। যেমন, একই dataset এর ভিতরে Sample A-তে হয়তো noise কম, কিন্তু Sample B-তে হয়তো noise বেশি। এই ক্ষেত্রে, soft thresholding গরিবার সময় Sample A-তে ছোট threshold use গরা উচিত। আর Sample B-তে বড় threshold use গরা উচিত। Deep neural networks এ যদিও এই features আর thresholds তাদের প্রকৃতিগত মানে হারাই ফেলে, কিন্তু মূল logic একই থাকে। সোজা হথা অইলো, প্রত্যেক sample এর একখান independent threshold থাকা লাগিবু। আর নির্দিষ্ট noise content এই threshold ঠিক গরিবু।

## 3. Attention Mechanism

Researchers রা computer vision field-এ attention mechanisms সহজেই বুঝিত পারে। প্রাণীর visual system পুরা এলাকা দ্রুত scan গরি target আলাদা গরিতে পারে। তারপর, visual system target object এর দিকে attention focus গরে। এই কাজ system-রে আরো details extract গরিতে সাহায্য গরে। একইসাথে, system irrelevant information বন্ধ রাখে। আরো details জানিবার লাই, attention mechanisms এর literature পড়িত পারেন।

Squeeze-and-Excitation Network (SENet) অইলো attention mechanisms use গরা একখান নতুন deep learning method. বিভিন্ন sample-এ, বিভিন্ন feature channels classification task-এ আলাদা আলাদা ভাবে সাহায্য গরে। SENet একখান ছোট sub-network use গরি এক set weights পায় (Learn a set of weights)। তারপর, SENet এই weights-রে ওই channels এর features এর লগে গুণ গরে (Apply weighting to each feature channel)। এই কাজ প্রত্যেক channel এর features এর পরিমাণ adjust গরে। আঁরা এই process-রে বিভিন্ন feature channels-এ আলাদা আলাদা attention দেয়া হিসাবে ধরিত পারি।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

এই পদ্ধতিতে, প্রত্যেক sample এর এক set independent weights থাকে। মানে, যেকোনো দুই খান sample এর weights আলাদা হয়। SENet-এ, weights পাওনোর আসল রাস্তা অইলো "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Deep Attention Mechanism এর লগে Soft Thresholding

Deep Residual Shrinkage Network SENet sub-network এর structure use গরে। Network এই structure use গরি deep attention mechanism এর under-এ soft thresholding implement গরে। Sub-network (যেইটা লাল box এর ভিতরে দেখানো হইছে) এক set thresholds learn গরে (Learn a set of thresholds)। তারপর, network এই thresholds use গরি প্রত্যেক feature channel-এ soft thresholding apply গরে।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

এই sub-network এ, system প্রথমে input feature map এর সব features এর absolute values হিসাব গরে। তারপর, system global average pooling আর averaging গরে যাতে একখান feature পাওয়া যায়, যারে *A* হিসাবে ধকা হয়। অন্য রাস্তা দি, system global average pooling এর পরে feature map-রে একখান ছোট fully connected network-এ ঢুকায়। এই fully connected network শেষ layer হিসাবে Sigmoid function use গরে। এই function output-রে 0 আর 1 এর মাঝখানে normalize গরে। এই process একখান coefficient দেয়, যারে *α* বলা হয়। আঁরা final threshold-রে *α × A* হিসাবে লিখিত পারি। তাই, threshold অইলো দুই খান number এর গুণফল। একখান number 0 আর 1 এর মাঝখানে থাকে। আরেকখান number অইলো feature map এর absolute values এর average. **এই method নিশ্চিত গরে যে threshold positive হবে। এই method আরো নিশ্চিত গরে যে threshold বেশি বড় ন হবে।**

**আরো হথা অইলো, বিভিন্ন sample বিভিন্ন thresholds create গরে। তাই, আঁরা এই method-রে একখান special attention mechanism হিসাবে বুঝিত পারি। এই mechanism বর্তমান task এর লগে মিল না থাকা features খুঁজি বাইর গরে। এই mechanism দুই খান convolutional layer use গরি এই features-রে 0 এর কাছাকাছি নিয়ে আসে। তারপর, এই mechanism soft thresholding use গরি এই features-রে zero বানাই দেয়। অথবা, এই mechanism বর্তমান task এর লগে দরকারি features খুঁজি পায়। এই mechanism দুই খান convolutional layer use গরি এই features-রে 0 থেকে দূরে সরাই নেয়। শেষে, এই mechanism এই features-রে ধরি রাখে।**

শেষ-মেশ, আঁরা কিছু basic modules stack গরি (Stack many basic modules)। আঁরা convolutional layers, batch normalization, activation functions, global average pooling, আর fully connected output layers ও add গরি। এই process পুরা Deep Residual Shrinkage Network তৈর গরে।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Deep Residual Shrinkage Network feature learning এর লাই একখান general method. কারণ অইলো, অনেক feature learning task-এ sample-এ প্রায় সময় noise থাকে। Sample-এ irrelevant information ও থাকে। এই noise আর irrelevant information feature learning এর performance খারাপ গরিতে পারে। যেমন:

Image classification এর হথা চিন্তা গরন। একখান ছবি-তে হয়তো অনেক অন্য objects থাকিলত পারে। আঁরা এই objects-রে "noise" হিসাবে ধরিত পারি। Deep Residual Shrinkage Network হয়তো attention mechanism use গরিতে পারে। Network এই "noise" খুঁজি পায়। তারপর, network soft thresholding use গরি এই "noise" এর লগে থাকা features-রে zero বানাই দেয়। এই কাজ image classification accuracy বাড়াইতে পারে।

Speech recognition এর হথা চিন্তা গরন। বিশেষ গরি, যখন রাস্তা কিনারে বা factory workshop এর মতো noisy জায়গায় হথা বলা হয়। Deep Residual Shrinkage Network speech recognition accuracy বাড়াইতে পারে। অন্তত, network একখান method দেয়। এই method speech recognition accuracy বাড়ানোর ক্ষমতা রাখে।

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

এই paper Google Scholar-এ ১৪০০ এর বেশি citations পাইছে।

অসমাপ্ত হিসাব মতে, researchers রা ১০০০ এর বেশি publications/studies-এ Deep Residual Shrinkage Network (DRSN) apply গরছে। এই applications অনেক field-এ ছড়ানো আছে। এই field গুলিনর মধ্যে আছে mechanical engineering, electrical power, vision, healthcare, speech, text, radar, আর remote sensing.
