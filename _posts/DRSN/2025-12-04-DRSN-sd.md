---
layout: post
title: "<div style='text-align: center; direction: rtl;'>Deep Residual Shrinkage Network: Highly Noisy Data lae hik Artificial Intelligence Method</div>"
subtitle: "<div style='text-align: center; direction: ltr;'>An Artificial Intelligence Method for Highly Noisy Data</div>"
date: 2025-12-04
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network asal mein Deep Residual Network jo hik behtar variant ahey. Bunyadi tor te, he Deep Residual Network, attention mechanisms, ain soft thresholding functions jo integration ahey."
lang: sd
dir: rtl
---

<!-- CSS Styling -->
<style>
  /* 1. Force Headers Right */
  h1, h2, h3, h4, h5, h6 { 
  text-align: right !important; 
  direction: ltr !important; 
  unicode-bidi: embed; /* 确保浏览器正确处理混合文本 */
}
  
  /* 2. Force Lists Right */
  ul, ol { text-align: right !important; direction: rtl !important; padding-right: 40px; padding-left: 0; }

  /* 3. Force Code LTR */
  pre, code { text-align: left !important; direction: ltr !important; }
  
  /* 4. Force References LTR */
  .references-ltr { text-align: left !important; direction: ltr !important; }
</style>

<!-- Main Content Wrapper -->
<div dir="rtl" markdown="1" style="text-align: right; direction: rtl; font-family: 'Sakkal Majalla', 'Traditional Arabic', serif; font-size: 1.1em;">

<strong>Deep Residual Shrinkage Network asal mein Deep Residual Network jo hik behtar variant ahey. Bunyadi tor te, he Deep Residual Network, attention mechanisms, ain soft thresholding functions jo integration ahey.</strong>

<strong>Hek hadd tain, Deep Residual Shrinkage Network je working principle khey heth dinal tareeqay saan samjhi saghje tho: he attention mechanisms khey use kandey unimportant features ji sunhann (identify) kare tho ain soft thresholding functions zariye unhan khey zero set kare tho; bi taraf, he important features khey identify kare tho ain unhan khey retain kare tho. He process deep neural network ji noise wari signals maan useful features extract karann ji salahiyat khey wadhaye tho.</strong>

## 1. Research Motivation

<strong>Pehryon, jadahn samples khey classify kyo wyo ahey, ta noise—jihan ta Gaussian noise, pink noise, ain Laplacian noise—jo hojjann lazmi ahey.</strong> Wadheek wasee paimany te, samples mein aksar ehri maloomat hundi ahey jehri current classification task lay irrelevant hundi ahey, un khey bi <strong>noise</strong> samjhi saghje tho. He <strong>noise</strong> classification performance te manfi asar (negative affect) vijhi saghey thi. (<strong>Soft thresholding</strong> ghanay <strong>signal denoising algorithms</strong> mein hik aham marhalo ahey.)

Misal tor te, sadak je kinary guftago doran, audio mein gaddiyun je horns ain pahiyn ja awaz mix thi saghan tha. Jadahn inhan signals te <strong>speech recognition</strong> kyo wyo ahey, ta nateejay lazmi tor te inhan background sounds khan mutasir thinda. <strong>Deep learning</strong> je nuqta-e-nazar (perspective) khan, horns ain wheels saan wabasta <strong>features</strong> khey <strong>deep neural network</strong> andar eliminate karann gurjey ta jian uhey <strong>speech recognition</strong> je nateejon khey mutasir na kan.

<strong>Byo he ta, sagye dataset mein bi, noise jo miqdar aksar sample dar sample mukhtalif hundo ahey.</strong> (He <strong>attention mechanisms</strong> saan mumasilat rakhy tho; <strong>image dataset</strong> jo misal wathann, ta target object ji location mukhtalif images mein mukhtalif thi saghey thi, ain <strong>attention mechanisms</strong> har image mein target object ji makhsoos location te focus kare saghan tha.)

Misal tor te, jadahn hik cat-and-dog <strong>classifier</strong> khey train kyo wyo ahey, panj images khey "dog" label kayo. Pehri image mein kutta ain kuan (mouse) huje, byi mein kutta ain hans (goose), teen mein kutta ain kukar (chicken), chothi mein kutta ain gadah (donkey), ain panjien mein kutta ain badak (duck) huje. Training doran, <strong>classifier</strong> lazmi tor te irrelevant objects jihan ta mice, geese, chickens, donkeys, ain ducks khan interference qabool kando, jenhin je nateejay mein <strong>classification accuracy</strong> ghatjee wehndi. Agar asan inhan irrelevant objects—mice, geese, chickens, donkeys, ain ducks—khey identify kare saghon ain unhan je corresponding <strong>features</strong> khey eliminate kayon, ta cat-and-dog <strong>classifier</strong> ji accuracy behtar karann mumkin ahey.

## 2. Soft Thresholding

<strong>Soft thresholding, ghanay signal denoising algorithms jo hik core step ahey. He unhan features khey eliminate kare tho jin ja absolute values hik makhsoos threshold khan ghat hunda ahin ain unhan features khey shrink kare tho jin ja absolute values hen threshold khan vadheek hunda ahin zero je taraf.</strong> In khey heth dinal formula zariye implement kare saghje tho:

<div dir="ltr">
$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$
</div>

<strong>Soft thresholding</strong> output jo input je lihaz khan derivative he ahey:

<div dir="ltr">
$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$
</div>

Jiyan ta mathay dekharyo wyo ahey, <strong>soft thresholding</strong> jo derivative ya 1 ahey ya 0. He property <strong>ReLU activation function</strong> wari ahey. Tanhnkare, <strong>soft thresholding</strong> deep learning algorithms mein <strong>gradient vanishing</strong> ain <strong>gradient exploding</strong> je khatray khey ghataye tho.

<strong>Soft thresholding function mein, threshold ji setting khey ba sharton (conditions) pooro karann gurjey: pehryon, threshold hik positive number huje; byo, threshold input signal je maximum value khan wadheek na huje, na ta output pooro zero thi wendo.</strong>

<strong>Wadheek he ta, threshold khey teeji shart poori karann behtar ahey: har sample jo hen je noise content je mutabiq penhjo azadana (independent) threshold hujann gurjey.</strong>

Chakaanta <strong>noise content</strong> aksar samples darmiyan mukhtalif hundo ahey. Misal tor te, sagye dataset mein aam ahey ta Sample A mein ghat <strong>noise</strong> huje jabkay Sample B mein wadheek <strong>noise</strong> huje. Hen soorat mein, jadahn <strong>denoising algorithm</strong> mein <strong>soft thresholding</strong> kyo wyo ahey, Sample A khey nandho threshold use karann gurjey, jabkay Sample B khey waddo threshold use karann gurjey. Agarcha he <strong>features</strong> ain thresholds <strong>deep neural networks</strong> mein penhiji wazeh physical definitions winyaye wehnda ahin, par bunyadi logic sagyi rahandi ahey. Lafzan mein, har sample jo penhjo independent threshold hujann gurjey jehro un je makhsoos noise content tay kyo wyo ahey.

## 3. Attention Mechanism

<strong>Attention mechanisms</strong> computer vision je maidan mein samajhann lay nisbatan asan ahin. Janwaran jo visual system poori area khey tezi saan scan kare targets khey distinguish kare saghey tho, bad mein <strong>target object</strong> te <strong>attention</strong> focus kare wadheek details extract kare tho ain irrelevant information khey suppress kare tho. Tafseelat lay, baraye meherbani <strong>attention mechanisms</strong> baray literature d'iso.

<strong>Squeeze-and-Excitation Network (SENet)</strong> hik nisbatan naya <strong>deep learning method</strong> ahey jehro <strong>attention mechanisms</strong> use kare tho. Mukhtalif samples darmiyan, <strong>classification task</strong> mein mukhtalif <strong>feature channels</strong> jo hisso (contribution) aksar mukhtalif hundo ahey. <strong>SENet</strong> hik nandho sub-network use kando ahey ta jian <strong>obtain a set of weights</strong> (weights jo set hasil karann) ain poe inhan <strong>weights</strong> khey respective channels je <strong>features</strong> saan multiply kare tho ta jian har channel mein <strong>features</strong> je magnitude khey adjust kare saghje. Hen process khey mukhtalif <strong>feature channels</strong> te mukhtalif levels ji <strong>attention</strong> apply karann tor d'isi saghje tho.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Hen tareeqay mein, har sample wat <strong>weights</strong> jo penhjo azadana set hundo ahey. Lafzan mein, kahin bi ba arbitrary samples ja <strong>weights</strong> mukhtalif hunda ahin. <strong>SENet</strong> mein, <strong>weights</strong> hasil karann jo makhsoos rasto he ahey "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

<strong>Deep Residual Shrinkage Network</strong> mathay dinal <strong>SENet</strong> sub-network structure khan inspiration wathi <strong>deep attention mechanism</strong> heth <strong>soft thresholding</strong> implement kare tho. Sub-network (jehko red box mein dekharyo wyo ahey) zariye, <strong>thresholds</strong> jo hik set <strong>learn</strong> kare saghje tho (<strong>Learn a set of thresholds</strong>) ta jian har <strong>feature channel</strong> te <strong>soft thresholding</strong> apply kare saghje.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Hen sub-network mein, input <strong>feature map</strong> je sabhni features ja absolute values pehryon calculate kya wenda ahin. Poe, <strong>global average pooling</strong> ain averaging zariye, hik feature hasil kyo wyo ahey, jenhin khey A zahir kyo wyo ahey. Bye rastey mein, <strong>global average pooling</strong> bad <strong>feature map</strong> khey hik nandhy <strong>fully connected network</strong> mein input kyo wyo ahey. He <strong>fully connected network</strong> penhiji final layer tor <strong>Sigmoid function</strong> use kare tho ta output khey 0 ain 1 darmiyan normalize kare saghje, jenhin saan hik coefficient miley tho jenhin khey α zahir kyo wyo ahey. Final threshold khey α × A tor express kare saghje tho. Tanhnkare, threshold hik 0 ain 1 darmiyan wary number ain <strong>feature map</strong> je absolute values je average jo product ahey. <strong>He tareeqo yaqeeni banaye tho ta threshold na sirf positive ahey par hadd khan wadheek waddo bi na ahey.</strong>

<strong>Wadheek he ta, mukhtalif samples ja nateejay mein mukhtalif thresholds milan tha. Nateejan, hek hadd tain, in khey hik specialized attention mechanism tor samjhi saghje tho: he current task lay irrelevant features khey identify kare tho, unhan khey ba convolutional layers zariye zero je qareeb values mein transform kare tho, ain soft thresholding use kandey unhan khey zero set kare tho; ya, he current task lay relevant features khey identify kare tho, unhan khey ba convolutional layers zariye zero khan passy (far) values mein transform kare tho, ain unhan khey preserve kare tho.</strong>

Aakhir mein, makhsoos tadaad mein <strong>basic modules</strong> (<strong>Stack many basic modules</strong>) khey <strong>convolutional layers</strong>, <strong>batch normalization</strong>, <strong>activation functions</strong>, <strong>global average pooling</strong>, ain <strong>fully connected output layers</strong> samait <strong>stack</strong> karann saan, mukammal <strong>Deep Residual Shrinkage Network</strong> tameer (construct) thi wye tho.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

<strong>Deep Residual Shrinkage Network</strong> darhaqeeqat hik aam <strong>feature learning method</strong> ahey. Chakaanta, ghanay <strong>feature learning tasks</strong> mein, samples mein ghat ya wadh koch <strong>noise</strong> ain irrelevant information hundi ahey. He <strong>noise</strong> ain irrelevant information <strong>feature learning</strong> performance khey mutasir kare saghey thi. Misal tor te:

<strong>Image classification</strong> mein, agar hik image mein hik waqt ghanay bya objects hujen, ta inhan objects khey "<strong>noise</strong>" samjhi saghje tho. <strong>Deep Residual Shrinkage Network</strong> shayad <strong>attention mechanism</strong> use kare hen "<strong>noise</strong>" khey notice kare saghey, ain poe <strong>soft thresholding</strong> use kare hen "<strong>noise</strong>" san wabasta features khey zero set kare, jenhin saan <strong>image classification accuracy</strong> behtar thi saghey thi.

<strong>Speech recognition</strong> mein, khaas tor te nisbatan noisy mahol mein jihan ta sadak kinary guftago ya factory workshop andar, <strong>Deep Residual Shrinkage Network</strong> shayad <strong>speech recognition accuracy</strong> improve kare saghey, ya kam az kam, hik ehro methodology pesh kare tho jehko <strong>speech recognition accuracy</strong> improve karann ji salahiyat rakhey tho.

## Academic Impact

<div dir="rtl" markdown="1" style="text-align: right; direction: rtl;">

He paper Google Scholar te 1,400 khan wadheek citations hasil kare chuko ahey.

Na-mukammal statistics je mutabiq, <strong>Deep Residual Shrinkage Network (DRSN)</strong> khey barah-e-rast apply kyo wyo ahey ya modify kare apply kyo wyo ahey 1,000 khan wadheek publications/studies mein jehky mechanical engineering, electrical power, vision, healthcare, speech, text, radar, ain remote sensing samait wasee maidanan (fields) te mushtamil ahin.

## حوالا (Reference)

<div class="references-ltr" dir="ltr" style="text-align: left; direction: ltr;">

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

<br>

<a href="https://ieeexplore.ieee.org/document/8850096">https://ieeexplore.ieee.org/document/8850096</a>

</div>

## BibTeX

```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```
