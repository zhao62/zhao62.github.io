---
layout: post
title: "Deep Residual Shrinkage Network: Hiji Metode Artificial Intelligence pikeun Data nu Ngandung Loba Noise"
date: 2025-12-02
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network teh nyaeta varian anu geus ditingkatkeun (improved variant) tina Deep Residual Network. Intina mah, ieu teh mangrupakeun integrasi tina Deep Residual Network, attention mechanisms, jeung soft thresholding functions."
---

**Deep Residual Shrinkage Network teh nyaeta varian anu geus ditingkatkeun (improved variant) tina Deep Residual Network. Intina mah, ieu teh mangrupakeun integrasi tina Deep Residual Network, attention mechanisms, jeung soft thresholding functions.**

**Dina batas nu tangtu, prinsip kerja Deep Residual Shrinkage Network teh bisa dipikaharti kieu: manehna ngagunakeun attention mechanisms pikeun nandaan fitur-fitur nu teu penting tur ngagunakeun soft thresholding functions pikeun nyetel fitur eta jadi nol; sabalikna, manehna bakal nandaan fitur-fitur penting tur mertahankeunana. Proses ieu ningkatkeun kamampuh deep neural network pikeun nyokot fitur-fitur nu aya gunana tina sinyal nu ngandung noise.**

## 1. Motivasi Panalungtikan

**Kahiji, nalika ngalakukeun klasifikasi sampel, ayana noise—saperti Gaussian noise, pink noise, jeung Laplacian noise—teh teu bisa dihindari.** Sacara leuwih lega, sampel sering ngandung informasi nu teu relevan jeung tugas klasifikasi nu keur dijalankeun, nu oge bisa dihartikeun salaku noise. Noise ieu bisa ngaganggu hasil klasifikasi. (Soft thresholding teh mangrupakeun lengkah konci dina loba algoritma signal denoising.)

Contona, nalika ngobrol di sisi jalan, sora obrolan bisa kacampur jeung sora klakson atawa sora ban mobil. Nalika ngalakukeun speech recognition kana sinyal-sinyal ieu, hasilna pasti bakal kapangaruhan ku sora-sora latar eta. Tina sudut pandang deep learning, fitur-fitur nu asalna tina klakson jeung ban mobil teh sakuduna dipiceun (eliminated) di jero deep neural network supaya teu ngaganggu hasil speech recognition.

**Kadua, sanajan dina dataset nu sarua, jumlah noise dina unggal sampel teh sering beda-beda.** (Ieu boga kasaruaan jeung attention mechanisms; nyokot conto dataset gambar, lokasi objek target dina unggal gambar bisa beda-beda, jeung attention mechanisms bisa fokus kana lokasi spesifik target eta dina unggal gambar.)

Contona, nalika ngalatih classifier ucing-jeung-anjing, bayangkeun aya lima gambar nu dilabelan "anjing." Gambar kahiji meureun aya anjing jeung beurit, nu kadua anjing jeung soang, nu katilu anjing jeung hayam, nu kaopat anjing jeung keledai, jeung nu kalima anjing jeung bebek. Pas keur training, classifier pasti bakal kaganggu ku objek-objek nu teu relevan saperti beurit, soang, hayam, keledai, jeung bebek, nu nyebabkeun akurasi klasifikasi turun. Lamun urang bisa nandaan objek-objek nu teu relevan ieu—beurit, soang, hayam, keledai, jeung bebek—terus miceun fitur nu sasuai (corresponding features), urang bisa ningkatkeun akurasi tina classifier ucing-jeung-anjing eta.

## 2. Soft Thresholding

**Soft thresholding teh mangrupakeun lengkah inti dina loba algoritma signal denoising. Ieu fungsina miceun fitur nu absolute values-na leuwih leutik tibatan threshold nu tangtu, jeung "ngaleutikan" (shrunk) fitur nu absolute values-na leuwih gede tibatan threshold eta ngarah deukeut ka nol.** Ieu bisa dilakukeun make rumus di handap:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Turunan tina output soft thresholding kana input-na nyaeta:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Jiga nu ditingali di luhur, turunan tina soft thresholding teh nilaina 1 atawa 0. Sifat ieu sarua jeung activation function ReLU. Ku kituna, soft thresholding oge bisa ngurangan risiko algoritma deep learning tina masalah gradient vanishing jeung gradient exploding.

**Dina soft thresholding function, setelan threshold kudu nyumponan dua syarat: kahiji, threshold kudu positif (positive number); kadua, threshold teu meunang leuwih gede ti nilai maksimum sinyal input, lamun henteu output-na bakal jadi nol kabeh.**

**Salalian ti eta, leuwih alus lamun threshold nyumponan syarat katilu: unggal sampel kudu boga threshold sorangan nu independen dumasar kana kandungan noise-na.**

Alesanna, kandungan noise dina unggal sampel teh sering beda-beda. Misalna, geus ilahar dina dataset nu sarua, Sampel A ngandung saeutik noise sedengkeun Sampel B ngandung loba noise. Dina kasus ieu, nalika ngajalankeun soft thresholding dina algoritma denoising, Sampel A kudu make threshold nu leutik, sedengkeun Sampel B kudu make threshold nu gede. Sanajan fitur jeung threshold ieu leungit definisi fisikna nu jelas (explicit physical definitions) dina deep neural networks, logika dasarna mah tetep sarua. Hartina, unggal sampel kudu boga threshold sorangan nu ditangtukeun ku kandungan noise spesifikna.

## 3. Attention Mechanism

Attention mechanisms teh rada gampang dipikaharti dina widang computer vision. Sistem visual sasatoan bisa ngabedakeun target ku cara nyisir (scanning) sakabeh area gancang-gancang, terus fokus (mere atensi) kana objek target pikeun nyokot detail nu leuwih loba bari ngahihilapkeun informasi nu teu relevan. Pikeun detailna, mangga tingali literatur ngeunaan attention mechanisms.

Squeeze-and-Excitation Network (SENet) mangrupakeun metode deep learning nu rada anyar nu ngagunakeun attention mechanisms. Dina sampel nu beda-beda, kontribusi tina feature channels nu beda-beda kana tugas klasifikasi teh sering teu sarua. SENet ngagunakeun sub-network leutik pikeun meunangkeun sakumpulan bobot (weights), terus ngakalikeun bobot ieu jeung fitur tina channel masing-masing pikeun nyetel badagna fitur dina unggal channel. Proses ieu bisa dianggap salaku mere level atensi (perhatian) nu beda-beda ka feature channels nu beda-beda.

<p align="center">
  <img src="/assets/img/DRSN/2025-12-02-DRSN-su/SENET_su_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Dina pendekatan ieu, unggal sampel boga sakumpulan bobot nu independen. Hartina, bobot pikeun dua sampel nu mana wae pasti beda. Dina SENet, jalur spesifik pikeun meunangkeun bobot teh nyaeta "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-12-02-DRSN-su/SENET_su_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Deep Residual Shrinkage Network nyokot inspirasi tina struktur sub-network SENet nu disebutkeun tadi pikeun nerapkeun soft thresholding make deep attention mechanism. Lewat sub-network (nu ditandaan ku kotak beureum), sakumpulan threshold bisa dipelajari pikeun nerapkeun soft thresholding kana unggal feature channel.

<p align="center">
  <img src="/assets/img/DRSN/2025-12-02-DRSN-su/DRSN_su_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Dina sub-network ieu, mimiti urang ngitung absolute values tina kabeh fitur dina input feature map. Terus, liwat global average pooling jeung dirata-rata, dimeunangkeun hiji fitur, nu disebut A. Dina jalur nu sejen, feature map sanggeus global average pooling diasupkeun kana fully connected network nu leutik. Fully connected network ieu make Sigmoid function salaku layer panungtungna pikeun ngaturna output antara 0 jeung 1, ngahasilkeun koefisien nu disebut α. Threshold akhir bisa ditulis salaku α × A. Jadi, threshold teh hasil kali tina angka antara 0 jeung 1 jeung rata-rata absolute values tina feature map. **Metode ieu ngajamin yen threshold henteu ngan ukur positif, tapi oge henteu gede teuing.**

**Salalian ti eta, sampel nu beda bakal ngahasilkeun threshold nu beda. Akibatna, nepi ka batas nu tangtu, ieu bisa dihartikeun salaku attention mechanism khusus: nandaan fitur nu teu relevan jeung tugas ayeuna, ngubah maranehna jadi nilai nu deukeut ka nol liwat dua convolutional layers, terus diset jadi nol make soft thresholding; atawa, nandaan fitur nu relevan, ngubah maranehna jadi nilai nu jauh ti nol liwat dua convolutional layers, terus dipertahankeun.**

Akhirna, ku cara numpuk (stacking) sababaraha basic modules bareng jeung convolutional layers, batch normalization, activation functions, global average pooling, jeung fully connected output layers, kabentuklah Deep Residual Shrinkage Network nu utuh.

<p align="center">
  <img src="/assets/img/DRSN/2025-12-02-DRSN-su/DRSN_su_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Kamampuh Generalisasi (Generalization Capability)

Deep Residual Shrinkage Network teh sabenerna mah metode feature learning anu umum (general). Ieu kulantaran dina loba tugas feature learning, sampel kurang leuwih ngandung noise jeung informasi nu teu relevan. Noise jeung informasi nu teu relevan ieu bisa mangaruhan performa feature learning. Contona:

Dina image classification, lamun hiji gambar ngandung loba objek sejen, objek-objek ieu bisa dianggap salaku "noise." Deep Residual Shrinkage Network meureun bisa make attention mechanism pikeun 'nyadar' kana "noise" ieu, terus make soft thresholding pikeun nyetel fitur nu sasuai jeung "noise" ieu jadi nol, nu ahirna bisa ningkatkeun akurasi image classification.

Dina speech recognition, hususna dina lingkungan nu gandeng (noisy) saperti sisi jalan atawa jero pabrik, Deep Residual Shrinkage Network meureun bisa ningkatkeun akurasi speech recognition, atawa sahenteuna, nawarkeun metodologi nu bisa ningkatkeun akurasi speech recognition.

## Referensi

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Dampak Akademis

Makalah ieu parantos nampi langkung ti 1,400 sitasi dina Google Scholar.

Dumasar kana statistik anu teu lengkep, Deep Residual Shrinkage Network (DRSN) parantos diterapkeun langsung atanapi dimodifikasi sareng diterapkeun dina langkung ti 1,000 publikasi/studi di sagala widang, kalebet teknik mesin (mechanical engineering), kakuatan listrik (electrical power), visi (vision), kasehatan (healthcare), sora (speech), teks (text), radar, sareng remote sensing.
