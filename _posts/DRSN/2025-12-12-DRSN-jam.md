---
layout: post
title: "Deep Residual Shrinkage Network: Wan AI Method fi Deal wid Highly Noisy Data"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-12
tags: [Deep Learning, AI]
mathjax: true
---

**Di Deep Residual Shrinkage Network a basically a upgraded version a di Deep Residual Network. Fi put it simple, di Deep Residual Shrinkage Network combine three main ting: Deep Residual Network, attention mechanisms, and soft thresholding functions.**

**Mek wi break down how di Deep Residual Shrinkage Network work. First ting, di network use attention mechanisms fi spot di feature dem weh nuh important. Den, di network use soft thresholding functions fi tun dem unimportant feature deh into zero. Same time, di network identify di important feature dem and keep dem safe. Dis process make di deep neural network stronger. It help di network extract useful feature from signal weh have nuff noise.**

## 1. Why Wi Do Dis Research (Research Motivation)

**First of all, noise a something yuh can't avoid when di algorithm a classify sample. Example a dem noise ya a Gaussian noise, pink noise, and Laplacian noise.** More broadly speaking, sample dem always have information weh nuh relate to di classification task weh wi a do. Wi can call dem irrelevant info "noise". Dis noise can mess up di classification performance. (Soft thresholding a wan key step ina nuff signal denoising algorithm.)

Fi example, imagine yuh deh pon road side a have a conversation. Di audio might have sound from car horn and wheel. Say we wah do speech recognition pon dem signal deh. Di background sound definitely a go affect di result. From a deep learning point of view, di deep neural network suppose to cut out di feature dem fi di horn and di wheel. When it do dat, it stop dem feature from mash up di speech recognition result.

**Secondly, the amount a noise ina each sample usually different. Dis happen even if di sample dem deh ina di same dataset.** (Dis variation similar to how attention mechanisms work. Take a image dataset fi example. The location a di target object might different ina every picture. Attention mechanisms can lock pon di specific location a di target object ina each image.)

Fi instance, say we a train a cat-and-dog classifier wid five image label as "dog." Image 1 might have a dog and a mouse. Image 2 might have a dog and a goose. Image 3 might have a dog and a chicken. Image 4 might have a dog and a donkey. Image 5 might have a dog and a duck. When training a gwaan, dem irrelevant object a go confuse di classifier. Mi a talk bout di mouse, goose, chicken, donkey, and duck. Dis interference cause di classification accuracy fi drop. Suppose we can identify dem irrelevant object deh. Den, we can eliminate di feature dem weh correspond to dem object. Ina dis way, we can improve di accuracy a di cat-and-dog classifier.

## 2. Soft Thresholding

**Soft thresholding a di core step ina nuff signal denoising algorithm. Di algorithm eliminate feature if di absolute value a di feature lower dan a certain threshold. But if di absolute value higher dan di threshold, di algorithm shrink di feature towards zero.** Researcher implement soft thresholding using dis formula:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Di derivative a di soft thresholding output with respect to the input a:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Di formula up top show seh di derivative a soft thresholding is either 1 or 0. Dis property exactly di same as di ReLU activation function. So den, soft thresholding can reduce di risk of gradient vanishing and gradient exploding ina deep learning algorithm.

**Ina di soft thresholding function, how yuh set di threshold must follow two rule. First, di threshold must be a positive number. Second, di threshold cannot bigger dan di maximum value a di input signal. If it bigger, di whole output a go just be zero.**

**Also, it better if di threshold follow a third rule. Every sample suppose to have its own independent threshold based on how much noise deh ina dat specific sample.**

Di reason is dat di noise content always vary between sample. Fi example, Sample A might have less noise while Sample B have nuff noise ina di same dataset. Ina dis case, Sample A should use a smaller threshold during soft thresholding. Sample B should use a larger threshold. Even though dem feature and threshold lose dem clear physical meaning inside a deep neural network, di basic logic stay di same. In other words, every sample must have a independent threshold. Di specific noise content determine dis threshold.

## 3. Attention Mechanism

Researcher can easy undastan attention mechanisms ina di field of computer vision. Animal visual system can distinguish target by scanning di whole area fast fast. After dat, di visual system focus attention pon di target object. Dis action allow di system fi extract more detail. Same time, di system block out irrelevant info. If yuh want more detail, check di literature bout attention mechanisms.

Di Squeeze-and-Excitation Network (SENet) represents a fairly new deep learning method weh use attention mechanisms. Across different sample, different feature channel contribute different amount to di classification task. SENet use a small sub-network fi **Learn a set of weights**. Den, SENet multiply dem weight ya by di feature of the respective channel. This operation adjust the magnitude of the feature ina each channel. Wi can si dis process as **Apply weighting to each feature channel** with different levels of attention.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Ina dis approach, every sample have a independent set of weights. That mean seh, the weights fi any two random sample are different. Ina SENet, di specific path fi get di weights a "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding wid Deep Attention Mechanism

Di Deep Residual Shrinkage Network use the structure of the SENet sub-network. Di network use dis structure fi run soft thresholding under a deep attention mechanism. Di sub-network (weh deh ina the red box) **Learn a set of thresholds**. Den, di network apply soft thresholding to each feature channel using dem threshold deh.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Ina dis sub-network, di system first calculate the absolute value of all feature ina the input feature map. Den, di system perform global average pooling and averaging fi get a feature, weh wi call *A*. Ina di next path, di system input the feature map into a small fully connected network after global average pooling. Dis fully connected network use the Sigmoid function as the final layer. Dis function normalize the output between 0 and 1. Dis process give wi a coefficient, weh wi call *α*. Wi can write di final threshold as *α × A*. So basically, di threshold is the product of two number. One number deh between 0 and 1. The other number is the average of the absolute values of the feature map. **Dis method make sure seh di threshold positive. It also ensure the threshold nuh too large.**

**Furthermore, different sample result in different threshold. So, we can look pon dis method as a specialized attention mechanism. Di mechanism spot feature weh irrelevant to the current task. Di mechanism transform dem feature into value close to zero via two convolutional layer. Den, di mechanism set dem feature to zero using soft thresholding. Or we can say, di mechanism spot feature weh relevant to the current task. Di mechanism transform dem feature into value far from zero via two convolutional layer. Finally, di mechanism preserve dem feature.**

At di end, we **Stack many basic modules**. We also include convolutional layer, batch normalization, activation function, global average pooling, and fully connected output layer. Dis process build di complete Deep Residual Shrinkage Network.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Di Deep Residual Shrinkage Network a a general method fi feature learning. Di reason a dat sample often have noise ina nuff feature learning task. Sample also have irrelevant info. Dem noise and irrelevant info might affect the performance of feature learning. Fi example:

Check image classification. One image might have nuff other object in deh same time. We can understand dem object as "noise." Di Deep Residual Shrinkage Network might able fi use the attention mechanism. Di network notice dis "noise." Den, di network use soft thresholding fi set the feature corresponding to dis "noise" to zero. Dis move likely improve image classification accuracy.

Check speech recognition. Specifically, tink bout noisy environment like conversation by a road side or inside a factory workshop. Di Deep Residual Shrinkage Network might improve speech recognition accuracy. Or at least, di network offer a methodology. Dis methodology capable fi improve speech recognition accuracy.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Dis paper receive over 1400 citation pon Google Scholar.

Based pon incomplete stats, researcher apply di Deep Residual Shrinkage Network (DRSN) ina over 1000 publication or study. Dem application cover a wide range a field. Dem field include mechanical engineering, electrical power, vision, healthcare, speech, text, radar, and remote sensing.
