---
layout: post
title: "<div style='text-align: center; direction: rtl;'>Deep Residual Shrinkage Network: Method-êki Artificial Intelligence bo Data-y Highly Noisy</div>"
subtitle: "<div style='text-align: center; direction: ltr;'>Deep Residual Shrinkage Network: An Artificial Intelligence Method for Highly Noisy Data</div>"
date: 2025-12-04
tags: [Deep Learning, AI, Fault Diagnosis]
mathjax: true
lang: ckb
dir: rtl
---

<!-- CSS Styling -->
<style>
  /* 1. Force Headers Right */
  h1, h2, h3, h4, h5, h6 { text-align: right !important; direction: rtl !important; unicode-bidi: embed; }
  
  /* 2. Force Lists Right */
  ul, ol { text-align: right !important; direction: rtl !important; padding-right: 40px; padding-left: 0; }

  /* 3. Force Code LTR */
  pre, code { text-align: left !important; direction: ltr !important; }
  
  /* 4. Force References LTR */
  .references-ltr { text-align: left !important; direction: ltr !important; }
</style>

<!-- Main Content Wrapper -->
<div dir="rtl" markdown="1" style="text-align: right; direction: rtl; font-family: 'Sakkal Majalla', 'Traditional Arabic', serif; font-size: 1.1em;">

<strong>Deep Residual Shrinkage Network variant-êki bash-tiri Deep Residual Network-e. Ba bonerrat, ama integration-i Deep Residual Network, attention mechanisms, u soft thresholding functions-e.</strong>

<strong>Ta radeyak, kar-kirdini Deep Residual Shrinkage Network detwanrêt awa têbigiyrêt: am network-e attention mechanisms bakar dehênêt bo nasinaway ew feature-aney ke girng nîn (unimportant features), u pashan soft thresholding functions bakar dehênêt bo sifir kirdinawayan (setting them to zero); ba pêchewanawa, feature-e girngekan denasêtewe u deyanparêzêt. Em process-e twanay deep neural network ziyatir dekat bo derhênani useful features lew signal-aney ke noise-yan têdaye.</strong>

## 1. Research Motivation

<strong>Yakem, katêk sample-kan classify dekeyn, bunî noise — wek Gaussian noise, pink noise, u Laplacian noise — shtêki nazirawa (inevitable).</strong> Be shêwayaki gishti-tir, zorjar sample-kan zanyari (information) wayan têdaye ke paywandi niyan be current classification task-ekewa, ke ama dekrêt wek noise seyr bikrêt. Em noise-ane detwanin karîgari xrap-yan habêt lasar performance-i classification. (<strong>Soft thresholding</strong> hangawêki sarakiya le zorbe-y algorîzmekani <strong>signal denoising</strong>.)

Ba nmuna, katêk lay qirax shaqam qsa dekeyt, dang-i qsa-kardanaka may-e têkall bêt lagall dangi horn-i sayara u tayey sayara. Katêk <strong>speech recognition</strong> (nasinaway dang) anjam dadayn lasar am signal-ane, anjamaka (results) bêguman karîgari dangi horn u taya-kani lasar debêt. Le rwangay <strong>deep learning</strong>-ewe, pêwista ew feature-aney paywandiyan ba horn u taya-kanawa haya labrên (eliminated) le naw <strong>deep neural network</strong>-ekada, bo away karîgari xrap-yan nabêt lasar anjami speech recognition-aka.

<strong>Duwam, tanat le naw haman dataset-ishda, brri noise zorjar le sample-êk bo sample-êki tir jyawaza.</strong> (Ama hawshêwaye lagall <strong>attention mechanisms</strong>; agar dataset-êki image warbigrin wek nmuna, shwêni ew object-ay maba-stmana layan mumkin e le har wênayakda jyawaz bêt, u attention mechanisms detwanêt focus bixata sar shwêni taybati object-aka le har wênayakda.)

Bo nmuna, katêk <strong>classifier</strong>-i sag-u-pshila (cat-and-dog) train dekeyn, ba wabihenin pênj wêna man haya ke wek "dog" label krawn. Wênay yakam mumkin e sag u mishk-i têda bêt, wênay duwam sag u qaz, wênay sêyam sag u mirishk, wênay chwaram sag u karr (donkey), u wênay pênjam sag u wurdaka (duck). Le kati training-da, classifier-aka bêguman tushi mudaxala (interference) debêt lelayen object-e napaywandidarakan wek mishk, qaz, mirishk, karr, u wurdaka, ke ama debête hoy kam bunaway <strong>classification accuracy</strong>. Agar bitwanin em object-e napaywandidarane binasin — watte mishk, qaz, mirishk, karr, u wurdaka — u ew feature-aney paywandiyan pêyanawa haya labareyn (eliminate), ewa mumkin e bitwanin accuracy-i cat-and-dog classifier-aka barz bkainawa.

## 2. Soft Thresholding

<strong>Soft thresholding hangawêki sarakiya (core step) le zorbe-y algorîzmekani signal denoising. Ama ew feature-aney ke absolute value-kayan kamtira le threshold-êki dyarikraw ladabhat (eliminate), u ew feature-aney ke absolute value-kayan gawratira lew threshold-e "shrink" dekat (kach dekat) baraw sifir.</strong> Ama detwanrêt jêbajê bikrêt ba bakarhênani em formula-ya:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Derivative-i output-i soft thresholding barambar ba input-aka briyti-ya le:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Wek le sarawa dyara, derivative-i soft thresholding ya 1-e ya 0. Em taybatmandiya hawshêway <strong>ReLU activation function</strong>-e. Boya, soft thresholding detwanêt metirsi (risk) tushbuni <strong>deep learning algorithms</strong> ba <strong>gradient vanishing</strong> u <strong>gradient exploding</strong> kam bkatawa.

<strong>Le soft thresholding function-da, danani threshold-aka pêwista du marj (conditions) jêbajê bkat: yakam, threshold-aka dabêt positive bêt (zhmara-yaki mujab bêt); duwam, threshold-aka nabêt le bahi hera gawray (maximum value) input signal-eke ziyatir bêt, agar na output-aka ba tawawati debêt ba sifir.</strong>

<strong>Harwaha, bash-tira ke threshold-aka marji sêyam-ish jêbajê bkat: har sample-êk dabêt threshold-i taybat be xoy (independent threshold) habêt ba pêy brri ew noise-ay ke têydayati.</strong>

Hokaraka amaya ke nawaroki noise zorjar jyawaza le naw sample-kanda. Bo nmuna, shtêki bawa ke le naw haman dataset-da, Sample A noise-i kamtiri têda bêt ballam Sample B noise-i ziyatiri têda bêt. Le katêki awada, katêk soft thresholding anjam dadayin le naw algorîzmêki denoising-da, Sample A pêwista threshold-êki bchuk-tir bakar bihênêt, le katêkda Sample B pêwista threshold-êki gawra-tir bakar bihênêt. Agar-chi em feature u threshold-ana manay fîzîki ashkarayan namênêt le naw deep neural networks, ballam mantiqe asasî-yaka har haman shta. Watte, har sample-êk dabêt threshold-i sarbaxoy habêt ke dyari-kraw bêt ba pêy nawaroki noise-aka-y.

## 3. Attention Mechanism

Têgahishtin le <strong>Attention mechanisms</strong> asan-tire le bwari <strong>computer vision</strong>. System-i bînini ajallan detwanêt target-kan jya bkatawa ba xێra scan kirdini hamu nawchaka, u pashan focus xistina sar target object-aka bo derhênani wurdakari ziyatir (extract more details) lagall sarrkutkirdini zanyari napaywandidar (suppressing irrelevant information). Bo zanyari wurd, tkaya sairi adabiyati paywandidar ba attention mechanisms bka.

<strong>Squeeze-and-Excitation Network (SENet)</strong> rêgayaki nisbatan nwêy deep learning-e ke attention mechanisms bakar dehênêt. Le naw sample-e jyawazakanda, baridari (contribution) channel-e jyawazakani feature bo classification task-aka zorjar jyawaza. SENet <strong>sub-network</strong>-êki bchuk bakar dehênêt bo badast-hênani komallêk weight (<strong>Learn a set of weights</strong>), u pashan em weight-ana zarb dekat le feature-i channel-e paywandidarakan bo gorrin (adjust) i qabaray feature-kan le har channel-êkda (<strong>Apply weighting to each feature channel</strong>). Em process-e detwanrêt bibinrêt wek anjamdani asti jyawazi attention bo sar feature channel-e jyawazakan.

Le em rêgaya-da, har sample-êk xawani set-êki sarbaxoya le weight. Ba watayaki tir, weight-eakan bo har du sample-êki jyawaz, jyawazan. Le naw SENet-da, ew rêirawa (path) y taybatay bo badast-hênani weight-akan bakardêt briytiya le "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

## 4. Soft Thresholding with Deep Attention Mechanism

<strong>Deep Residual Shrinkage Network</strong> sud (inspiration) lew structure-y sub-network-ey SENet war-dagirêt bo jêbajê kirdini soft thresholding le jêr <strong>deep attention mechanism</strong>. Le rêgay sub-network-ekawa (ke le naw qutiye surekeda nishan drawa), network-aka fêr debêt ke set-êk le threshold dabnêt (<strong>Learn a set of thresholds</strong>) bo anjamdani soft thresholding bo har feature channel-êk.

Le naw em sub-network-eda, sarata absolute value-i hamu feature-kani input feature map hesab dekrêt. Pashan, le rêgay <strong>global average pooling</strong> u average-kirdinawa, feature-êk badast dêt, ke ba A amajay bo dekrêt. Le rêirawakay tirda (path-ekay tir), feature map-eke pash global average pooling dachet-e naw <strong>fully connected network</strong>-êki bchuk. Em fully connected network-e <strong>Sigmoid function</strong> bakar dehênêt wek lay-i kotayi (final layer) bo normalize kirdini output-aka bo nêwan 0 u 1, ke coefficient-êk badast dehênêt ba navi α. Threshold-i kotayi detwanrêt denot bikrêt ba α×A. Kawata, threshold-aka briyti-ya le hasili zarbi zhmarayak le nêwan 0 u 1 lagall average-i absolute values-i feature map-aka. <strong>Em rêgaya zamin (ensure) dekat ke threshold-aka tanhar positive niya, ballku zor gawrash nabêt.</strong>

<strong>Ziyad lewa, sample-e jyawazakan debna hoy threshold-i jyawaz. Le anjamda, ta radeyak, ama detwanrêt lîkbidanaway bo bikrêt wek attention mechanism-êki taybatmand: feature-e napaywandidarakan ba task-i êsta denasêtewe, u le rêgay du convolutional layers digorrêt bo baha-yaki nizik le sifir, u pashan ba bakarhênani soft thresholding sifir-yan dekat; yan ba pêchewanawa, feature-e paywandidarakan ba task-i êsta denasêtewe, u le rêgay du convolutional layers digorrêt bo baha-yaki dur le sifir, u deyanparêzêt (preserves them).</strong>

Le kotayida, ba kalleka-kirdini (stacking) zhmara-yak le <strong>basic modules</strong> (<strong>Stack many basic modules</strong>) lagall convolutional layers, batch normalization, activation functions, global average pooling, u fully connected output layers, <strong>Deep Residual Shrinkage Network</strong>-i tawaw bunyad danrêt.

## 5. Generalization Capability

<strong>Deep Residual Shrinkage Network</strong>, le rastida, method-êki gishtiya bo <strong>feature learning</strong>. Ama hoyakay awaya ke le zor task-i feature learning-da, sample-kan kam ta zor noise u zanyari napaywandidar (irrelevant information) yan têdaye. Em noise u zanyari-ye napaywandidarana mumkin-e karîgari-yan habêt lasar performance-i feature learning. Bo nmuna:

Le <strong>Image classification</strong>, agar wênayak hawkat chandin object-i tiri têda bêt, em object-ana detwanrêt wek "noise" têbigiyrên. Deep Residual Shrinkage Network reng-e bitwanêt attention mechanism bakar bihênêt bo sarkijan (notice) be em "noise"-ane, u pashan <strong>soft thresholding</strong> bakar bihênêt bo sifir kirdini ew feature-aney ke paywandiyan ba em "noise"-anawa haya, bema-sh ehtimali barz bunaway image classification accuracy haya.

Le <strong>Speech recognition</strong>, ba taybati le hawkolêki (environment) noisy wek qsa-kardan lay qirax shaqam yan le naw kargayaki pîshasazi, Deep Residual Shrinkage Network reng-e accuracy-i speech recognition bash-tir bkat, yan layani kam, method-êk pêshkash dekat ke twanay bash-tirkirdini accuracy-i speech recognition-i habêt.

## Reference

<div class="references-ltr" dir="ltr" style="text-align: left; direction: ltr;">

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

<a href="https://ieeexplore.ieee.org/document/8850096">
https://ieeexplore.ieee.org/document/8850096</a>

</div>

## Academic Impact

Em paper-e ziyatir le 1400 jar <strong>citation</strong>-i bo kirawa le <strong>Google Scholar</strong>.

Ba pêy amar-e nasaraki-yekan (incomplete statistics), <strong>Deep Residual Shrinkage Network (DRSN)</strong> raste-wxo tatbiq kirawa yan ba destkari-yewa tatbiq kirawa le ziyatir le 1000 publications/studies le bwar-e jyawazakanda, wek mechanical engineering, electrical power, vision, healthcare, speech, text, radar, u remote sensing.

## BibTeX

```bibtex
@article{zhao2020deep,
  title={Deep residual shrinkage networks for fault diagnosis},
  author={Zhao, Minghang and Zhong, Shisheng and Fu, Xuyun and Tang, Baoping and Pecht, Michael},
  journal={IEEE Transactions on Industrial Informatics},
  volume={16},
  number={7},
  pages={4681--4690},
  year={2020},
  publisher={IEEE}
}
```
