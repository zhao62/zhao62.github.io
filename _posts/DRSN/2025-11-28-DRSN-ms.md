---
layout: post
title: "Deep Residual Shrinkage Network: Satu Kaedah Kecerdasan Buatan untuk Data yang Sangat Berhingar"
date: 2025-11-28
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network adalah satu varian penambahbaikan daripada Deep Residual Network (ResNet). Pada asasnya, ia adalah integrasi antara Deep Residual Network, mekanisme perhatian (*attention mechanisms*), dan fungsi *soft thresholding*.**

**Secara prinsipnya, cara kerja Deep Residual Shrinkage Network boleh difahami begini: ia menggunakan mekanisme perhatian untuk mengesan ciri-ciri (*features*) yang tidak penting, dan kemudian menggunakan fungsi *soft thresholding* untuk menukar nilai ciri-ciri tersebut kepada sifar; sebaliknya, ia mengesan ciri-ciri yang penting dan mengekalkannya. Proses ini meningkatkan keupayaan *deep neural network* untuk mengekstrak ciri-ciri berguna daripada isyarat yang mengandungi hingar (*noise*).**

## 1. Motivasi Kajian
**Pertama, apabila mengelaskan sampel, kehadiran hingar—seperti hingar Gauss (*Gaussian noise*), hingar merah jambu (*pink noise*), dan hingar Laplace—adalah tidak dapat dielakkan.** Secara lebih luas, sampel sering mengandungi informasi yang tidak relevan dengan tugas pengelasan semasa, dan informasi ini juga boleh dianggap sebagai hingar. Hingar ini boleh menjejaskan prestasi pengelasan secara negatif. (Perlu diketahui bahawa *soft thresholding* adalah langkah kunci dalam banyak algoritma penyahhingaran isyarat atau *signal denoising*).

Sebagai contoh, semasa berbual di tepi jalan, suara perbualan mungkin bercampur dengan bunyi hon kereta dan bunyi tayar. Apabila pengecaman suara (*speech recognition*) dilakukan ke atas isyarat ini, hasilnya pasti akan terjejas oleh bunyi latar belakang tersebut. Dari sudut pandang *deep learning*, ciri-ciri yang mewakili bunyi hon dan tayar ini sepatutnya disingkirkan di dalam *deep neural network* untuk mengelakkannya daripada menjejaskan hasil pengecaman suara.

**Kedua, walaupun dalam set data yang sama, jumlah hingar sering berbeza antara satu sampel dengan sampel yang lain.** (Konsep ini ada persamaan dengan mekanisme perhatian; mengambil set data imej sebagai contoh, lokasi objek sasaran mungkin berbeza dalam setiap gambar, dan mekanisme perhatian boleh memberi tumpuan kepada lokasi spesifik objek sasaran dalam setiap imej).

Contohnya, apabila melatih pengelas kucing-dan-anjing (*cat-and-dog classifier*), katakan ada 5 imej yang dilabel sebagai "anjing". Imej pertama mungkin mengandungi anjing dan tikus, imej kedua mengandungi anjing dan angsa, imej ketiga mengandungi anjing dan ayam, imej keempat mengandungi anjing dan keldai, dan imej kelima mengandungi anjing dan itik. Semasa latihan, pengelas (*classifier*) pasti akan terganggu oleh objek yang tidak relevan seperti tikus, angsa, ayam, keldai, dan itik, yang menyebabkan ketepatan pengelasan menurun. Jika kita mampu mengesan objek-objek tidak relevan ini dan menghapuskan ciri-ciri yang berkaitan dengannya, kita berpotensi meningkatkan ketepatan pengelas kucing-dan-anjing tersebut.

## 2. Soft Thresholding
**Soft thresholding adalah langkah teras dalam banyak algoritma penyahhingaran isyarat (*signal denoising*). Ia menghapuskan ciri-ciri yang nilai mutlaknya lebih rendah daripada *threshold* tertentu, dan "mengecutkan" (*shrinks*) ciri-ciri yang nilai mutlaknya lebih tinggi daripada *threshold* tersebut ke arah sifar.** Ia boleh dilaksanakan menggunakan formula berikut:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Terbitan (*derivative*) bagi output *soft thresholding* terhadap input adalah:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Seperti yang ditunjukkan di atas, terbitan bagi *soft thresholding* adalah sama ada 1 atau 0. Sifat ini adalah sama dengan fungsi pengaktifan ReLU (*ReLU activation function*). Oleh itu, *soft thresholding* juga mampu mengurangkan risiko algoritma *deep learning* menghadapi masalah kecerunan hilang (*gradient vanishing*) dan kecerunan meletup (*gradient exploding*).

**Dalam fungsi *soft thresholding*, tetapan nilai *threshold* mesti mematuhi dua syarat: Pertama, *threshold* mestilah nombor positif; Kedua, *threshold* tidak boleh melebihi nilai maksimum isyarat input, jika tidak, output akan menjadi sifar sepenuhnya.**

**Tambahan pula, nilai *threshold* sebaiknya mematuhi syarat ketiga: setiap sampel harus mempunyai *threshold* sendiri yang bebas berdasarkan kandungan hingarnya.**

Ini kerana kandungan hingar selalunya berbeza antara sampel. Sebagai contoh, sering berlaku dalam set data yang sama, Sampel A mengandungi kurang hingar manakala Sampel B mengandungi banyak hingar. Dalam kes ini, apabila melakukan *soft thresholding* dalam algoritma penyahhingaran, Sampel A sepatutnya menggunakan *threshold* yang lebih kecil, manakala Sampel B sepatutnya menggunakan *threshold* yang lebih besar. Dalam *deep neural network*, walaupun ciri-ciri dan *threshold* ini kehilangan definisi fizikal yang jelas, logik asasnya tetap sama. Dengan kata lain, setiap sampel harus mempunyai *threshold* sendiri yang ditentukan oleh kandungan hingar spesifiknya.

## 3. Mekanisme Perhatian (Attention Mechanism)
Mekanisme perhatian agak mudah difahami dalam bidang penglihatan komputer (*computer vision*). Sistem visual haiwan boleh mengimbas seluruh kawasan dengan pantas untuk menemui objek sasaran, dan kemudian memusatkan perhatian pada objek sasaran tersebut untuk mengekstrak lebih banyak butiran sambil menyekat informasi yang tidak relevan. Untuk butiran lanjut, sila rujuk literatur berkaitan mekanisme perhatian.

Squeeze-and-Excitation Network (SENet) adalah kaedah *deep learning* yang agak baharu yang menggunakan mekanisme perhatian. Dalam sampel yang berbeza, sumbangan saluran ciri (*feature channels*) yang berbeza terhadap tugas pengelasan selalunya tidak sama. SENet menggunakan satu sub-rangkaian (*sub-network*) kecil untuk mendapatkan satu set pemberat (*weights*), dan kemudian mendarabkan pemberat ini dengan ciri-ciri pada setiap saluran untuk melaraskan saiz ciri-ciri tersebut. Proses ini boleh dianggap sebagai memberikan tahap perhatian yang berbeza kepada saluran ciri yang berbeza.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-ms/SENET_ms_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Dalam kaedah ini, setiap sampel mempunyai set pemberatnya sendiri. Dalam erti kata lain, pemberat bagi mana-mana dua sampel adalah berbeza. Dalam SENet, laluan spesifik untuk mendapatkan pemberat adalah "Global Pooling → Fully Connected Layer → Fungsi ReLU → Fully Connected Layer → Fungsi Sigmoid".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-ms/SENET_ms_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding dengan Mekanisme Perhatian Mendalam
Deep Residual Shrinkage Network mengambil inspirasi daripada struktur sub-rangkaian SENet yang disebutkan di atas untuk melaksanakan *soft thresholding* di bawah mekanisme perhatian mendalam. Melalui sub-rangkaian (yang ditunjukkan dalam kotak merah), satu set *threshold* boleh dipelajari untuk melakukan *soft thresholding* pada setiap saluran ciri.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-ms/DRSN_ms_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Dalam sub-rangkaian ini, nilai mutlak bagi semua ciri dalam peta ciri input (*input feature map*) dikira terlebih dahulu. Kemudian, melalui *global average pooling* dan pemurataan, satu ciri diperolehi, yang ditanda sebagai A. Dalam laluan yang satu lagi, peta ciri selepas *global average pooling* dimasukkan ke dalam satu rangkaian *fully connected* yang kecil. Rangkaian ini menggunakan fungsi Sigmoid sebagai lapisan terakhir untuk menormalkan output kepada nilai antara 0 dan 1, menghasilkan satu pekali yang ditanda sebagai α. *Threshold* akhir boleh diungkapkan sebagai α×A. Oleh itu, *threshold* tersebut adalah hasil darab antara satu nombor (antara 0 dan 1) dengan purata nilai mutlak peta ciri tersebut. **Kaedah ini memastikan bahawa *threshold* bukan sahaja positif, tetapi juga tidak terlalu besar.**

**Tambahan pula, sampel yang berbeza akan menghasilkan *threshold* yang berbeza. Oleh itu, sedikit sebanyak, ini boleh difahami sebagai satu mekanisme perhatian yang istimewa: ia mengesan ciri-ciri yang tidak relevan dengan tugas semasa, menukarnya menjadi nilai yang menghampiri sifar melalui dua lapisan konvolusi, dan menetapkannya kepada sifar menggunakan *soft thresholding*; atau sebaliknya, ia mengesan ciri-ciri yang relevan, menukarnya menjadi nilai yang jauh dari sifar, dan mengekalkannya.**

Akhir sekali, dengan menyusun (*stacking*) sejumlah modul asas bersama lapisan konvolusi, *batch normalization*, fungsi pengaktifan, *global average pooling*, dan lapisan output *fully connected*, maka terhasillah Deep Residual Shrinkage Network yang lengkap.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-ms/DRSN_ms_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Keupayaan Generalisasi
Deep Residual Shrinkage Network sebenarnya adalah satu kaedah pembelajaran ciri (*feature learning*) yang umum. Ini kerana dalam banyak tugas pembelajaran ciri, sampel sedikit sebanyak mengandungi hingar serta informasi yang tidak relevan. Hingar dan informasi tidak relevan ini mungkin menjejaskan prestasi pembelajaran ciri. Contohnya:

Dalam pengelasan imej, jika imej turut mengandungi banyak objek lain, objek-objek ini boleh difahami sebagai "hingar". Deep Residual Shrinkage Network mungkin boleh menggunakan mekanisme perhatian untuk mengesan "hingar" ini, dan kemudian menggunakan *soft thresholding* untuk menetapkan ciri-ciri yang berkaitan dengan "hingar" ini kepada sifar, sekaligus berpotensi meningkatkan ketepatan pengelasan imej.

Dalam pengecaman suara, terutamanya dalam persekitaran yang agak bising seperti berbual di tepi jalan atau di dalam bengkel kilang, Deep Residual Shrinkage Network mungkin boleh meningkatkan ketepatan pengecaman suara, atau sekurang-kurangnya, memberikan satu idea yang mampu meningkatkan ketepatan pengecaman suara.

## Rujukan

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Impak Akademik

Kertas kerja ini telah menerima lebih daripada 1400 petikan (*citations*) di Google Scholar.

Mengikut statistik konservatif, Deep Residual Shrinkage Network telah digunakan secara langsung atau ditambah baik dalam lebih daripada 1000 penerbitan akademik dalam pelbagai bidang termasuk kejuruteraan mekanikal, kuasa elektrik, penglihatan komputer (*visual*), perubatan, suara, teks, radar, dan penderiaan jauh (*remote sensing*).
