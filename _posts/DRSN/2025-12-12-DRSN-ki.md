---
layout: post
title: "Deep Residual Shrinkage Network: Njĩra ya Artificial Intelligence ya kũhũthĩrwo harĩ Highly Noisy Data"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-12
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network nĩ mũthemba mwagĩrie wa Deep Residual Network. Kĩhoto kĩnene nĩ atĩ, Deep Residual Shrinkage Network yunganagia Deep Residual Network, attention mechanisms, na soft thresholding functions.**

**No tũtaũkĩrwo nĩ ũrutithio wa wĩra wa Deep Residual Shrinkage Network na njĩra ĩno. Mbere na mbere, network ĩyo ĩhũthagĩra attention mechanisms nĩguo ĩmenye features iria itarĩ na bata (unimportant features). Gũcoka, network ĩyo ĩhũthagĩra soft thresholding functions gũtua features icio itarĩ na bata zero. Na njĩra ĩyo ĩngĩ, network ĩyo nĩ ĩmenyaga features iria irĩ na bata na ĩkacigiria. Process ĩno nĩ yongagĩrĩra ũhoti wa deep neural network. Process ĩno ĩteithagia network gũthura features iria irĩ na ũguni kuuma harĩ signals iria irĩ na noise.**

## 1. Kĩrĩra kĩa Ũthuthuria (Research Motivation)

**Mbere na mbere, noise nĩ ũndũ ũtangĩthembo rĩrĩa algorithm ĩratua samples (classifies samples). Ngerekano cia noise ĩno nĩ ta Gaussian noise, pink noise, na Laplacian noise.** Na njĩra nene, samples kaingĩ cikoragwo na ũhoro ũtarĩ na bata na classification task ĩrĩa ĩrarutwo. No tũtaũkĩrwo nĩ ũhoro ũcio ũtarĩ na bata ta arĩ noise. Noise ĩno no ĩnyihie ũhoti wa classification. (Soft thresholding nĩ ikinya rĩa bata mũno thĩinĩ wa algorithms nyingĩ cia signal denoising.)

Kwa ngerekano, ta wĩcirie ndeereti ĩgĩthie na mbere mũkĩra-inĩ wa barabara. Mũgambo ũcio (audio) no ũkorwo na mĩgambo ya mĩbugĩrĩrio ya ngari na magũrũ macio. No tũgerie gwĩka speech recognition harĩ signals icio. Mĩgambo ĩyo ya background nĩ ĩkũhinyirĩria maumĩrĩra (results). Kuuma mwerekera wa deep learning, deep neural network nĩ yagĩrĩirwo kũniina features iria ciumanĩte na mĩbugĩrĩrio ya ngari na magũrũ. Kũniina ũndũ ũcio nĩ gũtũmaga features icio ciage gũthũkia maumĩrĩra ma speech recognition.

**Ya kerĩ, ũnene wa noise kaingĩ nĩ ũcenjagia gatagatĩ ka samples. Gũcenjia kũu gũkĩkaga o na thĩinĩ wa dataset ĩmwe.** (Gũcenjia kũu kũrĩ na ũndũ kũhaanaine na attention mechanisms. Ta wĩcirie dataset ya mbica/images. Harĩa kĩndũ kĩrĩa kĩrabatara kuoneka kĩrĩ (target object) no gũkorwo gũkĩgarũrũka kuuma mbica ĩmwe nginya ĩrĩa ĩngĩ. Attention mechanisms no ihote kwerekeria maitho harĩa hasuo target object ĩrĩ thĩinĩ wa o image.)

Kwa ngerekano, ta wĩcirie tũkĩmenyeria (training) classifier ya mbaka na ngui tũkĩhũthĩra mbica ithano cietĩtwo "ngui" (dog). Image 1 no ĩkorwo na ngui na mbĩa (mouse). Image 2 no ĩkorwo na ngui na bata (goose). Image 3 no ĩkorwo na ngui na ngũkũ (chicken). Image 4 no ĩkorwo na ngui na ndigiri (donkey). Image 5 no ĩkorwo na ngui na thambiri (duck). Hĩndĩ ya training, indo ici itarĩ na bata nĩ irĩthũkagia wĩra wa classifier. Indo icio nĩ ta mbĩa, bata, ngũkũ, ndigiri, na thambiri. Gũthũkia ũguo nĩ gũtũmaga classification accuracy ĩnyihe. Korwo no tũhote kũmenya indo ici itarĩ na bata. Rĩu, no tũniine features iria ciumanĩte na indo icio. Na njĩra ĩyo, no tũnyite ũhoti wa gũtua gwa classifier ya mbaka na ngui ũnenehe.

## 2. Soft Thresholding

**Soft thresholding nĩ ikinya rĩmunyu mũno thĩinĩ wa algorithms nyingĩ cia signal denoising. Algorithm nĩ ĩniinaga features angĩkorwo absolute values cia features icio nĩ nini gũkĩra threshold ĩna. Algorithm nĩ ĩnyihagia features icio ĩkacitwara mwerekera wa zero angĩkorwo absolute values cia features icio nĩ nene gũkĩra threshold ĩyo (shrinks features towards zero).** Athuthuria no mahũthĩre formula ĩno gwĩka soft thresholding:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Derivative ya soft thresholding output kuringana na input nĩ:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Formula ĩyo ya igũrũ ĩronania atĩ derivative ya soft thresholding nĩ 1 kana 0. Ũndũ ũyũ nĩ ũhaanaine biũ na ũrĩa ReLU activation function ĩrutaga wĩra. Kwoguo, soft thresholding no ĩnyihie ũgwo wa gradient vanishing na gradient exploding thĩinĩ wa deep learning algorithms.

**Thĩinĩ wa soft thresholding function, gũtuwo kwa threshold no mũhaka kũhingie maumĩrĩra meerĩ. Rĩa mbere, threshold no mũhaka ĩkorwo ĩrĩ namba positive. Rĩa kerĩ, threshold ndĩragĩrĩrwo gũkĩra maximum value ya input signal. Kwaga ũguo, output yothe ĩgũtuĩka zero.**

**Makĩria ma ũguo, nĩ kaba threshold ĩhingie ũndũ wa gatatũ. O sample nĩ yagĩrĩirwo gũkorwo na independent threshold kuringana na ũnene wa noise ĩrĩa ĩrĩ thĩinĩ wa sample ĩyo.**

Kĩhoto nĩ atĩ, ũnene wa noise kaingĩ nĩ ũcenjagia gatagatĩ ka samples. Kwa ngerekano, Sample A no ĩkorwo na noise nini no Sample B ĩkorwo na noise nyingĩ thĩinĩ wa dataset o ĩyo. Ha ũhoro ũcio, Sample A nĩ yagĩrĩirwo kũhũthĩra threshold nini hĩndĩ ya soft thresholding. Sample B nayo yagĩrĩirwo kũhũthĩra threshold nene. Thĩinĩ wa deep neural networks, features na thresholds ici no ciage gũkorwo na physical definitions ironeka wega. No, logic ĩyo ya mũthingi ĩtũuraga o ũguo. Nĩ kũuga, o sample yagĩrĩirwo gũkorwo na independent threshold. Ũnene wa noise harĩ sample ĩyo nĩguo ũtuaga threshold ĩyo.

## 3. Attention Mechanism

Athuthuria no mataũkĩrwo nĩ attention mechanisms na ũhũthũ thĩinĩ wa computer vision. Maitho ma nyamũ no mahote gũkũũrana targets na njĩra ya gũthuthuria kũndũ guothe na ihenya. Gũcoka, maitho macio makerekeria attention yao harĩ target object. Ikinya rĩu nĩ rĩhotithagia systems icio kũona details nyingĩ. Hĩndĩ o ĩyo, systems icio igatigana na ũhoro ũrĩa ũtarĩ na bata. Nĩguo ũmenye makĩria, thoma mabuku megiĩ attention mechanisms.

Squeeze-and-Excitation Network (SENet) nĩ njĩra njerũ ya Deep Learning ĩhũthagĩra attention mechanisms. Gatagatĩ ka samples itiganĩte, feature channels itiganĩte nĩ iteithagia na njĩra itiganĩte harĩ classification task. SENet ĩhũthagĩra sub-network nini kũgĩa gĩkundi kĩa weights (a set of weights). Gũcoka, SENet ĩka-multiply weights ici na features cia channels icio. Wĩra ũyũ nĩ ũgarũraga ũnene wa features thĩinĩ wa o channel. No tũtaũkĩrwo nĩ process ĩno ta kũhũthĩra mĩthemba ĩtiganĩte ya attention harĩ feature channels itiganĩte.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Na njĩra ĩno, o sample ĩkoragwo na independent set of weights. Nĩ kũuga, weights cia samples igĩrĩ o ciothe itiganĩte. Thĩinĩ wa SENet, njĩra ya kũgĩa weights icio nĩ "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Deep Residual Shrinkage Network ĩhũthagĩra mwakĩre wa sub-network ya SENet. Network ĩyo ĩhũthagĩra mwakĩre ũcio kũruta wĩra wa soft thresholding rungu rwa deep attention mechanism. Sub-network ĩyo (ĩronanio thĩinĩ wa gathandũkũ gatune) nĩ yĩrutaga gĩkundi kĩa thresholds (**Learns a set of thresholds**). Gũcoka, network ĩyo ĩkahũthĩra soft thresholding harĩ o feature channel ĩkĩhũthĩra thresholds ici.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Thĩinĩ wa sub-network ĩyo, system mbere na mbere nĩ ĩtaraga absolute values cia features ciothe thĩinĩ wa input feature map. Gũcoka, system ĩyo ĩkahaanda global average pooling na gũ-averaging nĩguo ĩgĩe na feature, ĩrĩa tũkũĩta A. Harĩ njĩra ĩrĩa ĩngĩ, system ĩyo ĩkaingĩria feature map thĩinĩ wa fully connected network nini thutha wa global average pooling. Fully connected network ĩyo ĩhũthagĩra Sigmoid function ta layer ya mũico. Function ĩno ĩtũmaga output ĩkorwo gatagatĩ ka 0 na 1. Process ĩno ĩciaraga coefficient, ĩrĩa tũkũĩta α. No tũandĩke threshold ya mũico ta α × A. Kwoguo, threshold nĩ maciaro ma namba igĩrĩ. Namba ĩmwe ĩrĩ gatagatĩ ka 0 na 1. Namba ĩyo ĩngĩ nĩ average ya absolute values cia feature map. **Njĩra ĩno nĩ ĩtũmaga threshold ĩkorwo ĩrĩ positive. Njĩra ĩno ningĩ nĩ ĩgiragĩrĩria threshold kũneneha mũno.**

**Makĩria ma ũguo, samples itiganĩte iciaraga thresholds itiganĩte. Nĩ ũndũ ũcio, no tũtaũkĩrwo nĩ njĩra ĩno ta attention mechanism ya mwanya. Mechanism ĩyo nĩ ĩmenyaga features iria itarĩ na bata na current task. Mechanism ĩyo ĩgarũraga features icio igatuĩka values ikuhĩrĩirie zero kũgerera convolutional layers igĩrĩ. Gũcoka, mechanism ĩyo ĩgatua features icio zero ĩkĩhũthĩra soft thresholding. Na njĩra ĩngĩ, mechanism ĩyo nĩ ĩmenyaga features iria irĩ na bata na current task. Mechanism ĩyo ĩgarũraga features icio igatuĩka values irĩ haraya na zero kũgerera convolutional layers igĩrĩ. Mũico-inĩ, mechanism ĩyo ĩkarekereria features icio (preserves these features).**

Mũthya-inĩ, tũkahaicithania (**Stack many basic modules**) gĩcunjĩ kĩa basic modules. Ningĩ nĩ tũkwingĩria convolutional layers, batch normalization, activation functions, global average pooling, na fully connected output layers. Process ĩno nĩyo yakaga Deep Residual Shrinkage Network yothe ĩkagana.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Deep Residual Shrinkage Network nĩ method ya kũhũthĩrwo kũndũ kwingĩ harĩ feature learning. Kĩhoto nĩ atĩ, samples kaingĩ cikoragwo na noise thĩinĩ wa feature learning tasks nyingĩ. Samples ningĩ cikoragwo na ũhoro ũtarĩ na bata (irrelevant information). Noise ĩno na ũhoro ũyũ ũtarĩ na bata no ithũkie ũhoti wa feature learning. Kwa ngerekano:

Ta wĩcirie image classification. Mbica (image) no ĩkorwo na indo nyingĩ. No tũtaũkĩrwo nĩ indo ici ta "noise." Deep Residual Shrinkage Network no ĩhote kũhũthĩra attention mechanism. Network ĩyo nĩ ĩmenyaga "noise" ĩyo. Gũcoka, network ĩyo ĩkahũthĩra soft thresholding, gũtua features iria ciumanĩte na "noise" ĩyo zero. Ikinya rĩu no rĩtũme image classification accuracy ĩnenehe.

Ta wĩcirie speech recognition. Makĩria, ta wĩcirie mazingira marĩ na mbugĩrĩrio ta ndeereti mũkĩra-inĩ wa barabara kana thĩinĩ wa kĩwanda (factory workshop). Deep Residual Shrinkage Network no ĩtũme speech recognition accuracy ĩnenehe. Kana o na kũrĩ na ũguo, network ĩyo nĩ ĩheananaga methodology. Methodology ĩyo ĩrĩ na ũhoti wa kũnenehia speech recognition accuracy.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Ũhoti wa Academic Impact (Academic Impact)

Paper ĩno nĩ yonekanĩte (cited) maita makĩria ma 1,400 thĩinĩ wa Google Scholar.

Kuringana na statistics itarĩ njigu, athuthuria nĩ mahũthĩrĩte Deep Residual Shrinkage Network (DRSN) thĩinĩ wa publications/studies makĩria ma 1,000. Mahũthĩro maya (applications) nĩ mahumbĩrĩte mĩgunda mĩingĩ ya ũthuthuria. Mĩgunda ĩyo nĩ hamwe na mechanical engineering, electrical power, vision, healthcare, speech, text, radar, na remote sensing.
