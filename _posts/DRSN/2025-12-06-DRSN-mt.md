---
layout: post
title: "Deep Residual Shrinkage Network: Metodu ta' Artificial Intelligence għal Data b'Livell Għoli ta' Noise"
date: 2025-12-06
tags: [Deep Learning, AI]
mathjax: true
description: "Id-Deep Residual Shrinkage Network huwa varjant imtejjeb tad-Deep Residual Network. Essenzjalment, huwa integrazzjoni tad-Deep Residual Network, attention mechanisms, u soft thresholding functions."
---

**Id-Deep Residual Shrinkage Network huwa varjant imtejjeb tad-Deep Residual Network. Essenzjalment, huwa integrazzjoni tad-Deep Residual Network, attention mechanisms, u soft thresholding functions.**

**Sa ċertu punt, il-prinċipju ta' kif jaħdem id-Deep Residual Shrinkage Network jista' jinftiehem hekk: juża attention mechanisms biex jidentifika unimportant features u jimpjega soft thresholding functions biex jagħmilhom zero; mill-banda l-oħra, jidentifika important features u jżommhom. Dan il-proċess isaħħaħ il-kapaċità tad-deep neural network li jiġbed useful features minn sinjali li fihom in-noise.**

## 1. Research Motivation

**L-ewwel nett, meta nikklassifikaw is-samples, il-preżenza tan-noise—bħal Gaussian noise, pink noise, u Laplacian noise—hija inevitabbli.** F'sens aktar wiesa', is-samples spiss ikun fihom informazzjoni irrilevanti għall-classification task kurrenti, li tista' tiġi interpretata wkoll bħala noise. Dan in-noise jista' jaffettwa ħażin il-classification performance. (Is-**Soft thresholding** huwa pass ewlieni f'ħafna signal denoising algorithms.)

Pereżempju, waqt konverżazzjoni f'tarf it-triq, l-awdjo jista' jkun imħallat mal-ħsejjes tal-ħornijiet tal-karozzi u r-roti. Meta nagħmlu speech recognition fuq dawn is-sinjali, ir-riżultati inevitabbilment jiġu affettwati minn dawn il-ħsejjes fl-isfond. Minn perspettiva ta' deep learning, il-features li jikkorrispondu għall-ħornijiet u r-roti għandhom jiġu eliminati fi ħdan id-deep neural network biex ma jħalluhomx jaffettwaw ir-riżultati tal-speech recognition.

**It-tieni, anke fl-istess dataset, l-ammont ta' noise spiss ivarja minn sample għal sample.** (Dan għandu xebh mal-attention mechanisms; jekk nieħdu image dataset bħala eżempju, il-post tal-oħġett fil-mira jista' jkun differenti bejn l-istampi, u l-attention mechanisms jistgħu jiffukaw fuq il-post speċifiku tal-oħġett fil-mira f'kull stampa.)

Pereżempju, meta nħarrġu classifier tal-qtates u l-klieb, ikkunsidra ħames stampi ttikkettjati bħala "kelb". L-ewwel stampa jista' jkun fiha kelb u ġurdien, it-tieni kelb u wiżża, it-tielet kelb u tiġieġa, ir-raba' kelb u ħmar, u l-ħames kelb u papra. Waqt it-training, il-classifier inevitabbilment se jkun suġġett għal interferenza minn oġġetti irrilevanti bħal ġrieden, wiżż, tiġieġ, ħmir u papri, li jirriżulta fi tnaqqis fil-classification accuracy. Jekk nistgħu nidentifikaw dawn l-oġġetti irrilevanti—il-ġrieden, il-wiżż, it-tiġieġ, il-ħmir u l-papri—u neliminaw il-features korrispondenti tagħhom, huwa possibbli li ntejbu l-accuracy tal-classifier tal-qtates u l-klieb.

## 2. Soft Thresholding

**Is-Soft thresholding huwa pass ewlieni f'ħafna signal denoising algorithms. Jelimina l-features li l-valuri assoluti tagħhom huma inqas minn ċertu threshold u jnaqqas (shrinks) il-features li l-valuri assoluti tagħhom huma ogħla minn dan it-threshold lejn iż-żero.** Jista' jiġi implimentat bil-formula li ġejja:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Id-derivattiv tal-output tas-soft thresholding fir-rigward tal-input huwa:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Kif muri hawn fuq, id-derivattiv tas-soft thresholding huwa jew 1 jew 0. Din il-proprjetà hija identika għal dik tar-ReLU activation function. Għalhekk, is-soft thresholding jista' wkoll inaqqas ir-riskju li d-deep learning algorithms jiltaqgħu ma' gradient vanishing u gradient exploding.

**Fil-funzjoni tas-soft thresholding, l-issettjar tat-threshold irid jissodisfa żewġ kundizzjonijiet: l-ewwel, it-threshold irid ikun numru pożittiv; it-tieni, it-threshold ma jistax jaqbeż il-valur massimu tas-sinjal tal-input, inkella l-output ikun kollu żero.**

**Barra minn hekk, huwa preferibbli li t-threshold jissodisfa t-tielet kundizzjoni: kull sample għandu jkollu t-threshold indipendenti tiegħu stess ibbażat fuq il-kontenut tan-noise tiegħu.**

Dan għaliex il-kontenut tan-noise spiss ivarja bejn is-samples. Pereżempju, huwa komuni fl-istess dataset li Sample A jkun fih inqas noise filwaqt li Sample B jkun fih aktar noise. F'dan il-każ, meta jsir soft thresholding f'denoising algorithm, Sample A għandu juża threshold iżgħar, filwaqt li Sample B għandu juża threshold akbar. Għalkemm dawn il-features u thresholds jitilfu d-definizzjonijiet fiżiċi espliċiti tagħhom fid-deep neural networks, il-loġika bażika tibqa' l-istess. Fi kliem ieħor, kull sample għandu jkollu t-threshold indipendenti tiegħu ddeterminat mill-kontenut speċifiku tan-noise tiegħu.

## 3. Attention Mechanism

L-Attention mechanisms huma relattivament faċli biex jinftiehmu fil-qasam tal-computer vision. Is-sistemi viżivi tal-annimali jistgħu jiddistingwu l-miri billi jiskennjaw malajr iż-żona kollha, u sussegwentement jiffukaw l-attention fuq l-oġġett fil-mira biex jiġbdu aktar dettalji filwaqt li jrażżnu informazzjoni irrilevanti. Għal dettalji speċifiċi, jekk jogħġbok irreferi għal-letteratura dwar l-attention mechanisms.

Is-Squeeze-and-Excitation Network (SENet) jirrappreżenta metodu ta' deep learning relattivament ġdid li juża attention mechanisms. F'samples differenti, il-kontribuzzjoni ta' feature channels differenti għall-classification task spiss tvarja. Is-SENet juża sub-network żgħir biex jikseb sett ta' weights (Learn a set of weights) u mbagħad jimmultiplika dawn il-weights mal-features tal-channels rispettivi biex jaġġusta l-kobor tal-features f'kull channel. Dan il-proċess jista' jitqies bħala **Apply weighting to each feature channel** (l-applikazzjoni ta' livelli differenti ta' attention fuq feature channels differenti).

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

F'dan l-approċċ, kull sample jippossjedi s-sett indipendenti ta' weights tiegħu stess. Fi kliem ieħor, il-weights għal kwalunkwe żewġ samples arbitrarji huma differenti. Fis-SENet, il-mogħdija speċifika biex jinkisbu l-weights hija "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Id-Deep Residual Shrinkage Network jieħu ispirazzjoni mill-istruttura tas-sub-network tas-SENet imsemmija hawn fuq biex jimplimenta soft thresholding taħt deep attention mechanism. Permezz tas-sub-network (indikat fil-kaxxa l-ħamra), jista' jiġi mgħallem sett ta' thresholds (**Learn a set of thresholds**) biex japplika soft thresholding fuq kull feature channel.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

F'dan is-sub-network, l-ewwel jiġu kkalkulati l-valuri assoluti tal-features kollha fil-input feature map. Imbagħad, permezz ta' global average pooling u l-medja, jinkiseb feature, indikat bħala $A$. Fil-mogħdija l-oħra, il-feature map wara l-global average pooling tiddaħħal f'netwerk żgħir fully connected. Dan in-netwerk juża s-Sigmoid function bħala s-saff finali biex jinnormalizza l-output bejn 0 u 1, u jagħti koeffiċjent indikat bħala $\alpha$. It-threshold finali jista' jiġi espress bħala $\alpha \times A$. Għalhekk, it-threshold huwa l-prodott ta' numru bejn 0 u 1 u l-medja tal-valuri assoluti tal-feature map. **Dan il-metodu jiżgura li t-threshold mhux biss ikun pożittiv iżda wkoll mhux kbir wisq.**

**Barra minn hekk, samples differenti jirriżultaw fi thresholds differenti. Konsegwentement, sa ċertu punt, dan jista' jiġi interpretat bħala attention mechanism speċjalizzat: jidentifika features irrilevanti għat-task kurrenti, jittrasformahom f'valuri qrib iż-żero permezz ta' żewġ convolutional layers, u jagħmilhom żero billi juża soft thresholding; alternattivament, jidentifika features rilevanti għat-task kurrenti, jittrasformahom f'valuri 'l bogħod minn żero permezz ta' żewġ convolutional layers, u jippreservahom.**

Fl-aħħarnett, billi jinġabru numru ta' basic modules (**Stack many basic modules**) flimkien ma' convolutional layers, batch normalization, activation functions, global average pooling, u fully connected output layers, jinbena d-Deep Residual Shrinkage Network komplut.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Id-Deep Residual Shrinkage Network huwa, fil-fatt, metodu ġenerali ta' feature learning. Dan għaliex, f'ħafna feature learning tasks, is-samples ftit jew wisq ikun fihom ftit noise kif ukoll informazzjoni irrilevanti. Dan in-noise u l-informazzjoni irrilevanti jistgħu jaffettwaw il-performance tal-feature learning. Pereżempju:

Fl-image classification, jekk immaġini simultanjament ikun fiha ħafna oġġetti oħra, dawn l-oġġetti jistgħu jinftiehmu bħala "noise." Id-Deep Residual Shrinkage Network jista' jkun kapaċi juża l-attention mechanism biex jinnota dan in-"noise" u mbagħad jimpjega s-soft thresholding biex jagħmel il-features li jikkorrispondu għal dan in-"noise" żero, u b'hekk potenzjalment itejjeb l-accuracy tal-image classification.

Fl-ispeech recognition, speċifikament f'ambjenti relattivament storbjużi bħal settings ta' konverżazzjoni f'tarf it-triq jew ġewwa workshop tal-fabbrika, id-Deep Residual Shrinkage Network jista' jtejjeb l-accuracy tal-ispeech recognition, jew għall-inqas, joffri metodoloġija kapaċi li ttejjeb l-ispeech recognition accuracy.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Dan id-dokument irċieva aktar minn 1,400 ċitazzjoni fuq Google Scholar.

Ibbażat fuq statistika mhux kompluta, id-Deep Residual Shrinkage Network (DRSN) ġie applikat direttament jew modifikat u applikat f'aktar minn 1,000 pubblikazzjoni/studju f'firxa wiesgħa ta' oqsma, inklużi l-inġinerija mekkanika, enerġija elettrika, vision, healthcare, speech, text, radar, u remote sensing.
