---
layout: post
title: "Deep Residual Shrinkage Network: Enn Metod Artificial Intelligence pou bann Data ki ena boukou Noise"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-12
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network li enn version amelyore Deep Residual Network. An gros, Deep Residual Shrinkage Network li met ansam Deep Residual Network, attention mechanisms, ek soft thresholding functions."
---

**Deep Residual Shrinkage Network li enn version amelyore Deep Residual Network. An gros, Deep Residual Shrinkage Network li met ansam Deep Residual Network, attention mechanisms, ek soft thresholding functions.**

**Nou kapav konpran kouma Deep Residual Shrinkage Network marse dan fason swivan. Premierman, network la servi attention mechanisms pou reper bann features ki pa inportan. Apre, network la servi soft thresholding functions pou met sa bann features ki pa inportan la a zero. Par kont, network la reper bann features inportan ek li gard sa bann features inportan la. Sa prosede la fer deep neural network la vinn pli for. Sa prosede la ed network la tir useful features depi bann signal ki ena noise.**

## 1. Research Motivation

**Premierman, noise li inevitab kan algorithm la pe klasifye bann samples. Egzamp sa bann noise la inklir Gaussian noise, pink noise, ek Laplacian noise.** Pli large, souvan bann **samples** ena informasyon ki pa lie avek task **classification** la kouran. Nou kapav interpret sa bann informasyon inpertinan la kouma **noise**. Sa **noise** la kapav fer **classification performance** la bese. (**Soft thresholding** li enn etape kle dan boukou **signal denoising algorithms**.)

Par egzamp, konsider enn konversasyon bor simin. **Audio** la kapav ena son korn loto ek son larou. Nou kapav fer **speech recognition** lor sa bann signal la. Bann son dan **background** pou afekte rezilta la inevitabman. Dan enn perspektiv **deep learning**, **deep neural network** la bizin eliminn bann **features** ki koresponn avek korn ek larou. Sa eliminasyon la anpes bann **features** la afekte rezilta **speech recognition** la.

**Deziemman, kantite noise la souvan varye ant bann samples. Sa variasyon la arive mem dan mem dataset.** (Sa variasyon la li resanble **attention mechanisms**. Pran enn **image dataset** kouma egzamp. Landrwa kot target object la ete kapav diferan dan sak zimaz. **Attention mechanisms** kapav fokaliz lor landrwa spesifik target object la dan sak zimaz.)

Par egzamp, konsider nou pe fer trenn enn **cat-and-dog classifier** avek sink zimaz ki marke "dog." Zimaz 1 kapav ena enn lisien ek enn lera. Zimaz 2 kapav ena enn lisien ek enn zwa. Zimaz 3 kapav ena enn lisien ek enn poul. Zimaz 4 kapav ena enn lisien ek enn bourik. Zimaz 5 kapav ena enn lisien ek enn kanar. Pandan **training**, bann obze ki pa lie ar task la pou deranz **classifier** la. Sa bann obze la inklir lera, zwa, poul, bourik, ek kanar. Sa linterferans la fer **classification accuracy** la bese. Si nou kapav reper sa bann obze inpertinan la. Lerla, nou kapav eliminn bann **features** ki koresponn ar sa bann obze la. Dan sa fason la, nou kapav amelyor **accuracy** pou **cat-and-dog classifier** la.

## 2. Soft Thresholding

**Soft thresholding li enn etape santral dan boukou signal denoising algorithms. Algorithm la eliminn bann features si zot absolute values pli ba ki enn sertin threshold. Algorithm la fer bann features la "shrink" (retresi) ver zero si zot absolute values pli ot ki threshold la.** Bann reserser kapav implement **soft thresholding** avek sa formul la:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Derive **soft thresholding output** par rapor a **input** li:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Formul lao la montre ki derive **soft thresholding** li swa 1 swa 0. Sa propriete la li parey kouma propriete **ReLU activation function**. Akoz sa, **soft thresholding** kapav reduir risk **gradient vanishing** ek **gradient exploding** dan bann **deep learning algorithms**.

**Dan soft thresholding function la, threshold la bizin respekte de kondisyon. Premierman, threshold la bizin enn nomb pozitif. Deziemman, threshold la pa kapav depas valer maximal input signal la. Sinon, output la pou vinn zero net.**

**An plis, li preferab ki threshold la respekte enn trwaziem kondisyon. Sak sample bizin ena so prop threshold independan, baze lor kantite noise ki ena dan sa sample la.**

Rezon la se ki kantite **noise** la souvan diferan ant bann **samples**. Par egzamp, Sample A kapav ena mwins **noise**, tandiki Sample B ena plis **noise** dan mem **dataset**. Dan sa ka la, Sample A bizin servi enn pli tipti **threshold** pandan **soft thresholding**. Sample B bizin servi enn pli gran **threshold**. Dan bann **deep neural networks**, sa bann **features** ek **thresholds** la perdi zot definisyon fizik eksplicit. Me, lozik debaz la res parey. Setadir, sak **sample** bizin ena enn **threshold** independan. Kantite **noise** spesifik la ki desid sa **threshold** la.

## 3. Attention Mechanism

Bann reserser kapav fasilman konpran **attention mechanisms** dan domenn **computer vision**. Sistem viziel bann zanimo kapav distingu bann target kan zot vit-vit scan tou laria la. Apre sa, sistem viziel la fokaliz latansyon lor target object la. Sa aksyon la permet sistem la tir plis detay. An mem tan, sistem la blok bann informasyon ki pa inportan. Pou detay spesifik, silvouple refer a bann literatir lor **attention mechanisms**.

**Squeeze-and-Excitation Network (SENet)** li reprezant enn metod **deep learning** ki inpe nouvo ek ki servi **attention mechanisms**. Dan diferan **samples**, diferan **feature channels** fer diferan kontribisyon pou task **classification** la. **SENet** servi enn ti **sub-network** pou gayn enn set **weights**. Apre, **SENet** miltipliye sa bann **weights** la avek bann **features** dan sak **channel**. Sa operasyon la azist grander bann **features** dan sak **channel**. Nou kapav trouv sa prosede la kouma aplik diferan nivo latansyon lor diferan **feature channels**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Dan sa apros la, sak **sample** posed enn set **weights** independan. Setadir, bann **weights** pou ninport ki de **samples** zot diferan. Dan **SENet**, semin spesifik pou gayn bann **weights** la se "**Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function**."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

**Deep Residual Shrinkage Network** la servi striktir **SENet sub-network** la. Network la servi sa striktir la pou fer **soft thresholding** anba enn **deep attention mechanism**. **Sub-network** la (ki indike dan bwat rouz) li aprann enn set **thresholds** ("Learn a set of thresholds"). Lerla, network la aplik **soft thresholding** lor sak **feature channel** en servan sa bann **thresholds** la.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Dan sa **sub-network** la, sistem la kalkil dabor **absolute values** tou bann **features** dan **input feature map** la. Apre, sistem la fer **global average pooling** ek **averaging** pou gayn enn **feature**, ki nou apel A. Dan lot semin la, sistem la avoy **feature map** la dan enn ti **fully connected network** apre **global average pooling**. Sa **fully connected network** la servi **Sigmoid function** kouma so dernie **layer**. Sa funksyon la normaliz **output** la ant 0 ek 1. Sa prosede la donn enn coefisyan, ki nou apel α. Nou kapav eksprim **final threshold** la kouma α × A. Donk, **threshold** la li produi de (2) nimero. Enn nimero li ant 0 ek 1. Lot nimero la li **average** bann **absolute values** dan **feature map** la. **Sa metod la garanti ki threshold la li pozitif. Sa metod la osi garanti ki threshold la pa tro gran.**

**An plis, diferan samples donn diferan thresholds. Akoz sa, nou kapav interpret sa metod la kouma enn attention mechanism spesyal. Mechanism la reper bann features ki pa lie ar task kouran. Mechanism la transform sa bann features la pou vinn bann valer pre kot zero via de (2) convolutional layers. Apre, mechanism la met sa bann features la a zero via soft thresholding. Ou swa, mechanism la reper bann features ki lie ar task kouran. Mechanism la transform sa bann features la pou vinn bann valer lwin ar zero via de (2) convolutional layers. Finalman, mechanism la prezerv sa bann features la.**

Finalman, nou "stack" (anpile) enn sertin kantite **basic modules** ("Stack many basic modules"). Nou osi inklir **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, ek **fully connected output layers**. Sa prosede la ranz **Deep Residual Shrinkage Network** la net.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

**Deep Residual Shrinkage Network** li enn metod zeneral pou **feature learning**. Rezon la se ki bann **samples** souvan ena **noise** dan boukou task **feature learning**. Bann **samples** ena osi informasyon ki pa pertinan. Sa **noise** ek informasyon inpertinan la kapav afekte performans **feature learning**. Par egzamp:

Konsider **image classification**. Enn zimaz kapav ena boukou lezot obze ladan an mem tan. Nou kapav konpran sa bann obze la kouma "**noise**." **Deep Residual Shrinkage Network** kapav servi **attention mechanism** la. Network la remark sa "**noise**" la. Apre, network la servi **soft thresholding** pou met bann **features** ki koresponn ar sa "**noise**" la a zero. Sa aksyon la kapav fer **image classification accuracy** la monte.

Konsider **speech recognition**. Spesifikman, konsider bann lanvironnman ki ena tapaz, kouma konversasyon bor simin ouswa dan enn **factory workshop**. **Deep Residual Shrinkage Network** kapav amelyor **speech recognition accuracy**. Ouswa o mwin, network la donn enn metodolozi. Sa metodolozi la li kapab amelyor **speech recognition accuracy**.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Sa papie la finn gayn plis ki 1400 sitasyon lor **Google Scholar**.

Baze lor statistik ki pa konple, bann reserser finn aplik **Deep Residual Shrinkage Network (DRSN)** dan plis ki 1000 piblikasyon/letid. Sa bann aplikasyon la kouver enn varyete domenn. Sa bann domenn la inklir **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, ek **remote sensing**.
