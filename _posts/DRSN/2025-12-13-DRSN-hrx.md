---
layout: post
title: "Deep Residual Shrinkage Network: En Artificial Intelligence Method fer Highly Noisy Data"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-13
tags: [Deep Learning, AI]
mathjax: true
description: ""
---

**Das *Deep Residual Shrinkage Network* is en verbesserte variant vom *Deep Residual Network*. Im grund genumm, das *Deep Residual Shrinkage Network* tuun das *Deep Residual Network*, *attention mechanisms*, un *soft thresholding functions* integriere.**

**Mer kenne das *working principle* vom *Deep Residual Shrinkage Network* so verstehn: Erscht, das *network* tuun *attention mechanisms* use fer die *unimportant features* zu identifiziere. Dann, das *network* tuun *soft thresholding functions* use fer diese *unimportant features* uf null setze. Andersrum, das *network* tuun *important features* identifiziere un tuun diese *important features* behalte. Das process tuun die ability vom *deep neural network* verbessere. Das helft dem *network*, *useful features* aus *signals* mit *noise* zu extrahiere.**

## 1. Research Motivation

**Erschtens, *noise* is unvermeidlich wenn de *algorithm* *samples* klassifiziere tuun. Beischpiele fer so *noise* sinn *Gaussian noise*, *pink noise*, un *Laplacian noise*.** Breiter gspron, *samples* hunn oft information was irrelevant is fer die *classification task*. Mer kenne diese irrelevant information als *noise* verstehn. Disser *noise* kann die *classification performance* reduziere. (*Soft thresholding* is en wichtig schritt in viele *signal denoising algorithms*.)

Zum Beischpiel, stellt euch vor en conversation an de stross. Das audio hot vielleicht *sounds* von *car horns* un *wheels*. Mer wolle vielleicht *speech recognition* mache uf diese *signals*. Die hintergrund *sounds* werre sicher das resultat beeinflusse. Von de *deep learning* perspektiv, das *deep neural network* sollt die *features* von de *horns* un *wheels* eliminiere. Das tuun verhindere, dass diese *features* die *speech recognition results* kaputt mache.

**Zweitens, die menge von *noise* is oft unnerschiedlich zwische de *samples*. Das variiert sogar im selbe *dataset*.** (Das is ähnlich wie bei *attention mechanisms*. Nemmt mol en *image dataset* als beischpiel. Die *location* vom *target object* kann unnerschiedlich sinn in verschiedene bilder. *Attention mechanisms* kenne fokussiere uf die spezifisch *location* vom *target object* in jedem bild.)

Zum Beischpiel, mer tuun en *cat-and-dog classifier* trainiere mit fimf bilder, wo als "dog" gelabelt sinn. Bild 1 hot vielleicht en hund un en maus. Bild 2 hot en hund un en gans. Bild 3 hot en hund un en huhn. Bild 4 hot en hund un en esel. Bild 5 hot en hund un en ent. Während em *training*, die irrelevante objekte wie mäus, gäns, hühner, esel un ente werre de *classifier* störee. Das resultiert in weniger *classification accuracy*. Wenn mer diese irrelevante objekte identifiziere kenne, dann kenne mer die *features* von dene objekte eliminiere. So kenne mer die *accuracy* vom *cat-and-dog classifier* verbessere.

## 2. Soft Thresholding

**Soft thresholding is en wichtig schritt in viele *signal denoising algorithms*. De *algorithm* tuun *features* eliminiere wenn de absolute wert von de *features* klener is als en bestimmt *threshold*. De *algorithm* tuun die *features* in richtung null *shrinke* (verklennere) wenn de absolute wert grösser is als de *threshold*.** Researcher kenne *soft thresholding* implementiere mit der formel hier:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

De *derivative* (ableitung) vom *soft thresholding output* in bezug uf de *input* is:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Die formel obe zeigt, dass de *derivative* von *soft thresholding* entweder 1 oder 0 is. Die property is gleich wie beim *ReLU activation function*. Also, *soft thresholding* kann das risiko von *gradient vanishing* un *gradient exploding* in *deep learning algorithms* reduziere.

**Im *soft thresholding function*, wenn mer de *threshold* einstellt, muss mer zwei conditioone achte. Erscht, de *threshold* muss en positiv nummer sinn. Zweit, de *threshold* darf net grösser sinn als de *maximum value* vom *input signal*. Sinscht werd de *output* komplett null sinn.**

**Dazu, de *threshold* sollt am beschte noch en dritti condition erfille. Jeder *sample* sollt sei eigne unabhängige *threshold* hunn, basiert uf dem *noise content* von dem *sample*.**

De grund is, dass de *noise content* oft variiert zwische de *samples*. Zum Beischpiel, *Sample A* hot vielleicht weniger *noise*, awer *Sample B* hot mehr *noise* im selbe *dataset*. In dem fall, *Sample A* sollt en kleneren *threshold* use beim *soft thresholding*. *Sample B* sollt en grösseren *threshold* use. In *deep neural networks*, diese *features* un *thresholds* verliere vielleicht ihri physikalische definition, awer die basis logic bleibt gleich. Das heesst, jeder *sample* braucht en independent *threshold*. De spezifische *noise content* bestimmt de *threshold*.

## 3. Attention Mechanism

Researcher kenne *attention mechanisms* im gebiet von *computer vision* leicht verstehn. Die visuelle systeme von tiere kenne *targets* unnerscheide indem sie schnell iwwer alles scanne. Dann, das visuelle system fokussiert die *attention* uf das *target object*. Das erlaubt dem system, mehr *details* zu extrahiere. Gleichzeitig, das system tuun irrelevante information unterdrücke. Fer details, guckt mol in die literatur iwwer *attention mechanisms*.

Das *Squeeze-and-Excitation Network* (SENet) is en relativ neie *deep learning method* wo *attention mechanisms* use tuun. Zwische verschiedene *samples*, verschiedene *feature channels* hunn en unnerschiedliche beitrag fer die *classification task*. SENet tuun en klenes *sub-network* use fer en set von *weights* zu krien. Dann, SENet tuun diese *weights* multipliziere mit de *features* von de respektive *channels*. Diese operation tuun die grösse von de *features* in jedem *channel* anpasse. Mer kenne das process so sehn: mer tuun **"Apply weighting to each feature channel"** (verschiedene levels von *attention* uf verschiedene *feature channels* mache).

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

In der method, jeder *sample* hot en independent set von *weights*. Das heesst, die *weights* fer irgendwelche zwei *samples* sinn unnerschiedlich. Im SENet, de spezifische weg fer die *weights* zu krien is "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding mit Deep Attention Mechanism

Das *Deep Residual Shrinkage Network* tuun die structure vom *SENet sub-network* use. Das *network* tuun diese structure use fer *soft thresholding* unter en *deep attention mechanism* zu implementiere. Das *sub-network* (im rote kaste) tuun **"Learn a set of thresholds"**. Dann, das *network* tuun *soft thresholding* uf jeden *feature channel* mache mit diese *thresholds*.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

In dem *sub-network*, das system rechnet erscht die absolute werte von alle *features* in de *input feature map*. Dann, macht das system *global average pooling* un average fer en *feature* zu krien, genannt *A*. In de anner *path*, das system tuun die *feature map* in en klenes *fully connected network* schicke noh dem *global average pooling*. Das *fully connected network* use die *Sigmoid function* als letschti *layer*. Die function tuun de *output* normalisiere zwische 0 un 1. Das process gibt en coefficient, genannt *α*. Mer kenne de finale *threshold* ausdrücke als *α × A*. Also, de *threshold* is das produkt von zwei nummere. Ein nummer is zwische 0 un 1. Die anner nummer is de average von de absolute werte von de *feature map*. **Die method garantiert dass de *threshold* positiv is. Die method garantiert aach dass de *threshold* net zu gross is.**

**Weiterhin, verschiedene *samples* produziere verschiedene *thresholds*. Also kenne mer die method als en spezial *attention mechanism* verstehn. De *mechanism* identifiziert *features* wo irrelevant sinn fer die *current task*. De *mechanism* transformiert diese *features* in werte noh bei null durch zwei *convolutional layers*. Dann, de *mechanism* setzt diese *features* uf null mit *soft thresholding*. Oder mer kann sage, de *mechanism* identifiziert *features* wo relevant sinn fer die *current task*. De *mechanism* transformiert diese *features* in werte weit weg von null durch zwei *convolutional layers*. Am end, de *mechanism* tuun diese *features* behalte.**

Zum schluss, mer tuun **"Stack many basic modules"**. Mer tuun aach *convolutional layers*, *batch normalization*, *activation functions*, *global average pooling*, un *fully connected output layers* defzu. Das process baut das komplette *Deep Residual Shrinkage Network*.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Das *Deep Residual Shrinkage Network* is en general method fer *feature learning*. De grund is, dass *samples* oft *noise* hunn in viele *feature learning tasks*. *Samples* hunn aach irrelevante information. Disser *noise* un irrelevante information kenne die *performance* von *feature learning* beeinflusse. Zum Beischpiel:

Denkt mol an *image classification*. En bild kann gleichzeitig viele annere objekte hunn. Mer kenne diese objekte als "noise" verstehn. Das *Deep Residual Shrinkage Network* kann vielleicht de *attention mechanism* use. Das *network* merkt dissen "noise". Dann, das *network* tuun *soft thresholding* use fer die *features* von dem "noise" uf null setze. Das kann vielleicht die *image classification accuracy* verbessere.

Denkt mol an *speech recognition*. Speziell in *noisy environments* wie bei conversations an de stross oder in en fabrik. Das *Deep Residual Shrinkage Network* kann vielleicht die *speech recognition accuracy* verbessere. Oder zumindescht, das *network* gibt uns en method. Die method is fähig fer *speech recognition accuracy* zu verbessere.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

Diese *paper* hot iwwer 1400 *citations* uf *Google Scholar* kriet.

Basiert uf nicht-komplette statistike, researcher hunn das *Deep Residual Shrinkage Network* (DRSN) schon in iwwer 1000 *publications* un studie use gemacht. Diese applications decke viele gebiete ab. Die gebiete sinn *mechanical engineering*, *electrical power*, *vision*, *healthcare*, *speech*, *text*, *radar*, un *remote sensing*.
