---
layout: post
title: "Deep Residual Shrinkage Network: Sabuah Metode Artificial Intelligence untuak Highly Noisy Data"
date: 2025-12-07
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network adolah varian nan alah diparancak dari Deep Residual Network. Pado dasarnyo, Deep Residual Shrinkage Network manggabuang-an Deep Residual Network, attention mechanisms, sarato soft thresholding functions.**

**Awak bisa mamahami prinsip karajo dari Deep Residual Shrinkage Network jo caro mode iko. Partamo, network ko mamanfaatkan attention mechanisms untuak manandoi fitur-fitur nan indak pantiang. Kudian, network mamakai soft thresholding functions untuak mambuek fitur-fitur nan indak pantiang tadi manjadi nol. Sabaliknyo, network manandoi fitur-fitur nan pantiang sarato mampatahankan fitur-fitur pantiang tu. Proses iko mampakuat kamampuan deep neural network. Proses iko mambantu network untuak maambiak fitur nan baguno dari sinyal nan manganduang noise.**

## 1. Motivasi Penelitian

**Partamo, noise memang indak bisa diilakkan wakatu algoritma mangelompokkan (classify) sampel. Contoh dari noise ko tamasuak Gaussian noise, pink noise, jo Laplacian noise.** Labiah luweh lai, sampel acok manganduang informasi nan indak relevan jo tugas klasifikasi kini. Awak bisa manafsirkan informasi nan indak relevan tu sabagai *noise*. *Noise* iko bisa manurunkan performa klasifikasi. (**Soft thresholding** adolah langkah kunci dalam banyak algoritma *signal denoising*.)

Contohnyo, cubo bayangkan urang ma-ota di tapi jalan. Audio tu mungkin manganduang suaro klakson oto jo suaro roda. Awak mungkin nio malakukan *speech recognition* pado sinyal-sinyal ko. Suaro latar balakang tu pasti ka mampangaruhi hasilnyo. Dari suduik pandang *deep learning*, *deep neural network* samestinyo mambuang fitur nan sasuai jo klakson sarato roda tadi. Pambuangan iko mancagah fitur-fitur tadi mangganggu hasil *speech recognition*.

**Kaduo, jumlah noise acok babeda-beda antaro satu sampel jo sampel lainnyo. Variasi iko tajadi bahkan di dalam dataset nan samo.** (Variasi iko punyo kemiripan jo *attention mechanisms*. Ambik contoh *dataset* gambar. Lokasi objek target mungkin babeda-beda di tiok gambar. *Attention mechanisms* bisa fokus pado lokasi spesifik dari objek target di masiang-masiang gambar.)

Misalnyo, pertimbangkan awak malatiah *classifier* kuciang-jo-anjiang pakai limo gambar nan balabel "anjiang". Gambar 1 mungkin ado anjiang jo tikuih. Gambar 2 mungkin ado anjiang jo angso. Gambar 3 mungkin ado anjiang jo ayam. Gambar 4 mungkin ado anjiang jo kaledai. Gambar 5 mungkin ado anjiang jo itiak. Wakatu *training*, objek nan indak relevan ka mangganggu *classifier*. Objek-objek iko tamasuak tikuih, angso, ayam, kaledai, jo itiak. Gangguan iko manyababkan turunnyo akurasi klasifikasi. Kalau awak bisa manandoi objek-objek nan indak relevan ko. Mako, awak bisa mambuang fitur nan sasuai jo objek-objek tu. Jo caro iko, awak bisa maningkek-an akurasi dari *classifier* kuciang-jo-anjiang tadi.

## 2. Soft Thresholding

**Soft thresholding adolah langkah inti dalam banyak algoritma signal denoising. Algoritma ko mambuang fitur kalau nilai absolut dari fitur tu labiah randah dari threshold tatantu. Algoritma ko manyusuikkan (shrunk) fitur manuju nol kalau nilai absolut dari fitur tu labiah tinggi dari threshold ko.** Para peneliti bisa manerapkan **Soft thresholding** manggunokan rumus barikuik:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Turunan dari *output* **Soft thresholding** tahadap *input*-nyo adolah:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Rumus di ateh manunjuakkan kalau turunan dari **Soft thresholding** itu nilainyo 1 atau 0. Sifaik iko identik jo sifaik fungsi aktivasi ReLU. Mako dari itu, **Soft thresholding** bisa mangurangi risiko *gradient vanishing* (gradien hilang) jo *gradient exploding* (gradien maladak) dalam algoritma *deep learning*.

**Dalam fungsi soft thresholding, pa-aturan threshold musti mamanuhi duo syarat. Partamo, threshold musti angka positif. Kaduo, threshold indak buliah malabiahi nilai maksimum dari sinyal input. Kalau indak, output-nyo ka jadi nol sadonyo.**

**Salain itu, threshold saeloknyo mamanuhi syarat katigo. Tiok sampel harus punyo threshold surang (independen) badasarkan kanduangan noise pado sampel tu.**

Alasannyo adolah kanduangan *noise* acok babeda di antaro sampel. Contohnyo, Sampel A mungkin manganduang *noise* nan labiah saketek samantaro Sampel B manganduang *noise* nan labiah banyak dalam *dataset* nan samo. Dalam kasus ko, Sampel A paralu manggunokan *threshold* nan labiah ketek wakatu **Soft thresholding**. Sampel B paralu manggunokan *threshold* nan labiah gadang. Fitur jo *thresholds* iko memang kahilangan definisi fisik nan jaleh di dalam *deep neural networks*. Tapi, logika dasar nan mandasarinyo tatap samo. Aratinyo, tiok sampel harus punyo *threshold* nan independen. Kanduangan *noise* nan spesifik tu nan manantukan *threshold* iko.

## 3. Attention Mechanism

Para peneliti bisa jo mudah mamahami *attention mechanisms* di bidang *computer vision*. Sistem visual binatang bisa mambedakan target jo caro memindai sadoalah area sacaro capek. Sasudah tu, sistem visual memfokuskan paratian (*attention*) ka objek target. Aksi iko mambuek sistem bisa maambiak labiah banyak detail. Di saat nan samo, sistem manakan informasi nan indak relevan. Untuak spesifiknyo, silakan ruajuk literatur tantang *attention mechanisms*.

Squeeze-and-Excitation Network (SENet) mawakili sabuah metode *deep learning* nan agak baru nan mamanfaatkan *attention mechanisms*. Di antaro sampel nan babeda, *feature channels* nan babeda mambari kontribusi nan beda pulo ka tugas klasifikasi. SENet manggunokan *sub-network* ketek untuak mandapek-an sakumpulan bobot (*weights*). Kudian, SENet mangalikan bobot-bobot iko jo fitur dari masiang-masiang *channel*. Operasi iko manyasuaikan gadangnyo fitur di tiok *channel*. Awak bisa mancaliak proses iko sabagai manerapkan level *attention* nan babeda-beda ka *feature channels* nan babeda.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Dalam pandakek-an iko, tiok sampel mampunyoi sakumpulan bobot nan independen. Jo kato lain, bobot untuak duo sampel manapun adolah babeda. Di SENet, jalan (*path*) spesifik untuak mandapek-an bobot adolah "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Deep Residual Shrinkage Network manggunokan struktur *sub-network* SENet. Network ko mamakai struktur tu untuak manerapkan **Soft thresholding** di bawah *deep attention mechanism*. *Sub-network* tu (nan ditandoi dalam kotak merah) **Learn a set of thresholds**. Kudian, network manerapkan **Soft thresholding** ka tiok *feature channel* manggunokan *thresholds* iko.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Di dalam *sub-network* iko, sistem partamo-tamo maituang nilai absolut dari sadonyo fitur di *input feature map*. Kudian, sistem malakukan *global average pooling* jo marato-rato-an untuak mandapek-an sabuah fitur, nan disabuik A. Di jalan (*path*) nan satu lai, sistem mamasuakkan *feature map* ka dalam *fully connected network* ketek sasudah *global average pooling*. *Fully connected network* iko manggunokan fungsi Sigmoid sabagai *layer* tarakhia. Fungsi iko manormalisasi *output* antaro 0 jo 1. Proses iko manghasilkan sabuah koefisien, nan disabuik α. Awak bisa manyatoan *threshold* akhia sabagai α × A. Mako dari itu, *threshold* adolah hasil kali dari duo angka. Satu angka antaro 0 jo 1. Angka nan satu lai adolah rato-rato dari nilai absolut *feature map*. **Metode iko manjamin kalau threshold tu positif. Metode iko manjamin pulo threshold tu indak gadang bana.**

**Salain itu, sampel nan babeda manghasilkan threshold nan babeda pulo. Akibaiknyo, awak bisa manafsirkan metode iko sabagai attention mechanism nan khusus. Mekanisme ko manandoi fitur nan indak relevan jo tugas kini. Mekanisme ko ma-ubah fitur-fitur tu manjadi nilai nan dakek ka nol lewat duo convolutional layers. Kudian, mekanisme ko mambuek fitur tu jadi nol pakai Soft thresholding. Atau bisa dikatoan, mekanisme ko manandoi fitur nan relevan jo tugas kini. Mekanisme ko ma-ubah fitur-fitur tu manjadi nilai nan jauah dari nol lewat duo convolutional layers. Akhirnyo, mekanisme ko mampatahankan fitur-fitur tu.**

Tarakhia, awak manumpuak (*stack*) sakian banyak modul dasar (**Stack many basic modules**). Awak mamasuakkan juo *convolutional layers*, *batch normalization*, *activation functions*, *global average pooling*, jo *fully connected output layers*. Proses iko mambangun *Deep Residual Shrinkage Network* nan langkok.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Kemampuan Generalisasi

Deep Residual Shrinkage Network adolah metode umum untuak *feature learning*. Alasannyo adolah karano sampel acok manganduang *noise* dalam banyak tugas *feature learning*. Sampel manganduang pulo informasi nan indak relevan. *Noise* jo informasi indak relevan iko mungkin mampangaruhi performa *feature learning*. Contohnyo:

Pertimbangkan *image classification*. Sabuah gambar mungkin sakaligus manganduang banyak objek lain. Awak bisa mamahami objek-objek iko sabagai "*noise*". Deep Residual Shrinkage Network mungkin bisa mamanfaatkan *attention mechanism*. Network manyadari "*noise*" iko. Kudian, network mamakai **Soft thresholding** untuak mambuek fitur nan sasuai jo "*noise*" iko manjadi nol. Aksi iko berpotensi maningkek-an akurasi *image classification*.

Pertimbangkan *speech recognition*. Khususnyo, pertimbangkan lingkuangan nan lumayan *noisy* cando tampek ma-ota di tapi jalan atau di dalam bengkel pabrik. Deep Residual Shrinkage Network mungkin maningkek-an akurasi *speech recognition*. Atau satidaknyo, network manawarkan sabuah metodologi. Metodologi iko mampu maningkek-an akurasi *speech recognition*.

## Referensi

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Dampak Akademis

Paper iko alah manarimo labiah dari 1.400 kutipan di Google Scholar.

Badasarkan statistik nan indak langkok, para peneliti alah manarapkan Deep Residual Shrinkage Network (DRSN) di labiah dari 1.000 publikasi/penelitian. Aplikasi-aplikasi iko mancakup bidang nan luweh. Bidang-bidang iko tamasuak teknik masin, tenaga listrik, visi (*vision*), kasehatan, suaro (*speech*), teks, radar, jo *remote sensing*.
