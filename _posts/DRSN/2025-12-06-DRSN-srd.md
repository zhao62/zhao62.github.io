---
layout: post
title: "Deep Residual Shrinkage Network: Unu mètodu de Artificial Intelligence pro "Data" cun meda "Noise""
date: 2025-12-06
tags: [Deep Learning, AI]
mathjax: true
---

**Sa Deep Residual Shrinkage Network est una variante megiorada de sa Deep Residual Network. Essentzialmente, est una integratzione de sa Deep Residual Network, attention mechanisms, e funtziones de soft thresholding.**

**In tzertu modu, su printzìpiu de funtzionamentu de sa Deep Residual Shrinkage Network si podet cumprèndere aici: impreat is attention mechanisms pro notare is features non importantes e impreat funtziones de soft thresholding pro ddas ponner a zero; a s'imbesse, identificat is features importantes e ddas mantenet. Custu protzessu megiorat sa capatzidade de sa deep neural network de extraire features ùtiles dae signals chi cuntenent noise.**

## 1. Motivatzione de sa Richerca

**Primo, cando classificamus is samples, sa presèntzia de noise — comente Gaussian noise, pink noise, e Laplacian noise — est inevitàbile.** In sensu prus largu, is **samples** s'ispissu cuntenent informatzione suta forma de datos chi non tenent relatzione cun su **classification task** currente, chi puru si podet interpretare comente **noise**. Custu **noise** podet influire negativamente in sa **performance** de classificatzione. (Su **Soft thresholding** est unu passu clave in medas **signal denoising algorithms**.)

Pro esempru, durante una conversatzione a costadu de s'istrada, s'àudio podet èssere misturadu cun sonos de trombas de màchina e rodas. Cando faghimus **speech recognition** in custos **signals**, is risultados inevitabilmente ant a èssere influentzados dae custos sonos de fundu. Dae una prospetiva de **deep learning**, is **features** chi currispondent a is trombas e a is rodas diant dèpere èssere eliminadas intro de sa **deep neural network** pro evitare chi influentzont is risultados de su **speech recognition**.

**Segundu, fintzas intro de su matessi dataset, sa cantidade de noise s'ispissu càmbiat dae sample a sample.** (Custu tenet similitùdines cun is **attention mechanisms**; pighende unu **image dataset** comente esempru, sa positzione de s'ogetu "target" podet èssere diferente intre is imàgines, e is **attention mechanisms** si podent cuntzentrare in sa positzione dislindada de s'ogetu "target" in cada imàgine.)

Pro esempru, cando addestramus unu **classifier** cane-e-gatu, cunsideramus chimbe imàgines etichetadas comente "cane". Sa prima imàgine diat pòdere cuntènnere unu cane e unu sòrighe, sa segunda unu cane e un'oca, sa tertza unu cane e unu pibiddu, sa quarta unu cane e unu burricu, e sa quinta unu cane e un'anade. Durante su **training**, su **classifier** at a atzapàre interferèntzia dae ogetos irrelevantes comente sòrighes, ocas, pibiddos, burricos e anades, causende una diminutzione in sa **classification accuracy**. Si podimus identificare custos ogetos irrelevantes — is sòrighes, ocas, pibiddos, burricos e anades — e eliminare is **features** chi ddis currispondent, est possìbile megiorare sa **accuracy** de su **classifier** cane-e-gatu.

## 2. Soft Thresholding

**Su Soft thresholding est unu passu tzentrale in medas signal denoising algorithms. Eliminat is features chi tenent valores assolutos prus bassos de unu tzertu threshold e "shrinks" (contraet) is features chi tenent valores assolutos prus artos de custu threshold cara a zero.** Si podet implementare impreende sa fòrmula sighente:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Sa derivada de s'output de su **soft thresholding** respetu a s'input est:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Comente mustradu in subra, sa derivada de su **soft thresholding** est o 1 o 0. Custa proprietade est idèntica a cussa de sa **ReLU activation function**. Duncas, su **soft thresholding** podet puru reduire su rísicu chi is **deep learning algorithms** atzapent **gradient vanishing** e **gradient exploding**.

**In sa soft thresholding function, s'impostatzione de su threshold depet satisfàghere duas cunditziones: primo, su threshold depet èssere unu nùmeru positivu; segundu, su threshold non podet superare su valore màssimu de su input signal, si nono s'output at a èssere totu zero.**

**In prus, est preferìbile chi su threshold satisfatzat una tertza cunditzione: cada sample depet tènnere su pròpiu threshold indipendente basadu in su cuntenutu suo de noise.**

Custu est ca su cuntenutu de **noise** s'ispissu càmbiat intre is **samples**. Pro esempru, est comune intro de su matessi **dataset** chi su Sample A cuntèngiat mancu **noise** mentres su Sample B nd'at de prus. In custu casu, cando si faghet **soft thresholding** in unu **denoising algorithm**, su Sample A diat dèpere impreare unu **threshold** prus piticu, mentres su Sample B diat dèpere impreare unu **threshold** prus mannu. Mancari custas **features** e **thresholds** perdent is definitziones fìsicas esplìcitas issoro in is **deep neural networks**, sa lògica de base abarrat sa matessi. In àteras paràulas, cada **sample** depet tènnere su **threshold** indipendente suo determinadu dae su cuntenutu specìficu suo de **noise**.

## 3. Attention Mechanism

Is **Attention mechanisms** sunt relativamente fàtziles de cumprèndere in su campu de sa **computer vision**. Is sistemas visivos de is animales podent distìnghere "targets" iscanerizende rapidamente s'àrea intrea, e a pustis ponende s'atentzione in s'ogetu "target" pro extraire prus detàllios e reprimire informatzione irrelevante. Pro is detàllios, pro praghere cunsultade sa literadura subra is **attention mechanisms**.

Sa **Squeeze-and-Excitation Network (SENet)** rapresentat unu mètodu de **deep learning** relativamente nou chi impreat **attention mechanisms**. Intre **samples** diferentes, sa contributzione de **feature channels** diferentes a su **classification task** s'ispissu càmbiat. **SENet** impreat una **sub-network** pitica pro otènnere unu set de **weights** (**Learn a set of weights**) e a pustis multiplicat custos **weights** pro is **features** de is **channels** respetivos pro agiustare sa mannària de is **features** in cada **channel**. Custu protzessu si podet bìdere comente s'aplicatzione de livellos variàbiles de "atentzione" a **feature channels** diferentes (**Apply weighting to each feature channel**).

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

In custu acostamentu, cada **sample** tenet su set indipendente suo de **weights**. In àteras paràulas, is **weights** pro duos **samples** arbitràrios sunt diferentes. In **SENet**, su caminu specìficu pro otènnere is **weights** est "**Global Pooling** → **Fully Connected Layer** → **ReLU Function** → **Fully Connected Layer** → **Sigmoid Function**."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Sa **Deep Residual Shrinkage Network** pigat ispiratzione dae s'istrutura **sub-network** de **SENet** mentovada in subra pro implementare su **soft thresholding** suta unu **deep attention mechanism**. Pormèdiu de sa **sub-network** (inditada in sa cassa ruja), si podet imparare unu set de thresholds (**Learn a set of thresholds**) pro aplicare su **soft thresholding** a cada **feature channel**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

In custa **sub-network**, si càrculant in antis is valores assolutos de totu is **features** in sa **input feature map**. A pustis, pormèdiu de **global average pooling** e faghende sa mèdia, si otenet una **feature**, denotada comente A. In s'àteru caminu, sa **feature map** a pustis de su **global average pooling** benit insertada in una **fully connected network** pitica. Custa **fully connected network** impreat sa **Sigmoid function** comente **layer** finale pro normalizare s'output intre 0 e 1, produinde unu coefitziente denotadu comente α. Su **threshold** finale si podet espressare comente α×A. Duncas, su **threshold** est su produtu de unu nùmeru intre 0 e 1 e sa mèdia de is valores assolutos de sa **feature map**. **Custu mètodu garantit chi su threshold no est solu positivu ma mancu tropu mannu.**

**In prus, samples diferentes produent thresholds diferentes. De cunsighèntzia, in una tzertu manera, custu si podet interpretare comente unu attention mechanism ispetzializadu: identificat features irrelevantes pro su task currente, ddas trasformat in valores a curtzu a zero pormèdiu de duos convolutional layers, e ddas ponet a zero impreende soft thresholding; o sinono, identificat features relevantes pro su task currente, ddas trasformat in valores a tesu dae zero pormèdiu de duos convolutional layers, e ddas mantenet.**

A sa fine, faghende su stacking de unu tzertu nùmeru de **basic modules** (**Stack many basic modules**) impare cun **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, e **fully connected output layers**, si fraigat sa **Deep Residual Shrinkage Network** completa. (N.B.: Unu **Identity path** est puru incluidu pro facilitare su flussu de su gradiente).

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Capatzidade de Generalizatzione

Sa **Deep Residual Shrinkage Network** est, de fatu, unu mètodu generale de **feature learning**. Custu est ca, in medas **feature learning tasks**, is **samples** cuntenent prus o mancu unu pagu de **noise** e puru informatzione irrelevante. Custu **noise** e informatzione irrelevante podent influire in sa **performance** de su **feature learning**. Pro esempru:

In sa **image classification**, si un'imàgine cuntenet in su matessi tempus medas àteros ogetos, custos ogetos si podent cumprèndere comente "**noise**". Sa **Deep Residual Shrinkage Network** diat pòdere èssere capatze de impreare s'**attention mechanism** pro notare custu "**noise**" e a pustis impreare su **soft thresholding** pro ponner is **features** chi currispondent a custu "**noise**" a zero, megiorende potentzialmente sa **image classification accuracy**.

In su **speech recognition**, specificamente in ambientes cun relativamenti meda **noise** comente cuntestos de conversatzione a costadu de s'istrada o intro de un'ofitzina de fàbrica, sa **Deep Residual Shrinkage Network** podet megiorare sa **speech recognition accuracy**, o a su mancu, ofèrrere una metodologia capatze de megiorare sa **speech recognition accuracy**.

## Bibliografia

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Impatu Acadèmicu

Custa "paper" (artìculu) at retzidu prus de 1,400 tzitatziones in Google Scholar.

Basadu in istatìsticas non cumpletas, sa **Deep Residual Shrinkage Network (DRSN)** est istada aplicada diretamente o modificada e aplicada in prus de 1,000 publicatziones/istùdios in una gama larga de campos, includende ingegneria mecànica, **electrical power** (potèntzia elètrica), **vision**, **healthcare** (santidade), **speech**, testu, radar, e **remote sensing**.
