---
layout: post
title: "Deep Residual Shrinkage Network: метод искусственного интеллекта для сильно зашумленных данных"
date: 2025-11-27
tags: [Deep Learning, AI, Глубокое обучение, Искусственный интеллект]
mathjax: true
---

**Deep Residual Shrinkage Network (DRSN) — это улучшенная версия глубоких остаточных сетей (ResNet). По сути, эта архитектура представляет собой интеграцию ResNet, механизмов внимания (attention mechanisms) и функций мягкого порога (soft thresholding).**

**В определенной степени принцип работы Deep Residual Shrinkage Network можно описать так: с помощью механизма внимания сеть выявляет незначимые признаки и обнуляет их через функцию мягкого порога; в то же время значимые признаки сохраняются. Такой подход усиливает способность глубокой нейронной сети извлекать полезные признаки из зашумленных сигналов.**

## 1. Мотивация исследования
**Во-первых, при классификации образцов данные неизбежно содержат шум, такой как гауссов шум, розовый шум, шум Лапласа и другие.** В более широком смысле, образцы часто содержат информацию, не относящуюся к текущей задаче классификации, которую также можно интерпретировать как шум. Этот шум может негативно сказаться на результатах классификации. (Стоит отметить, что мягкая пороговая обработка является ключевым этапом во многих алгоритмах шумоподавления).

Рассмотрим пример: во время разговора на обочине дороги к речи могут примешиваться звуки автомобильных клаксонов, шум колес и т.д. При выполнении задачи распознавания речи эти посторонние звуки неизбежно повлияют на результат. С точки зрения глубокого обучения (Deep Learning), признаки, соответствующие звукам клаксонов и колес, должны быть удалены внутри нейронной сети, чтобы избежать их влияния на распознавание речи.

**Во-вторых, даже в пределах одного набора данных (датасета) уровень шума в разных образцах часто варьируется.** (Это перекликается с принципом работы механизмов внимания. Если взять в качестве примера набор изображений, то целевой объект может находиться в разных частях кадра на разных снимках; механизм внимания позволяет сфокусироваться именно на локации целевого объекта для каждого конкретного изображения).

Например, при обучении классификатора «кошки против собак» возьмем 5 изображений с меткой «собака». На первом изображении могут быть собака и мышь, на втором — собака и гусь, на третьем — собака и курица, на четвертом — собака и осел, а на пятом — собака и утка. Обучая классификатор, мы неизбежно сталкиваемся с помехами от посторонних объектов (мышей, гусей, кур, ослов и уток), что снижает точность классификации. Если бы мы могли автоматически выявлять эти посторонние объекты и удалять соответствующие им признаки, это позволило бы повысить точность классификатора.

## 2. Мягкая пороговая обработка (Soft Thresholding)
**Мягкая пороговая обработка — это центральный этап многих алгоритмов шумоподавления. Она удаляет признаки, абсолютное значение которых ниже определенного порога, и «стягивает» к нулю признаки, абсолютное значение которых выше этого порога.** Математически это выражается следующей формулой:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Производная функции мягкого порога по входу составляет:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Как видно из формул, производная мягкого порога равна либо 1, либо 0. Это свойство идентично свойству функции активации ReLU. Следовательно, использование мягкого порога также позволяет снизить риск возникновения проблем затухания (vanishing gradient) и взрыва градиента (exploding gradient) в алгоритмах глубокого обучения.

**При настройке функции мягкого порога должны соблюдаться два условия: во-первых, порог должен быть положительным числом; во-вторых, порог не должен превышать максимальное значение входного сигнала, иначе все выходные значения будут равны нулю.**

**Кроме того, желательно соблюдение третьего условия: каждый образец должен иметь свой собственный независимый порог, зависящий от уровня шума в этом конкретном образце.**

Это объясняется тем, что зашумленность образцов часто неодинакова. Например, в одном датасете образец А может содержать мало шума, а образец Б — много. Соответственно, при шумоподавлении для образца А следует использовать меньший порог, а для образца Б — больший. Хотя в глубоких нейронных сетях признаки и пороги теряют явный физический смысл, базовая логика остается прежней: каждому образцу необходим индивидуальный порог, определяемый содержанием шума в нем.

## 3. Механизм внимания (Attention Mechanism)
Механизмы внимания сравнительно легко понять на примере компьютерного зрения. Зрительная система животных способна быстро сканировать все поле зрения, обнаруживать целевой объект и фокусировать на нем внимание для извлечения деталей, одновременно подавляя нерелевантную информацию. Подробнее см. литературу по механизмам внимания.

Squeeze-and-Excitation Network (SENet) — это один из современных методов глубокого обучения, использующий механизм внимания. В разных образцах вклад различных каналов признаков (feature channels) в задачу классификации часто неодинаков. SENet использует небольшую подсеть для получения набора весов, которые затем перемножаются с признаками соответствующих каналов. Это позволяет регулировать значимость признаков в каждом канале. Данный процесс можно рассматривать как наложение «внимания» разной силы на разные каналы признаков.

<p align="center">
  <img src="/assets/img/2025-11-27-DRSN-ru/SENET_ru_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

При таком подходе каждый образец получает свой независимый набор весов. Другими словами, для любых двух произвольных образцов веса будут различаться. В SENet путь получения весов выглядит следующим образом: «Глобальный пулинг (Global Pooling) → Полносвязный слой → Функция ReLU → Полносвязный слой → Функция Sigmoid».

<p align="center">
  <img src="/assets/img/2025-11-27-DRSN-ru/SENET_ru_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Мягкая пороговая обработка на основе глубокого механизма внимания
Архитектура **Deep Residual Shrinkage Network** заимствует структуру подсети у упомянутого выше SENet для реализации мягкой пороговой обработки, управляемой механизмом внимания. С помощью подсети (выделена красной рамкой на схеме алгоритма) сеть обучается генерировать набор порогов для применения мягкого порога к каждому каналу признаков.

<p align="center">
  <img src="/assets/img/2025-11-27-DRSN-ru/DRSN_ru_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

В этой подсети сначала вычисляются абсолютные значения всех признаков во входной карте признаков (feature map). Затем, после глобального усредняющего пулинга (Global Average Pooling), получается усредненный признак, обозначим его как A. Далее этот признак подается в небольшую полносвязную сеть. Последним слоем этой полносвязной сети является функция Sigmoid, которая нормализует выходное значение в диапазоне от 0 до 1, давая коэффициент, обозначим его как α. Итоговый порог можно выразить как α × A. Таким образом, порог представляет собой произведение числа от 0 до 1 и среднего абсолютного значения карты признаков. **Такой метод гарантирует, что порог не только будет положительным, но и не окажется слишком большим.**

**Более того, для разных образцов генерируются разные пороги. В определенной степени это можно интерпретировать как специальный механизм внимания: сеть замечает признаки, не относящиеся к текущей задаче, преобразует их в значения, близкие к нулю (через сверточные слои), и окончательно обнуляет их с помощью функции мягкого порога. И наоборот: признаки, важные для задачи, преобразуются в значения, далекие от нуля, и сохраняются.**

Наконец, объединив определенное количество таких базовых модулей со сверточными слоями, пакетной нормализацией (Batch Normalization), функциями активации, глобальным усредняющим пулингом и выходным полносвязным слоем, мы получаем полную архитектуру **Deep Residual Shrinkage Network**.

<p align="center">
  <img src="/assets/img/2025-11-24-DRSN-zh/DRSN_zh_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Универсальность метода
Deep Residual Shrinkage Network фактически является универсальным методом обучения признаков (feature learning). Это связано с тем, что во многих задачах обучения образцы в той или иной степени содержат шум и нерелевантную информацию, которые могут ухудшить качество обучения. Например:

В задачах классификации изображений, если картинка содержит множество посторонних объектов, их можно рассматривать как «шум». DRSN, используя механизм внимания, может заметить этот «шум», а затем с помощью мягкой пороговой обработки обнулить соответствующие ему признаки, что потенциально повысит точность классификации.

В задачах распознавания речи, особенно в зашумленной обстановке (например, разговор у дороги или в заводском цеху), DRSN может повысить точность распознавания или, как минимум, предлагает перспективный подход для решения этой проблемы.


## Ссылки

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2942898}
}
```

## Академическое влияние

Количество цитирований данной статьи в Google Академии (Google Scholar) превысило 1400.

По неполным статистическим данным, **Deep Residual Shrinkage Network** была применена (напрямую или в модифицированном виде) в более чем 1000 научных работах в самых разных областях, включая машиностроение, электроэнергетику, компьютерное зрение, медицину, обработку речи и текста, радиолокацию, дистанционное зондирование и другие.
