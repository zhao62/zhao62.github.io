---
layout: post
title: "<div style='text-align: center; direction: rtl;'>Deep Residual Shrinkage Network: د Highly Noisy Data لپاره یوه Artificial Intelligence طریقه</div>"
subtitle: "<div style='text-align: center; direction: ltr;'>Deep Residual Shrinkage Network: An Artificial Intelligence Method for Highly Noisy Data</div>"
date: 2025-12-04
tags: [Deep Learning, AI]
mathjax: true
lang: ps
dir: rtl
---

<!-- CSS Styling -->
<style>
  /* 1. Force Headers Right */
  h1, h2, h3, h4, h5, h6 { text-align: right !important; direction: rtl !important; }
  
  /* 2. Force Lists Right */
  ul, ol { text-align: right !important; direction: rtl !important; padding-right: 40px; padding-left: 0; }

  /* 3. Force Code LTR */
  pre, code { text-align: left !important; direction: ltr !important; }
  
  /* 4. Force References LTR */
  .references-ltr { text-align: left !important; direction: ltr !important; }
</style>

<!-- Main Content Wrapper -->
<div dir="rtl" markdown="1" style="text-align: right; direction: rtl; font-family: 'Sakkal Majalla', 'Traditional Arabic', serif; font-size: 1.1em;">

<strong>Deep Residual Shrinkage Network د Deep Residual Network یوه اصلاح شوې بڼه ده. په حقیقت کې، دا د Deep Residual Network، attention mechanisms او soft thresholding functions یو integration (یوځای والی) دی.</strong>

<strong>تر یوې اندازې پورې، د Deep Residual Shrinkage Network کاري اصول په لاندې ډول درک کیدی شي: دا د attention mechanisms څخه استفاده کوي ترڅو غیر مهم features وپېژني او د soft thresholding functions په واسطه هغوی صفر (zero) ته set کړي؛ برعکس، دا مهم features پېژني او هغه ساتي. دا پروسه د deep neural network وړتیا لوړوي ترڅو د هغو signals څخه ګټور features راوباسي کوم چې noise لري.</strong>

## 1. د څېړنې انګېزه (Research Motivation)

<strong>لومړی، کله چې موږ samples طبقه بندي (classify) کوو، د noise شتون — لکه Gaussian noise، pink noise او Laplacian noise — حتمي دی.</strong> په پراخ مفهوم کې، samples اکثرا داسې معلومات لري چې د اوسني classification task سره تړاو نلري، چې دا هم د noise په توګه ګڼل کیدی شي. دا noise ممکن د classification په فعالیت باندې منفي اغیزه وکړي. (Soft thresholding په ډیرو signal denoising algorithms کې یو کلیدي ګام دی.)

د مثال په توګه، د سړک په غاړه د خبرو اترو پرمهال، غږ ممکن د موټر د هارنونو او ټایرونو د غږونو سره ګډ شي. کله چې په دې signals باندې speech recognition ترسره کیږي، پایلې به حتماً د دې شالید غږونو (background sounds) لخوا اغیزمنې شي. د deep learning له نظره، هغه features چې د هارنونو او ټایرونو سره مطابقت لري باید په deep neural network کې له مینځه یوړل شي ترڅو د speech recognition په پایلو باندې د دوی د اغیزې مخه ونیول شي.

<strong>دویم، حتی په یوه dataset کې، د noise اندازه اکثرا د یوې sample څخه تر بلې پورې توپیر لري.</strong> (دا د attention mechanisms سره ورته والی لري؛ د image dataset د مثال په توګه، په عکسونو کې د هدف شوي څیز موقعیت ممکن توپیر ولري، او attention mechanisms کولی شي په هر عکس کې د هدف شوي څیز په ځانګړي موقعیت تمرکز وکړي.)

د مثال په توګه، کله چې یو cat-and-dog classifier روزل کیږي (training)، پنځه عکسونه په پام کې ونیسئ چې د "dog" په توګه لیبل شوي دي. لومړی عکس ممکن یو سپی او یو موږک ولري، دویم یو سپی او یو قاز، دریم یو سپی او یوه چرګه، څلورم یو سپی او یو خر، او پنځم یو سپی او یو هیلۍ ولري. د training په جریان کې، classifier به حتماً د غیر اړونده څیزونو لکه موږک، قاز، چرګې، خر او هیلۍ لخوا د interference سره مخ شي، چې پایله یې د classification accuracy کمښت دی. که موږ وکولی شو دا غیر اړونده څیزونه وپېژنو او د هغوی اړوند features له مینځه یوسو، نو دا ممکنه ده چې د cat-and-dog classifier دقت (accuracy) ښه کړو.

## 2. Soft Thresholding

<strong>Soft thresholding د ډیرو signal denoising algorithms یو اصلي ګام دی. دا هغه features له مینځه وړي چې absolute values یې د یو ټاکلي threshold څخه کم وي، او هغه features چې absolute values یې د دې threshold څخه لوړ وي، د صفر په لور shrink (راکم) کوي.</strong> دا د لاندې فارمول په کارولو سره پلي کیدی شي:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

د input په نسبت د soft thresholding د output مشتق (derivative) په لاندې ډول دی:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

لکه څنګه چې پورته ښودل شوي، د soft thresholding مشتق یا 1 دی یا 0. دا ځانګړتیا د ReLU activation function سره یو شان ده. له همدې امله، soft thresholding کولی شي د deep learning algorithms سره د gradient vanishing او gradient exploding ستونزو خطر هم کم کړي.

<strong>په soft thresholding function کې، د threshold تنظیم کول باید دوه شرایط پوره کړي: لومړی، threshold باید یو مثبت عدد (positive number) وي؛ دویم، threshold نشي کولی د input signal د اعظمي ارزښت څخه ډیر وي، که نه نو output به په بشپړ ډول صفر وي.</strong>

<strong>سربیره پردې، دا غوره ده چې threshold دریم شرط هم پوره کړي: هره sample باید د خپل noise مینځپانګې پراساس خپل خپلواک threshold ولري.</strong>

دلیل یې دا دی چې د noise اندازه اکثرا د samples ترمنځ توپیر لري. د مثال په توګه، په ورته dataset کې دا معمول ده چې Sample A لږ noise ولري پداسې حال کې چې Sample B ډیر noise ولري. پدې حالت کې، کله چې په denoising algorithm کې soft thresholding ترسره کیږي، Sample A باید کوچنی threshold وکاروي، پداسې حال کې چې Sample B باید لوی threshold وکاروي. که څه هم دا features او thresholds په deep neural networks کې خپل روښانه فزیکي تعریفونه له لاسه ورکوي، مګر اصلي منطق یو شان پاتې کیږي. په بل عبارت، هره sample باید خپل خپلواک threshold ولري چې د هغې د ځانګړي noise مینځپانګې لخوا ټاکل کیږي.

## 3. Attention Mechanism

Attention mechanisms د computer vision په برخه کې پوهیدل نسبتا اسانه دي. د څارویو لید سیسټمونه کولی شي د ټولې سیمې په ګړندۍ سکین کولو سره اهداف وپېژني، او وروسته په هدف شوي څیز باندې attention متمرکز کړي ترڅو نور توضیحات راوباسي پداسې حال کې چې غیر اړونده معلومات فشاروي (suppress). د توضیحاتو لپاره، مهرباني وکړئ د attention mechanisms اړوند ادبیاتو ته مراجعه وکړئ.

Squeeze-and-Excitation Network (SENet) یو نسبتا نوی deep learning میتود دی چې د attention mechanisms څخه کار اخلي. په مختلفو samples کې، د classification task لپاره د مختلفو feature channels ونډه (contribution) اکثرا توپیر لري. SENet یو کوچنی sub-network کاروي ترڅو د وزنونو یوه ټولګه زده کړي (<strong>Learn a set of weights</strong>)، او بیا دا weights د اړوندو channels د features سره ضرب کوي ترڅو په هر channel کې د features کچه تنظیم کړي (<strong>Apply weighting to each feature channel</strong>). دا پروسه په مختلفو feature channels باندې د <strong>Weighting</strong> پلي کولو په توګه لیدل کیدی شي.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

پدې طریقه کې، هره sample د weights خپله خپلواکه ټولګه لري. په بل عبارت، د هرې دوه خپلسري samples لپاره weights توپیر لري. په SENet کې، د وزنونو د ترلاسه کولو ځانګړې لاره "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function" ده.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Deep Residual Shrinkage Network د پورته یاد شوي SENet sub-network جوړښت څخه الهام اخلي ترڅو د deep attention mechanism لاندې soft thresholding پلي کړي. د sub-network له لارې (چې په سور بکس کې ښودل شوی)، موږ کولی شو د thresholds یوه ټولګه زده کړو (<strong>Learn a set of thresholds</strong>) ترڅو په هر feature channel باندې <strong>Soft thresholding</strong> پلي کړو.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

په دې sub-network کې، لومړی د input feature map د ټولو features مطلق ارزښتونه (absolute values) محاسبه کیږي. بیا، د global average pooling او اوسط (averaging) له لارې، یو feature ترلاسه کیږي، چې د A په توګه ښودل کیږي. په بله لاره کې، د global average pooling وروسته feature map یو کوچني fully connected network ته داخلیږي. دا fully connected network د Sigmoid function د خپل وروستي پرت (layer) په توګه کاروي ترڅو output د 0 او 1 ترمنځ normalize کړي، او یو ضریب (coefficient) تولیدوي چې د α په توګه ښودل کیږي. وروستی threshold د α × A په توګه څرګند کیدی شي. له همدې امله، threshold د 0 او 1 ترمنځ د یوې شمیرې او د feature map د مطلق ارزښتونو د اوسط حاصل ضرب دی. <strong>دا طریقه ډاډ ورکوي چې threshold نه یوازې مثبت دی بلکه ډیر لوی هم نه دی.</strong>

<strong>سربیره پردې، مختلف samples د مختلف thresholds پایله لري. په پایله کې، تر یوې اندازې پورې، دا د یو ځانګړي attention mechanism په توګه تشریح کیدی شي: دا هغه features پېژني چې د اوسني task سره تړاو نلري، د دوه convolutional layers له لارې یې صفر ته نږدې ارزښتونو ته بدلوي، او د soft thresholding په کارولو سره یې صفر ته set کوي؛ یا برعکس، دا هغه features پېژني چې د اوسني task سره تړاو لري، د دوه convolutional layers له لارې یې داسې ارزښتونو ته بدلوي چې له صفر څخه لرې وي، او د Identity path له لارې یې ساتي.</strong>

په نهایت کې، د یو ټاکلي شمیر basic modules د <strong>Stack many basic modules</strong> په توګه یوځای کولو سره، د convolutional layers، batch normalization، activation functions، global average pooling، او fully connected output layers سره یوځای، بشپړ Deep Residual Shrinkage Network جوړیږي.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. عمومي وړتیا (Generalization Capability)

Deep Residual Shrinkage Network په حقیقت کې د feature learning یو عمومي میتود دی. دلیل یې دا دی چې په ډیرو feature learning tasks کې، samples لږ یا ډیر noise او همدارنګه غیر اړونده معلومات لري. دا noise او غیر اړونده معلومات ممکن د feature learning په فعالیت اغیزه وکړي. د مثال په توګه:

په image classification کې، که یو عکس په ورته وخت کې ډیر نور شیان ولري، دا شیان د "noise" په توګه پوهیدل کیدی شي. Deep Residual Shrinkage Network ممکن د دې وړتیا ولري چې د attention mechanism څخه کار واخلي ترڅو دا "noise" وګوري او بیا د soft thresholding څخه کار واخلي ترڅو د دې "noise" اړوند features صفر ته set کړي، چې پدې توګه ممکن د image classification دقت (accuracy) ښه کړي.

په speech recognition کې، په ځانګړي توګه په نسبتا شور لرونکي چاپیریال کې لکه د سړک په غاړه یا د فابریکې ورکشاپ کې د خبرو اترو پرمهال، Deep Residual Shrinkage Network ممکن د speech recognition دقت ښه کړي، یا لږترلږه، یوه داسې میتودولوژي وړاندې کړي چې د speech recognition د دقت د ښه کولو وړتیا ولري.

## اکاډمیک اغیز (Academic Impact)

دې مقالې په Google Scholar کې له 1,400 څخه ډیر citations ترلاسه کړي دي.

د نیمګړو احصایو پراساس، Deep Residual Shrinkage Network (DRSN) په مستقیم ډول پلي شوی یا اصلاح شوی او په میخانیکي انجینرۍ (mechanical engineering)، بریښنا (electrical power)، بصري (vision)، روغتیا پاملرنې (healthcare)، وینا (speech)، متن (text)، رادار (radar)، او ریموټ سینسنګ (remote sensing) په شمول په پراخه برخو کې په 1,000 څخه زیاتو خپرونو/مطالعاتو کې کارول شوی دی.

<!-- 使用 HTML h2 标签替代 Markdown 的 ## -->
<h2>اخځلیکونه (References)</h2>

<!-- 强制左对齐的容器 -->
<div class="references-ltr" dir="ltr" style="text-align: left; direction: ltr; margin-bottom: 20px;">
  <p>
    Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.
  </p>
  <p>
    <!-- 使用 HTML a 标签替代 Markdown 链接 -->
    <a href="https://ieeexplore.ieee.org/document/8850096">https://ieeexplore.ieee.org/document/8850096</a>
  </p>
</div>

<h2>BibTeX</h2>

<!-- 使用 HTML pre/code 标签替代 Markdown 代码块，并强制 LTR -->
<div dir="ltr" style="text-align: left; direction: ltr;">
<pre style="background-color: #f6f8fa; padding: 16px; border-radius: 6px; overflow: auto;"><code>@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}</code></pre>
</div>
