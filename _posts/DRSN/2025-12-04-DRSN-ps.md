---
layout: post
title: "<div style='text-align: center; direction: rtl;'>Deep Residual Shrinkage Network: Highly Noisy Data لاءِ هڪ Artificial Intelligence جو طريقو</div>"
subtitle: "<div style='text-align: center; direction: ltr;'>The integration of Deep Residual Network, Attention Mechanism, and Soft Thresholding.</div>"
date: 2025-12-04
tags: [Deep Learning, AI]
mathjax: true
lang: sd
dir: rtl
---

<!-- CSS Styling -->
<style>
  /* 1. Force Headers Right */
  h1, h2, h3, h4, h5, h6 { 
  text-align: right !important; 
  direction: ltr !important; 
  unicode-bidi: embed; /* 确保浏览器正确处理混合文本 */
}
  
  /* 2. Force Lists Right */
  ul, ol { text-align: right !important; direction: rtl !important; padding-right: 40px; padding-left: 0; }

  /* 3. Force Code LTR */
  pre, code { text-align: left !important; direction: ltr !important; }
  
  /* 4. Force References LTR */
  .references-ltr { text-align: left !important; direction: ltr !important; }
</style>

<!-- Main Content Wrapper -->
<div dir="rtl" markdown="1" style="text-align: right; direction: rtl; font-family: 'Sakkal Majalla', 'Traditional Arabic', serif; font-size: 1.1em;">

<strong>Deep Residual Shrinkage Network اصل ۾ Deep Residual Network جو هڪ improved variant (بيترين قسم) آهي. بنيادي طور تي، هي Deep Residual Network، Attention mechanisms، ۽ soft thresholding functions جو هڪ ميلاپ (integration) آهي.</strong>

<strong>ڪجهه حد تائين، Deep Residual Shrinkage Network جي ڪم ڪرڻ جي اصول (working principle) کي هن طرح سمجهي سگهجي ٿو: هي Attention mechanisms استعمال ڪري unimportant features (غير اهم خصوصيتن) کي سڃاڻي ٿو ۽ soft thresholding functions ذريعي انهن کي zero ڪري ڇڏي ٿو؛ ۽ ان جي ابتڙ، هي important features (اهم خصوصيتن) کي سڃاڻي انهن کي برقرار رکي ٿو. اهو عمل deep neural network جي صلاحيت کي وڌائي ٿو ته جيئن اهو noise واري signals مان مفيد features حاصل ڪري سگهي.</strong>

## 1. Research Motivation (تحقيق جو مقصد)

<strong>پهرين، جڏهن samples جي classification ڪئي ويندي آهي، ته noise جي موجودگي—جهڙوڪ Gaussian noise، Pink noise، ۽ Laplacian noise—اڻ ٽر (inevitable) آهي.</strong> وڌيڪ واضع طور تي، samples ۾ اڪثر اهڙي information هوندي آهي جيڪا موجوده classification task سان لاڳاپيل ناهي هوندي، جنهن کي پڻ noise سمجهي سگهجي ٿو. هي noise ممڪن طور تي classification performance تي ناڪاري اثر وجهي سگهي ٿو. (soft thresholding ڪيترن ئي signal denoising algorithms ۾ هڪ اهم قدم آهي.)

مثال طور، روڊ جي ڀرسان ڳالهه ٻولهه دوران، آڊيو ۾ گاڏين جي Horns ۽ Wheels جا آواز شامل ٿي سگهن ٿا. جڏهن انهن signals تي speech recognition ڪيو ويندو، ته نتيجا لازمي طور تي انهن background sounds کان متاثر ٿيندا. Deep learning جي نقطه نظر کان، Horns ۽ Wheels سان لاڳاپيل features کي deep neural network جي اندر ختم ڪرڻ گهرجي ته جيئن اهي speech recognition جي نتيجن تي اثر انداز نه ٿين.

<strong>ٻيو، ساڳئي dataset جي اندر به، اڪثر samples ۾ noise جو مقدار مختلف هوندو آهي.</strong> (هي attention mechanisms سان ملندڙ جلندڙ آهي؛ هڪ image dataset جو مثال وٺو، مختلف تصويرن ۾ target object جي جڳهه مختلف ٿي سگهي ٿي؛ attention mechanisms هر تصوير ۾ target object جي مخصوص جڳهه تي focus ڪري سگهن ٿا.)

مثال طور، جڏهن هڪ cat-and-dog classifier کي train ڪيو وڃي ٿو، فرض ڪريو 5 تصويرون آهن جن جو label "dog" آهي. پهرين تصوير ۾ ڪتو ۽ ڪوئو (mouse) ٿي سگهي ٿو، ٻي تصوير ۾ ڪتو ۽ راج हंस (goose)، ٽي تصوير ۾ ڪتو ۽ ڪڪڙ (chicken)، چوٿين ۾ ڪتو ۽ گڏهه (donkey)، ۽ پنجين ۾ ڪتو ۽ بدڪ (duck) شامل ٿي سگهن ٿا. Training دوران، classifier لازمي طور تي غير لاڳاپيل شيون جهڙوڪ mice، geese، chickens، donkeys، ۽ ducks کان متاثر ٿيندو، جنهن جي نتيجي ۾ classification accuracy گهٽ ٿي ويندي. جيڪڏهن اسان انهن غير لاڳاپيل شين—mice، geese، chickens، donkeys، ۽ ducks—کي سڃاڻي سگهون ۽ انهن جا corresponding features ختم ڪري ڇڏيون، ته پوءِ cat-and-dog classifier جي accuracy کي بهتر ڪرڻ ممڪن آهي.

## 2. Soft Thresholding

<strong>soft thresholding ڪيترن ئي signal denoising algorithms ۾ هڪ اهم قدم آهي، جيئن مٿي ڏيکاريو ويو آهي، ۽ اها هيٺ ڏنل فارمولي ذريعي لاڳو ڪري سگهجي ٿو:</strong>

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

soft thresholding output جو input جي حوالي سان derivative هي آهي:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

جيئن مٿي ڏيکاريو ويو آهي، soft thresholding جو derivative يا ته 1 آهي يا 0. اها خاصيت بلڪل ReLU activation function جهڙي آهي. تنهن ڪري، soft thresholding پڻ deep learning algorithms کي gradient vanishing ۽ gradient exploding جي خطرن کان بچائڻ ۾ مدد ڪري سگهي ٿو.

<strong>soft thresholding function ۾، threshold جي setting لاءِ ٻن شرطن جو پورو ٿيڻ ضروري آهي: پهرين، threshold هڪ positive نمبر هجڻ گهرجي؛ ٻيو، threshold کي input signal جي maximum value کان وڌيڪ نه هجڻ گهرجي، ورنہ output مڪمل طور تي zero ٿي ويندي.</strong>

<strong>ان سان گڏ، threshold بهتر آهي ته ٽئين شرط کي به پورو ڪري: هر sample جو پنهنجو الڳ independent threshold هجڻ گهرجي جيڪو ان جي noise content جي بنياد تي هجي.</strong>

ان جو سبب اهو آهي ته noise content اڪثر samples جي وچ ۾ مختلف هوندو آهي. مثال طور، ساڳئي dataset ۾ عام طور تي Sample A ۾ گهٽ noise ٿي سگهي ٿو جڏهن ته Sample B ۾ وڌيڪ noise آهي. اهڙي صورت ۾، جيڪڏهن denoising algorithm ۾ soft thresholding ڪئي وڃي، ته Sample A لاءِ ننڍو threshold استعمال ٿيڻ گهرجي، جڏهن ته Sample B لاءِ وڏو threshold استعمال ٿيڻ گهرجي. جيتوڻيڪ deep neural networks ۾ اهي features ۽ thresholds پنهنجي واضح طبعي تعريف (physical definitions) وڃائي ويهندا آهن، پر بنيادي منطق (logic) ساڳيو رهي ٿو. ٻين لفظن ۾، هر sample جو پنهنجو independent threshold هجڻ گهرجي جيڪو ان جي مخصوص noise content طرفان مقرر ڪيل هجي.

## 3. Attention Mechanism

Attention mechanisms کي Computer Vision جي field ۾ سمجهڻ نسبتاً آسان آهي. جانورن جو بصري نظام (visual systems) سڄي علائقي کي تيزيءَ سان scan ڪري targets کي سڃاڻي سگهي ٿو، ۽ پوءِ target object تي attention focus ڪري وڌيڪ تفصيل (details) حاصل ڪري ٿو جڏهن ته غير لاڳاپيل information کي نظرانداز ڪري ٿو. تفصيل لاءِ، مهرباني ڪري attention mechanisms بابت موجود مواد (literature) ڏسو.

Squeeze-and-Excitation Network (SENet) هڪ نسبتاً نئون deep learning method آهي جيڪو attention mechanisms استعمال ڪري ٿو. مختلف samples ۾، مختلف feature channels جو classification task ۾ حصو (contribution) اڪثر مختلف هوندو آهي. SENet هڪ ننڍو sub-network استعمال ڪري **Learn a set of weights** (weights جو هڪ سيٽ سکي) حاصل ڪري ٿو ۽ پوءِ انهن weights کي لاڳاپيل channels جي features سان multiply ڪري **Apply weighting to each feature channel** ڪري ٿو، يعني هر channel ۾ features جي magnitude کي adjust ڪري ٿو. هن عمل کي مختلف feature channels تي مختلف سطح جي attention لاڳو ڪرڻ طور ڏسي سگهجي ٿو.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

هن طريقي ۾، هر sample وٽ weights جو پنهنجو independent set هوندو آهي. ٻين لفظن ۾، ڪنهن به ٻن samples جا weights مختلف هوندا آهن. SENet ۾، weights حاصل ڪرڻ جو مخصوص رستو هي آهي: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

Deep Residual Shrinkage Network مٿي بيان ڪيل SENet sub-network ساخت مان متاثر ٿي deep attention mechanism جي تحت soft thresholding کي لاڳو ڪري ٿو. ڳاڙهي باڪس يا red box جي اندر ڏنل sub-network جي ذريعي، thresholds جو هڪ سيٽ سکي سگهجي ٿو (**Learn a set of thresholds**) ته جيئن هر feature channel تي soft thresholding لاڳو ڪري سگهجي.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

هن sub-network ۾، سڀ کان پهرين input feature map جي تمام features جي absolute values جو حساب ڪيو ويندو آهي. پوءِ، global average pooling ۽ averaging ذريعي، هڪ feature حاصل ڪيو ويندو آهي، جنهن کي A چيو ويندو آهي. ٻئي رستي (path) ۾، global average pooling کان پوءِ واري feature map کي هڪ ننڍي fully connected network ۾ input ڪيو ويندو آهي. هي fully connected network پنهنجي آخري layer طور Sigmoid function استعمال ڪري output کي 0 ۽ 1 جي وچ ۾ normalize ڪري ٿو، جنهن سان هڪ coefficient حاصل ٿئي ٿو جنهن کي α چيو ويندو آهي. آخري threshold کي α × A طور ظاهر ڪري سگهجي ٿو. تنهن ڪري، threshold اصل ۾ 0 ۽ 1 جي وچ ۾ هڪ نمبر ۽ feature map جي absolute values جي average جو حاصل ضرب (product) آهي. <strong>اهو طريقو يقيني بڻائي ٿو ته threshold نه رڳو positive آهي پر گهڻو وڏو (excessively large) به ناهي.</strong>

<strong>وڌيڪ اهو ته، مختلف samples جا thresholds مختلف هوندا آهن. تنهنڪري، ڪجهه حد تائين، ان کي هڪ specialized attention mechanism سمجهي سگهجي ٿو: اهو موجوده task سان غير لاڳاپيل features کي سڃاڻي ٿو، انهن کي ٻن convolutional layers ذريعي zero جي ويجهو values ۾ تبديل ڪري ٿو، ۽ soft thresholding استعمال ڪندي انهن کي zero ڪري ڇڏي ٿو؛ يا وري، اهو موجوده task سان لاڳاپيل features کي سڃاڻي ٿو، انهن کي ٻن convolutional layers ذريعي zero کان پري values ۾ تبديل ڪري ٿو، ۽ انهن کي محفوظ (preserve) ڪري ٿو.</strong>

آخر ۾، ڪجهه بنيادي ماڊلز (**Stack many basic modules**) کي convolutional layers، batch normalization، activation functions، global average pooling، ۽ fully connected output layers سان گڏ stack ڪرڻ سان، مڪمل Deep Residual Shrinkage Network تيار ٿي وڃي ٿو. ان ۾ <strong>Identity path</strong> ۽ <strong>Soft thresholding</strong> جا رستا شامل آهن، جن جي آخري ۾ **Weighting** ۽ مجموعي (summation) جا عمل آهن.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Deep Residual Shrinkage Network اصل ۾ هڪ general feature learning method آهي. ان جو سبب اهو آهي ته، اڪثر feature learning tasks ۾، samples ۾ گهٽ يا وڌ noise ۽ غير لاڳاپيل information شامل هوندي آهي. هي noise ۽ غير لاڳاپيل information شايد feature learning جي ڪارڪردگي تي اثر انداز ٿي سگهي ٿي. مثال طور:

Image classification ۾، جيڪڏهن هڪ تصوير ۾ ٻيون گهڻيون شيون به موجود آهن، ته انهن شين کي "noise" سمجهي سگهجي ٿو. Deep Residual Shrinkage Network شايد attention mechanism استعمال ڪري هن "noise" کي notice ڪري سگهي ٿو ۽ پوءِ soft thresholding استعمال ڪندي هن "noise" سان لاڳاپيل features کي zero ڪري سگهي ٿو، جنهن سان image classification accuracy بهتر ٿيڻ جو امڪان آهي.

Speech recognition ۾، خاص طور تي شور واري ماحول ۾، جيئن روڊ جي ڀرسان يا ڪنهن فئڪٽري ورڪشاپ جي اندر ڳالهه ٻولهه دوران، Deep Residual Shrinkage Network شايد speech recognition accuracy کي بهتر ڪري سگهي ٿو، يا گهٽ ۾ گهٽ، اهو هڪ اهڙو طريقو (methodology) فراهم ڪري ٿو جيڪو speech recognition accuracy کي بهتر ڪرڻ جي صلاحيت رکي ٿو.

## Academic Impact (تعليمي اثر)

هن paper کي Google Scholar تي 1400 کان وڌيڪ citations مليون آهن.

نامڪمل انگن اکرن موجب، Deep Residual Shrinkage Network (DRSN) کي سڌو سنئون يا تبديل ڪري 1000 کان وڌيڪ publications/studies ۾ استعمال ڪيو ويو آهي، جن ۾ mechanical engineering، electrical power، vision، healthcare، speech، text، radar، ۽ remote sensing جا شعبا شامل آهن.

## مراجع (References)

<div class="references-ltr" dir="ltr" style="text-align: left; direction: ltr;">

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

<a href="https://ieeexplore.ieee.org/document/8850096">https://ieeexplore.ieee.org/document/8850096</a>

</div>

## BibTeX

```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```
