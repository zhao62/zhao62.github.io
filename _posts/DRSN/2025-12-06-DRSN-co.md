---
layout: post
title: "Deep Residual Shrinkage Network: Un Metudu d'Intelligenza Artificiale per Dati Assai Rumorosi (Highly Noisy Data)"
date: 2025-12-06
tags: [Deep Learning, AI]
mathjax: true
description: "U Deep Residual Shrinkage Network (DRSN) hè una variante migliurata di u Deep Residual Network. Essenzialmente, hè una integrazione di u Deep Residual Network, di i "attention mechanisms", è di e funzioni di soft thresholding."
---

**U Deep Residual Shrinkage Network (DRSN) hè una variante migliurata di u Deep Residual Network. Essenzialmente, hè una integrazione di u Deep Residual Network, di i "attention mechanisms", è di e funzioni di "soft thresholding".**

**In una certa misura, u principiu di funziunamentu di u Deep Residual Shrinkage Network si pò capisce cusì: utilizza i "attention mechanisms" per identificà e features (caratteristiche) micca impurtanti è adopra e funzioni di "soft thresholding" per mette le à zeru; à u cuntrariu, identifica e features impurtanti è e cunserva. Stu prucessu migliura a capacità di a deep neural network di estrae features utili da i signali chì cuntenenu rumore (noise).**

## 1. Motivazione di a Ricerca

**Prima, quandu si classificanu i campioni (samples), a presenza di rumore—cum'è Gaussian noise, pink noise, è Laplacian noise—hè inevitabile.** In un sensu più largu, i campioni cuntenenu spessu infurmazioni chì ùn sò micca pertinenti per u compitu di classificazione attuale, è questu pò ancu esse interpretatu cum'è rumore. Stu rumore pò influenzà negativamente a performance di a classificazione. ("Soft thresholding" hè un passu chjave in parechji algoritmi di signal denoising.)

Per esempiu, durante una cunversazione accantu à a strada, l'audio pò esse mischju cù i soni di e trombe di vitture è di e rote. Quandu si face a **speech recognition** (ricunniscenza vucale) annantu à sti signali, i risultati seranu inevitabilmente affettati da sti soni di fondu. Da una perspettiva di **deep learning**, e features chì currispondenu à e trombe è à e rote devenu esse eliminate in a deep neural network per impedisce ch'elli affettinu i risultati di a speech recognition.

**Siconda, ancu in u listessu dataset, a quantità di rumore varieghja spessu da un campione à l'altru.** (Questu hà sumiglianze cù i **attention mechanisms**; pigliendu un dataset d'imaghjini cum'è esempiu, a pusizione di l'ughjettu di destinazione pò esse sfarente in diverse imaghjini, è i attention mechanisms ponu fucalizà nantu à a pusizione specifica di l'ughjettu in ogni imagine.)

Per esempiu, quandu si entrena un classificatore cane-ghjattu, cunsiderate cinque imaghjini etichettate cum'è "cane". A prima maghjina puderia cuntene un cane è un topu, a siconda un cane è una oca, a terza un cane è un pollu, a quarta un cane è un sumere, è a quinta un cane è una anatra. Durante u training, u classificatore serà inevitabilmente suggettu à l'interferenza di l'ughjetti irrilevanti cum'è i topi, l'oche, i polli, i sumeri è l'anatre, resultendu in una diminuzione di l'accuratezza di a classificazione. S'è no pudemu identificà sti ughjetti irrilevanti—i topi, l'oche, i polli, i sumeri è l'anatre—è eliminà e so features currispundenti, hè pussibule di migliurà l'accuratezza di u classificatore cane-ghjattu.

## 2. Soft Thresholding

**U "Soft thresholding" hè un passu core in parechji algoritmi di signal denoising. Elimineghja e features chì i so valori assoluti sò più bassi chè un certu threshold (soglia), è riduce e features chì i so valori assoluti sò più alti chè stu threshold versu u zeru.** Pò esse implementatu cù a formula seguente:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

A derivata di l'output di u soft thresholding rispettu à l'input hè:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Cum'è mostratu sopra, a derivata di u soft thresholding hè o 1 o 0. Sta pruprietà hè identica à quella di a funzione d'attivazione ReLU. Dunque, u soft thresholding pò ancu riduce u risicu chì l'algoritmi di deep learning scontrinu prublemi di **gradient vanishing** è **gradient exploding**.

**In a funzione di soft thresholding, l'impostazione di u threshold deve suddisfà duie cundizioni: prima, u threshold deve esse un numeru pusitivu; siconda, u threshold ùn pò micca superà u valore massimu di u signale di input, altrimenti l'output serà tuttu zeru.**

**In più, hè preferibile chì u threshold soddisfi una terza cundizione: ogni campione deve avè u so propiu threshold indipendente basatu annantu à u so cuntenutu di rumore.**

Questu hè perchè u cuntenutu di rumore varieghja spessu trà i campioni. Per esempiu, hè cumunu in u listessu dataset chì u Campione A cunteni menu rumore mentre u Campione B cunteni più rumore. In stu casu, quandu si face u soft thresholding in un algoritmu di denoising, u Campione A duveria utilizà un threshold più chjucu, mentre u Campione B duveria utilizà un threshold più grande. Benchì ste features è thresholds perdonu e so definizioni fisiche esplicite in e deep neural networks, a logica di basa ferma a stessa. In altre parolle, ogni campione deve avè u so propiu threshold indipendente determinatu da u so cuntenutu specificu di rumore.

## 3. Attention Mechanism

I **Attention mechanisms** sò relativamente faciuli à capisce in u campu di a **computer vision**. I sistemi visuali di l'animali ponu distingue i bersagli scannendu rapidamente tutta l'area, è dopu fucalizendu l'attenzione nantu à l'ughjettu di destinazione per estrae più dettagli mentre supprimenu l'infurmazioni irrilevanti. Per i dettagli, per piacè riferitevi à a literatura riguardanti i attention mechanisms.

U **Squeeze-and-Excitation Network (SENet)** raprisenta un metudu di deep learning relativamente novu chì utilizza attention mechanisms. À traversu diversi campioni, u cuntributu di i diversi **feature channels** à u compitu di classificazione varieghja spessu. SENet impiega una piccula sub-network per uttene un set di pesi (weights) è dopu multiplica sti pesi per e features di i canali rispettivi per aghjustà a magnitudine di e features in ogni canale. Stu prucessu pò esse vistu cum'è l'applicazione di vari livelli d'attenzione à diversi feature channels.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

In questu approcciu, ogni campione pussede u so propiu set di pesi indipendente ("**Learn a set of weights**"). In altre parolle, i pesi per duie campioni arbitrarii sò sfarenti. In SENet, u percorsu specificu per uttene i pesi hè "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding cun Deep Attention Mechanism

U **Deep Residual Shrinkage Network** piglia ispirazione da a struttura di a sub-network SENet sopra menzionata per implementà u soft thresholding sottu un **deep attention mechanism**. Attraversu a sub-network (indicata in a casella rossa in i diagrammi), un set di thresholds pò esse amparatu ("**Learn a set of thresholds**") per applicà u soft thresholding à ogni feature channel.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

In sta sub-network, i valori assoluti di tutte e features in a input feature map sò prima calculati. Dopu, attraversu u **global average pooling** è facendu a media, si ottene una feature, denotata cum'è A. In l'altru percorsu ("**Identity path**"), a feature map dopu à u global average pooling hè inserita in una piccula fully connected network. Sta fully connected network usa a funzione Sigmoid cum'è u so ultimu stratu per normalizà l'output trà 0 è 1, dendu un coefficiente denotatu cum'è α. U threshold finale pò esse espressu cum'è α×A. Dunque, u threshold hè u pruduttu di un numeru trà 0 è 1 è a media di i valori assoluti di a feature map. **Stu metudu assicura chì u threshold ùn sia micca solu pusitivu ma ancu micca troppu grande.**

**Inoltre, diversi campioni risultanu in diversi thresholds. Di cunsiguenza, in una certa misura, questu pò esse interpretatu cum'è un attention mechanism specializatu: identifica e features irrilevanti per u compitu attuale, e trasforma in valori vicini à zeru via dui strati cunvuluziunali (convolutional layers), è e mette à zeru usendu "**Soft thresholding**"; o sinnò, identifica e features pertinenti per u compitu attuale, e trasforma in valori luntani da zeru via dui convolutional layers, è e cunserva.**

Infine, impilendu ("**Stack many basic modules**") un certu numeru di moduli di basa inseme cù convolutional layers, **batch normalization**, activation functions, global average pooling, è fully connected output layers, si custruisce u **Deep Residual Shrinkage Network** cumpletu. Avemu dinù bisognu di applicà u pesu à ogni canale ("**Apply weighting to each feature channel**") o realizà una "**Weighting**" curretta per assicurà u funziunamentu.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Capacità di Generalizazione (Generalization Capability)

U **Deep Residual Shrinkage Network** hè, in fattu, un metudu generale di **feature learning**. Questu hè perchè, in parechji compiti di feature learning, i campioni cuntenenu più o menu un pocu di rumore è ancu infurmazioni irrilevanti. Stu rumore è ste infurmazioni irrilevanti ponu affettà a performance di u feature learning. Per esempiu:

In a classificazione d'imaghjini (image classification), s'è un'imagine cuntene simultaneamente parechji altri ughjetti, sti ughjetti ponu esse capiti cum'è "rumore". U Deep Residual Shrinkage Network puderia esse capace di utilizà l'attention mechanism per nutà stu "rumore" è dopu impiegà u soft thresholding per mette e features chì currispondenu à stu "rumore" à zeru, migliurendu cusì putenzialmente l'accuratezza di a classificazione d'imaghjini.

In a **speech recognition**, specificamente in ambienti relativamente rumorosi cum'è cunfigurazioni di cunversazione accantu à una strada o indrentu à un attellu di fabbrica, u Deep Residual Shrinkage Network pò migliurà l'accuratezza di a ricunniscenza vucale, o à u minimu, offre una metodulugia capace di migliurà l'accuratezza.

## Referenze

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Impattu Accademicu

Stu articulu hà ricevutu più di 1400 citazioni nantu à Google Scholar.

Basatu nantu à statistiche incomplete, u **Deep Residual Shrinkage Network (DRSN)** hè statu direttamente applicatu o mudificatu è applicatu in più di 1000 publicazioni/studii in una larga gamma di campi, inclusi l'ingegneria meccanica, l'energia elettrica, a visione (vision), a salute, a parolla (speech), u testu, u radar, è u remote sensing.
