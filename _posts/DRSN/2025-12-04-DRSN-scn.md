---
layout: post
title: "Deep Residual Shrinkage Network: Un Mètudu di Artificial Intelligence pi Dati assai Rumorusi (Highly Noisy Data)"
date: 2025-12-04
tags: [Deep Learning, AI]
mathjax: true
---

**A Deep Residual Shrinkage Network è na varianti migghiurata dâ Deep Residual Network. Nta sostanza, è na ntegrazzioni di Deep Residual Network, attention mechanisms, e funzioni di Soft thresholding.**

**Nta un certu sensu, u principiu di funzionamentu dâ Deep Residual Shrinkage Network si pò capiri accussì: usa i attention mechanisms pi identificari i features ca nun sunnu mpurtanti e usa i funzioni di Soft thresholding pi impustàrili a zeru; ô cuntrariu, identifica i features mpurtanti e i manteni. Stu prucessu migghiura a capacità dâ deep neural network di estràiri features ùtili di signali ca cuntenunu noise.**

## 1. Motivazzioni dâ Ricerca

**Prima di tuttu, quannu classificamu i campiuni, a prisenza di noise — comu Gaussian noise, pink noise, e Laplacian noise — è inevitàbili.** Cchiù in ginirali, i campiuni spissu cuntenunu nfurmazzioni ca nun c'entrano nenti cu u task di classificazzioni currenti, e chistu si pò vidiri macari comu noise. Stu noise pò aviri effetti negativi supra a performance dâ classificazzioni. (U **Soft thresholding** è un passu chiavi nta assai algoritmi di signal denoising.)

Pi esempiu, duranti na cunversazzioni ô latu dâ strata, l'audio pò essiri ammiscatu cu i soni di clacson e roti. Quannu facemu speech recognition supra sti signali, i risurtati sunnu pi forza nfruenzati di sti soni di suttafunnu. Dâ pruspittiva dû deep learning, i features ca currispunnunu ê clacson e ê roti avìssiru a essiri eliminati dintra a deep neural network pi evitari ca nfruenzanu i risurtati dû speech recognition.

**Secunnu, macari dintra u stessu dataset, a quantità di noise spissu cancia di campiuni a campiuni.** (Chistu havi punti in cumuni cu i attention mechanisms; pigghiannu comu esempiu un image dataset, a pusizzioni di l'uggettu target pò essiri diversa tra i vari mmàggini, e i attention mechanisms ponnu fucalizzàrisi supra a pusizzioni spicìfica di l'uggettu target in ogni mmàggini.)

Pi esempiu, quannu addestramu un cat-and-dog classifier, cunziddiramu cincu mmàggini etichettati comu "cani". A prima mmàggini putissi aviri un cani e un surci, a secunna un cani e un oca, a terza un cani e un puddru, a quarta un cani e un sceccu, e a quinta un cani e na papira. Duranti u training, u classifier sarà inevitabbilmenti suggettu a nterferenzi di uggetti irrelevanti comu surci, ochi, puddri, scecchi e papiri, purtannu a na diminuzzioni dâ pricisioni dâ classificazzioni. Si putìssimu identificari sti uggetti irrelevanti — i surci, l'ochi, i puddri, i scecchi e i papiri — e eliminari i features currispunnenti, fussi pussìbili migghiurari l'accuratezza dû cat-and-dog classifier.

## 2. Soft Thresholding

**U Soft thresholding è un passu funnamintali nta assai algoritmi di signal denoising. Elimina i features ca hannu valuri assoluti cchiù vasci di na certa threshold e "restringi" (shrinks) i features ca hannu valuri assoluti cchiù àuti versu u zeru.** Si pò implementari usannu a siguenti fòrmula:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

A dirivata dû output dû soft thresholding rispettu all'input è:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Comu si vidi supra, a dirivata dû soft thresholding è 1 o 0. Sta pruprietà è identica a chidda dâ ReLU activation function. Dunca, u soft thresholding pò macari ridùciri u rìsicu ca i deep learning algorithms scuntranu u gradient vanishing e u gradient exploding.

**Nta soft thresholding function, l'impostazzioni dâ threshold havi a rispittari dui cundizzioni: prima, a threshold havi a essiri un nùmmuru pusitivu; secunna, a threshold nun pò essiri cchiù granni dû valuri màssimu dû signali di input, sinnò l'output sarà tuttu zeru.**

**Inoltre, è prifirìbbili ca a threshold rispetta na terza cundizzioni: ogni campiuni avissi aviri a so threshold indipendenti basata supra u so cuntinutu di noise.**

Chistu pirchì u cuntinutu di noise spissu cancia tra i campiuni. Pi esempiu, è cumuni dintra u stessu dataset ca u Campiuni A cunteni menu noise mentri u Campiuni B cunteni cchiù noise. Nta stu casu, quannu si fa soft thresholding nta un denoising algorithm, u Campiuni A avissi a usari na threshold cchiù nica, mentri u Campiuni B avissi a usari na threshold cchiù granni. Macari si sti features e thresholds pèrdinu i so definizzioni fìsichi espliciti nê deep neural networks, a lògica di basi arresta a stessa. In autri paroli, ogni campiuni avissi aviri a so threshold indipendenti ditirminata dû so spicìficu cuntinutu di noise.

## 3. Attention Mechanism

I Attention mechanisms sunnu rilativamenti fàcili di capiri ntô campu dâ computer vision. I sistemi visivi di l'armali ponnu distìnguiri i target scannannu rapidamenti tutta l'aria, e appoi fucalizzannu l'attinzioni supra l'uggettu target pi estràiri cchiù dettagli mentri supprimunu i nfurmazzioni irrelevanti. Pi i dettagli, pi favuri faciti rifirimentu â littiratura supra i attention mechanisms.

A Squeeze-and-Excitation Network (SENet) rapprisenta un mètudu di deep learning rilativamenti novu ca usa attention mechanisms. Tra diversi campiuni, u cuntribbutu di diversi feature channels ô classification task spissu cancia. SENet usa na sub-network nica pi ottèniri un set di **weights** ("Learn a set of weights") e appoi murtìplica sti **weights** pi i features dî canali rispittivi pi agghiustari a grannizza dî features in ogni canali. Stu prucessu si pò vìdiri comu l'applicazzioni di vari livelli di attinzioni a diversi **feature channels** ("Apply weighting to each feature channel").

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Nta stu approcciu, ogni campiuni pussedi u so set indipendenti di **weights**. In autri paroli, i **weights** pi du' campiuni arbitrari sunnu diversi. Nta SENet, u percursu spicìficu pi ottèniri i **weights** è "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding cu Deep Attention Mechanism

A Deep Residual Shrinkage Network pigghia ispirazzioni dâ struttura dâ sub-network SENet sopra muntuvata pi implementari u soft thresholding sutta un deep attention mechanism. Tramiti a sub-network (indicata dintra u riquadru russu), si pò imparari un set di **thresholds** ("Learn a set of thresholds") pi applicari u **Soft thresholding** a ogni **feature channel**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Nta sta sub-network, prima si càlculanu i valuri assoluti di tutti i features ntâ input feature map. Appoi, tramiti global average pooling e facennu a media, si otteni un feature, denotatu comu A. Nta l'autru percursu, a feature map doppu u global average pooling veni data in input a na nica fully connected network. Sta fully connected network usa a Sigmoid function comu strata finali pi normalizzari l'output tra 0 e 1, duna un coefficienti denotatu comu α. A threshold finali si pò esprìmiri comu α×A. Dunca, a threshold è u pruduttu di un nùmmuru tra 0 e 1 e a media dî valuri assoluti dâ feature map. **Stu mètudu assicura ca a threshold nun è sulu pusitiva ma macari ca nun è eccessivamenti granni.**

**Inoltre, campiuni diversi pòrtanu a thresholds diversi. Di cunsiguenza, nta un certu sensu, chistu si pò ntèrpritari comu un attention mechanism spicializzatu: nota i features irrelevanti pi u task currenti, i trasforma in valuri vicini a zeru tramiti du' convolutional layers, e i imposta a zeru usannu u soft thresholding; in alternativa, nota i features rilevanti pi u task currenti, i trasforma in valuri luntani di zeru tramiti du' convolutional layers, e i priserva.**

Finalmenti, mittennu unu supra l'autru (*Stacking*) un certu nùmmuru di basic modules ("Stack many basic modules") nsemmula a convolutional layers, **Batch Normalization**, activation functions, global average pooling, e fully connected output layers, si costruisci a **Deep Residual Shrinkage Network** cumpleta. Nota ca ntô schema c'è macari un **Identity path** e l'operazzioni di **Weighting**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Capacità di Generalizzazzioni

A Deep Residual Shrinkage Network è, di fattu, un mètudu ginirali di feature learning. Chistu pirchì, nta assai feature learning tasks, i campiuni cchiù o menu cuntenunu qualchi noise e macari nfurmazzioni irrelevanti. Stu noise e i nfurmazzioni irrelevanti ponnu nfruenzari a performance dû feature learning. Pi esempiu:

Nta image classification, si na mmàggini cunteni cuntimpuraniamenti assai autri uggetti, sti uggetti ponnu essiri caputi comu "noise". A Deep Residual Shrinkage Network putissi essiri in gradu di usari l'attention mechanism pi notari stu "noise" e appoi usari u soft thresholding pi impustari i features currispunnenti a stu "noise" a zeru, putennu accussì migghiurari l'accuratezza dâ image classification.

Nta speech recognition, spicificamenti in ambienti rilativamenti rumorusi comu cuntesti di cunversazzioni ô latu dâ strata o dintra n'officina di fàbbrica, a Deep Residual Shrinkage Network putissi migghiurari l'accuratezza dû speech recognition, o armenu offriri na metoduluggìa capaci di migghiurari l'accuratezza dû speech recognition.

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Impattu Accadèmicu

Stu paper havi ricivutu cchiù di 1400 citazzioni supra Google Scholar.

Basatu supra statìstiche nun completi, a Deep Residual Shrinkage Network (DRSN) è stata applicata direttamenti o modificata e applicata in cchiù di 1000 pubblicazzioni/studi nta na vasta gamma di campi, inclusi ncignirìa miccànica, putenza elettrica, vision, sanità, speech, testu, radar, e remote sensing.
