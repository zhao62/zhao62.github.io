---
layout: post
title: "Profunda Resta Ŝrump-Reto (Deep Residual Shrinkage Network): Metodo de Artefarita Inteligenteco por Tre Bruaj Datenoj"
date: 2025-12-07
tags: [Deep Learning, AI]
mathjax: true
description: ""
---

**La *Deep Residual Shrinkage Network* estas plibonigita varianto de la fama *Deep Residual Network*. Esence, la *Deep Residual Shrinkage Network* integras tri gravajn komponentojn: la *Deep Residual Network*, *attention mechanisms*, kaj funkciojn de *soft thresholding*.**

**Ni povas kompreni la funkcian principon de la *Deep Residual Shrinkage Network* en la sekva maniero. Unue, la reto uzas *attention mechanisms* por identigi la malgravajn *features* (trajtojn). Poste, la reto uzas funkciojn de *soft thresholding* por nuligi ĉi tiujn malgravajn *features* (meti ilin al nulo). Male, la reto identigas la gravajn *features* kaj konservas ilin. Ĉi tiu procezo plifortigas la kapablon de la *deep neural network*. Ĉi tiu procezo helpas la reton eltiri utilajn *features* el signaloj, kiuj enhavas multe da *noise* (bruo).**

## 1. Esplora Motivado (Research Motivation)

**Unue, *noise* estas neevitebla kiam la *algorithm* klasifikas specimenojn. Ekzemple, ekzistas *Gaussian noise*, *pink noise*, kaj *Laplacian noise*.** Pli vaste, specimenoj ofte enhavas informojn, kiuj tute ne rilatas al la nuna klasifika tasko. Ni povas interpreti ĉi tiujn senrilatajn informojn kiel *noise*. Ĉi tiu *noise* eble malpliigos la klasifikan efikecon. (Ni notu, ke *Soft thresholding* estas kerna paŝo en multaj algoritmoj por *signal denoising*.)

Ekzemple, imagu konversacion ĉe la vojrando. La aŭdio eble enhavas la sonojn de aŭtaj kornoj kaj radoj. Ni eble volas fari *speech recognition* (parol-rekonon) sur ĉi tiuj signaloj. La fonaj sonoj neeviteble influos la rezultojn. De la perspektivo de *deep learning*, la *deep neural network* devus elimini la *features* respondajn al la kornoj kaj radoj. Ĉi tiu elimino malhelpas, ke tiuj *features* influu la rezultojn de la *speech recognition*.

**Due, la kvanto de *noise* ofte varias inter malsamaj specimenoj. Ĉi tiu variado okazas eĉ ene de la sama datumar-aro (dataset).** (Ĉi tiu variado havas similecojn kun *attention mechanisms*. Prenu bildan datumar-aron kiel ekzemplon. La loko de la cel-objekto eble malsamas tra la bildoj. La *attention mechanisms* povas fokusiĝi al la specifa loko de la cel-objekto en ĉiu bildo.)

Ekzemple, konsideru la trejnadon de klasifikilo por katoj kaj hundoj. Ni havas kvin bildojn etikeditajn kiel "hundo". Bildo 1 eble enhavas hundon kaj muson. Bildo 2 eble enhavas hundon kaj anseron. Bildo 3 eble enhavas hundon kaj kokidon. Bildo 4 eble enhavas hundon kaj azenon. Bildo 5 eble enhavas hundon kaj anason. Dum la trejnado, la senrilataj objektoj interferos kun la klasifikilo. Ĉi tiuj objektoj inkluzivas musojn, anserojn, kokidojn, azenojn, kaj anasojn. Ĉi tiu interfero rezultigas malpliigon de la klasifika precizeco. Supozu, ke ni povas identigi ĉi tiujn senrilatajn objektojn. Tiam, ni povas elimini la *features* respondajn al ĉi tiuj objektoj. Tiamaniere, ni povas plibonigi la precizecon de la klasifikilo por katoj kaj hundoj.

## 2. Pri Soft Thresholding (Soft Thresholding)

**La *Soft thresholding* estas kerna paŝo en multaj algoritmoj por *signal denoising*. La algoritmo eliminas *features* se la absolutaj valoroj de la *features* estas pli malaltaj ol certa *threshold* (sojlo). La algoritmo ŝrumpas *features* al nulo se la absolutaj valoroj de la *features* estas pli altaj ol ĉi tiu *threshold*.** Esploristoj povas efektivigi *soft thresholding* uzante la jenan formulon:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

La derivaĵo de la eligo de *soft thresholding* rilate al la enigo estas:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

La supra formulo montras, ke la derivaĵo de *soft thresholding* estas aŭ 1 aŭ 0. Ĉi tiu eco estas identa al la eco de la aktiviga funkcio *ReLU*. Tial, *soft thresholding* povas redukti la riskon de *gradient vanishing* kaj *gradient exploding* en algoritmoj de *deep learning*.

**En la funkcio de *soft thresholding*, la agordo de la *threshold* devas plenumi du kondiĉojn. Unue, la *threshold* devas esti pozitiva nombro. Due, la *threshold* ne povas superi la maksimuman valoron de la eniga signalo. Alie, la eligo estos tute nulo.**

**Krome, la *threshold* preferinde plenumu trian kondiĉon. Ĉiu specimeno havu sian propran sendependan *threshold* surbaze de la enhavo de *noise* en tiu specimeno.**

La kialo estas, ke la enhavo de *noise* ofte varias inter specimenoj. Ekzemple, Specimeno A eble enhavas malpli da *noise* dum Specimeno B enhavas pli da *noise* ene de la sama datumar-aro. En ĉi tiu kazo, Specimeno A devus uzi pli malgrandan *threshold* dum la *soft thresholding*. Specimeno B devus uzi pli grandan *threshold*. Kvankam ĉi tiuj *features* kaj *thresholds* perdas siajn eksplicitajn fizikajn difinojn en *deep neural networks*, la baza suba logiko restas la sama. Alivorte, ĉiu specimeno devus havi sendependan *threshold*. La specifa enhavo de *noise* determinas ĉi tiun *threshold*.

## 3. Mekanismo de Atento (Attention Mechanism)

Esploristoj povas facile kompreni *attention mechanisms* en la kampo de *computer vision*. La vidaj sistemoj de bestoj povas distingi celojn rapide skanante la tutan areon. Poste, la vidaj sistemoj fokusigas la atenton al la cel-objekto. Ĉi tiu ago permesas al la sistemoj eltiri pli da detaloj. Samtempe, la sistemoj subpremas senrilatajn informojn. Por detaloj, bonvolu konsulti la literaturon pri *attention mechanisms*.

La *Squeeze-and-Excitation Network* (SENet) reprezentas relative novan *deep learning* metodon, kiu utiligas *attention mechanisms*. Trans malsamaj specimenoj, malsamaj *feature channels* kontribuas malsame al la klasifika tasko. SENet uzas malgrandan sub-reton por akiri aron da *weights* (pezoj). Tiam, SENet multiplikas ĉi tiujn *weights* per la *features* de la respektivaj kanaloj. Ĉi tiu operacio alĝustigas la grandecon de la *features* en ĉiu kanalo. Ni povas rigardi ĉi tiun procezon kiel: **Apply weighting to each feature channel** (apliki pezon al ĉiu trajto-kanalo).

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

En ĉi tiu aliro, ĉiu specimeno posedas sendependan aron da *weights*. Alivorte, la *weights* por iuj ajn du arbitraj specimenoj estas malsamaj. En SENet, la specifa vojo por akiri *weights* estas "*Global Pooling* → *Fully Connected Layer* → *ReLU Function* → *Fully Connected Layer* → *Sigmoid Function*".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding kun Profunda Attention Mechanism (Soft Thresholding with Deep Attention Mechanism)

La *Deep Residual Shrinkage Network* uzas la strukturon de la sub-reto SENet. La reto uzas ĉi tiun strukturon por efektivigi *soft thresholding* sub profunda *attention mechanism*. La sub-reto (indikita ene de la ruĝa skatolo en la diagramo) faras gravan taskon: **Learn a set of thresholds** (lerni aron da sojloj). Tiam, la reto aplikas *soft thresholding* al ĉiu *feature channel* uzante ĉi tiujn *thresholds*.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

En ĉi tiu sub-reto, la sistemo unue kalkulas la absolutajn valorojn de ĉiuj *features* en la eniga *feature map*. Tiam, la sistemo faras *global average pooling* kaj averaĝadon por akiri trajton, notitan kiel A. En la alia vojo, la sistemo enigas la *feature map* en malgrandan *fully connected network* post la *global average pooling*. Ĉi tiu *fully connected network* uzas la funkcion *Sigmoid* kiel la finan tavolon. Ĉi tiu funkcio normaligas la eligon inter 0 kaj 1. Ĉi tiu procezo donas koeficienton, notitan kiel α. Ni povas esprimi la finan *threshold* kiel α × A. Do, la *threshold* estas la produkto de du nombroj. Unu nombro estas inter 0 kaj 1. La alia nombro estas la averaĝo de la absolutaj valoroj de la *feature map*. **Ĉi tiu metodo certigas, ke la *threshold* estas pozitiva. Ĉi tiu metodo ankaŭ certigas, ke la *threshold* ne estas troe granda.**

**Krome, malsamaj specimenoj rezultigas malsamajn *thresholds*. Sekve, ni povas interpreti ĉi tiun metodon kiel specialigitan *attention mechanism*. La mekanismo identigas *features* senrilatajn al la nuna tasko. La mekanismo transformas ĉi tiujn *features* en valorojn proksimajn al nulo per du *convolutional layers*. Tiam, la mekanismo metas ĉi tiujn *features* al nulo uzante *soft thresholding*. Alternative, la mekanismo identigas *features* rilatajn al la nuna tasko. La mekanismo transformas ĉi tiujn *features* en valorojn malproksimajn de nulo per du *convolutional layers*. Finfine, la mekanismo konservas ĉi tiujn *features*.**

Finfine, ni devas **Stack many basic modules** (stakigi multajn bazajn modulojn). Ni ankaŭ inkluzivas *convolutional layers*, *batch normalization*, *activation functions*, *global average pooling*, kaj *fully connected output layers*. Ĉi tiu procezo konstruas la kompletan *Deep Residual Shrinkage Network*. Kiel vi vidas en la strukturo, ekzistas ankaŭ **Identity path** (identeca vojo) kaj **Weighting** (pezado) komponantoj.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Ĝeneraliga Kapablo (Generalization Capability)

La *Deep Residual Shrinkage Network* estas ĝenerala metodo por *feature learning*. La kialo estas, ke specimenoj ofte enhavas *noise* en multaj taskoj de *feature learning*. Specimenoj ankaŭ enhavas senrilatajn informojn. Ĉi tiu *noise* kaj senrilataj informoj eble influos la efikecon de *feature learning*. Ekzemple:

Konsideru bildan klasifikon. Bildo eble samtempe enhavas multajn aliajn objektojn. Ni povas kompreni ĉi tiujn objektojn kiel "*noise*". La *Deep Residual Shrinkage Network* eble povos utiligi la *attention mechanism*. La reto rimarkas ĉi tiun "*noise*". Tiam, la reto uzas *soft thresholding* por meti la *features* respondajn al ĉi tiu "*noise*" al nulo. Ĉi tiu ago eble plibonigos la precizecon de bilda klasifiko.

Konsideru *speech recognition*. Specife, konsideru relative bruajn mediojn kiel konversaciajn agordojn ĉe vojrando aŭ ene de fabrika laborejo. La *Deep Residual Shrinkage Network* eble plibonigos la precizecon de *speech recognition*. Aŭ almenaŭ, la reto ofertas metodologion. Ĉi tiu metodologio kapablas plibonigi la precizecon de *speech recognition*.

## Referencoj (Reference)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Akademia Efiko (Academic Impact)

Ĉi tiu papero ricevis pli ol 1400 citaĵojn en *Google Scholar*.

Surbaze de nekompleta statistiko, esploristoj jam aplikis la *Deep Residual Shrinkage Network* (DRSN) en pli ol 1000 publikaĵoj/studoj. Ĉi tiuj aplikoj kovras larĝan gamon de kampoj. Ĉi tiuj kampoj inkluzivas mekanikan inĝenieradon, elektran energion, *vision*, sanservon, paroladon, tekston, radaron, kaj *remote sensing*.
