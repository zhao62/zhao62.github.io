---
layout: post
title: "Deep Residual Shrinkage Network: Highly Noisy Data लेई इक Artificial Intelligence Method"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-08
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network असल च Deep Residual Network दा ही इक सुधरेआ होआ रूप (improved variant) ऐ। मूल रूप च, Deep Residual Shrinkage Network अपने अंदर Deep Residual Network, attention mechanisms, ते soft thresholding functions नू जोड़दा ऐ।"
---

**Deep Residual Shrinkage Network असल च Deep Residual Network दा ही इक सुधरेआ होआ रूप (improved variant) ऐ। मूल रूप च, Deep Residual Shrinkage Network अपने अंदर Deep Residual Network, attention mechanisms, ते soft thresholding functions नू जोड़दा ऐ।**

**तुस Deep Residual Shrinkage Network दे कम्म करने दे तरीके नू इस चाल्ली समझ सकदे ओ। पैह्लें, एह network "attention mechanisms" दा इस्तेमाल करियै एह पता लगांदा ऐ जे केह्ड़े features important नहीं न। फिर, एह network "soft thresholding functions" दा इस्तेमाल करियै उन्ना unimportant features नू zero सेट करी दिंदा ऐ। इसदे उल्ट, एह network important features दी पछान करदा ऐ ते उन्ना नू संभालियै रक्खदा ऐ। एह process, deep neural network दी काबिलियत नू बधांदा ऐ। एह process, network दी मदद करदा ऐ तां जे ओ noise आह्‌ले signals बिच्चो useful features कड्ढी सकै।**

## 1. Research Motivation (शोध दा मकसद)

**पैह्‌ली गल, जदूं algorithm samples नू classify करदा ऐ, तां noise (शोर) कोला बचना नामुमकिन ऐ। इस noise दे examples न Gaussian noise, pink noise, ते Laplacian noise.** थोड़ी होर विस्तार च दिक्खीए, तां samples च अक्सर इहे जी information हुंदी ऐ जिदा current classification task कन्नै कोई लेना-देना नहीं हुंदा। अस इस irrelevant information नू **noise** समझ सकदे आं। एह noise तुंदी classification performance नू घट्ट करी सकदा ऐ। (Soft thresholding मता सारे signal denoising algorithms दा इक बड़ा जरूरी step ऐ।)

मिसाल दे तौर उप्पर, सड़क किनारे दी गल्ल-बात (conversation) नू लै लौ। Audio च गड्डियें दे horn ते पहियें दी आवाज होई सकदी ऐ। अस शायद इन्ना signals उप्पर **speech recognition** करगे। पर background sounds पक्का नतीजे उप्पर असर पानगियां। **Deep learning** दे नजरिए कन्नै दिक्खेआ जा, तां **deep neural network** नू horn ते पहियें दे features नू खत्म करी देना चाहिदा। एह्दा फायदा एह है कि एह features तुंदे **speech recognition** दे नतीजें नू खराब नहीं करन।

**दूजी गल, बक्ख-बक्ख samples च noise दी मात्रा अक्सर अलग-अलग हुंदी ऐ। एह्दा मतलब ऐ जे इक dataset दे अंदर बी noise दी मात्रा बदलदी रौह्‍न्दी ऐ।** (एह बदलाव **attention mechanisms** कन्नै काफी मिलता-जुलता ऐ। तुस इक image dataset दी मिसाल लै लौ। हर image च target object दी location बक्ख-बक्ख होई सकदी ऐ। **Attention mechanisms** हर image च target object दी खास location उप्पर focus करी सकदा ऐ।)

समझने लेई, मन्नी लौ कि तुस इक cat-and-dog classifier नू train कर करदे ओ, जिस च 5 फोटो उप्पर "dog" (कुत्ता) दा label लग्गा ऐ।
*   Image 1 च कुत्ता ते चूहा (mouse) होई सकदे न।
*   Image 2 च कुत्ता ते बतख (goose) होई सकदे न।
*   Image 3 च कुत्ता ते कुक्कड़ (chicken) होई सकदे न।
*   Image 4 च कुत्ता ते गधा (donkey) होई सकदे न।
*   Image 5 च कुत्ता ते बत्तख (duck) होई सकदे न।

Training दे दौरान, एह irrelevant objects (जियां चूहा, बतख, कुक्कड़, गधा, ते बत्तख) classifier नू disturb करनगे। इस interference दे कारण classification accuracy घट्ट होई सकदी ऐ। अगर अस इन्ना irrelevant objects नू पछानी सकचै, तां अस उन्ना दे features नू खत्म करी सकदे आं। इस तरीके कन्नै, अस cat-and-dog classifier दी accuracy नू बधा सकदे आं।

## 2. Soft Thresholding

**Soft thresholding मता सारे signal denoising algorithms दा इक core step ऐ। अगर features दी absolute values इक खास threshold कोला घट्ट होन्, तां algorithm उन्ना features नू खत्म करी दिंदा ऐ। अगर features दी absolute values उस threshold कोला ज्यादा होन्, तां algorithm उन्ना features नू zero दी तरफ shrink (सुकेड़ना) करी दिंदा ऐ।** Researchers इसनू हेठ लिखे फार्मूले (formula) कन्नै implement करी सकदे न:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Input दे मुकाबले soft thresholding output दा derivative ऐ:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

उप्पर दित्ता गया फार्मूला दसदा ऐ जे soft thresholding दा derivative जां तां 1 ऐ जां फिर 0 ऐ। एह property बिलकुल **ReLU activation function** दी property बांगर ऐ। इस करियै, **soft thresholding** deep learning algorithms च **gradient vanishing** ते **gradient exploding** दे खतरा नू घट्ट करी सकदा ऐ।

**Soft thresholding function दे बिच्च, threshold दी setting दो शर्तें (conditions) पूरी करनी चाहिदी। पैह्‌ली, threshold इक positive number होना चाहिदा। दूजी, threshold input signal दे maximum value कोला वड्डा नहीं होना चाहिदा। नहीं तां, सारा output zero होई जाग।**

**इसदे अलावा, threshold नू इक तीजी शर्त बी पूरी करनी चाहिदी। हर sample दा अपना बक्खरा (independent) threshold होना चाहिदा, जो उस sample दे noise content दे हिसाब कन्नै होवे।**

एह्दा कारण एह है कि हर sample च noise दी मात्रा बक्ख-बक्ख हुंदी ऐ। मिसाल लेई, इक ही dataset च, Sample A च शायद घट्ट noise होवे, पर Sample B च ज्यादा noise होवे। इस case च, soft thresholding दे दौरान Sample A नू छोटा threshold इस्तेमाल करना चाहिदा, ते Sample B नू वड्डा threshold इस्तेमाल करना चाहिदा। **Deep neural networks** च भले ही एह features ते thresholds अपनी साफ physical definitions खोई दिंदे न, पर बुनियादी logic ओही रौह्‍न्दा ऐ। मतलब कि, हर sample दा इक independent threshold होना चाहिदा। उस्दा specific noise content ही उस threshold नू तय करदा ऐ।

## 3. Attention Mechanism

Researchers लेई **computer vision** दी field च **attention mechanisms** नू समझना आसान ऐ। जानबरें दा visual system पूरे area नू तेजी कन्नै scan करियै targets नू पछानी लैंदा ऐ। उसदे बाद, visual system अपना ध्यान (attention) target object उप्पर focus करदा ऐ। एह action सिस्टम नू जादा details लब्भने च मदद करदा ऐ। कन्नै ही, सिस्टम irrelevant information नू दबाई (suppress) दिंदा ऐ। जादा जानकारी लेई, तुस **attention mechanisms** कन्नै जुड़ी literature पढ़ी सकदे ओ।

**Squeeze-and-Excitation Network (SENet)**, **attention mechanisms** नू इस्तेमाल करने आह्‌ला इक नमां **deep learning method** ऐ। अलग-अलग samples दे बिच्च, बक्ख-बक्ख feature channels classification task च अलग-अलग योगदान (contribution) दिंदे न। **SENet** इक छोटा sub-network यूज़ करदा ऐ तां जे weights दा इक set हासल कीता जाई सकै। फिर, **SENet** इन्ना weights नू उन्ना दे channels दे features कन्नै multiply (गुणा) करदा ऐ। एह operation हर channel दे features दे size नू adjust करदा ऐ। तुस इस process नू इस चाल्ली दिखी सकदे ओ कि बक्ख-बक्ख feature channels उप्पर अलग-अलग लेवल दी **attention** दित्ती जा करदी ऐ।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

इस तरीके च, हर sample कोला weights दा इक independent set हुंदा ऐ। आसान लफ्जें च, किन्हीं बी दो samples दे weights अलग-अलग हुंदे न। **SENet** च, weights हासल करने दा रस्ता (path) ऐ: "**Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function**."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism

**Deep Residual Shrinkage Network**, **SENet** sub-network दे structure दा इस्तेमाल करदा ऐ। Network इस structure दा use करियै **deep attention mechanism** दे तहत **soft thresholding** नू लागू करदा ऐ। एह sub-network (जेह्ड़ा लाल डिब्बे / red box च दिखाया गया ऐ) "**Learn a set of thresholds**" (thresholds दा सेट) सिखदा ऐ। फिर, network इन्ना thresholds दा इस्तेमाल करियै हर feature channel उप्पर **soft thresholding** अप्लाई करदा ऐ।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

इस sub-network च, सिस्टम पैह्लें input feature map दे सारे features दी absolute values कड्ढदा ऐ। फिर, सिस्टम **Global Average Pooling** ते averaging करदा ऐ तां जे इक feature मिलै, जिसनू *A* मनिये। दूजे रस्ते (path) च, **Global Average Pooling** दे बाद feature map नू इक छोटे **fully connected network** च पाह्‌या जांदा ऐ। एह **fully connected network** अपनी आखरी layer दे तौर उप्पर **Sigmoid function** दा use करदा ऐ। एह function output नू 0 ते 1 दे बिच्च normalize करदा ऐ। एत्थों इक coefficient मिलता ऐ, जिसनू *α* मनिये। अस final threshold नू *α × A* दे रूप च लिखी सकदे आं। इस करिए, threshold दो नंबरें दा product (गुणा) ऐ। इक नंबर 0 ते 1 दे बिच्च ऐ। दूजा नंबर feature map दी absolute values दी average ऐ। **एह तरीका पक्का करदा ऐ जे threshold positive होवे। एह तरीका एह बी पक्का करदा ऐ जे threshold बहुत ज्यादा वड्डा न होवे।**

**इसदे कन्नै-कन्नै, अलग-अलग samples दे thresholds बी अलग-अलग हुंदे न। नतीजे दे तौर उप्पर, अस इस method नू इक "specialized attention mechanism" समझ सकदे आं। एह mechanism पछान करदा ऐ जे केहड़े features current task लेई irrelevant न। फिर एह mechanism दो convolutional layers दे राहीं उन्ना features नू zero दे करीब लैयांदा ऐ। फिर, एह mechanism "**soft thresholding**" कन्नै उन्ना features नू zero सेट करी दिंदा ऐ। जां फिर, एह mechanism पछान करदा ऐ जे केहड़े features current task लेई relevant (जरूरी) न। एह mechanism दो convolutional layers दे राहीं उन्ना features नू zero कोला दूर लई जांदा ऐ। अंत च, एह mechanism उन्ना features नू preserve करदा (बचाई रक्खदा) ऐ।**

अंत च, अस कुझ **basic modules** नू stack करदे आं (इक दे उप्पर इक रखदे आं)। कन्नै ही अस **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, ते **fully connected output layers** बी शामिल करदे आं। एह सारा process मिलियै पूरा **Deep Residual Shrinkage Network** बनांदा ऐ।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability (आम इस्तेमाल दी काबिलियत)

**Deep Residual Shrinkage Network**, feature learning लेई इक general method ऐ। एह्दा कारण एह है कि मता सारे feature learning tasks च samples दे अंदर अक्सर noise हुंदा ऐ। Samples च irrelevant information बी हुंदी ऐ। एह noise ते irrelevant information, feature learning दी performance उप्पर असर पाई सकदे न। मिसाल दे तौर उप्पर:

**Image classification** नू लै लौ। इक फोटो च होर बी कई objects होई सकदे न। अस उन्ना objects नू "noise" समझ सकदे आं। **Deep Residual Shrinkage Network** शायद **attention mechanism** दा फायदा चुक्की सकदा ऐ। Network इस "noise" नू notice करदा ऐ। फिर, network **soft thresholding** दा use करियै उस "noise" दे features नू zero सेट करी दिंदा ऐ। एह action **image classification** दी accuracy नू बधाने च मदद करी सकदा ऐ।

**Speech recognition** नू लै लौ। खास करियै शोर-शराबे आह्‌ले माहौल च, जियां सड़क किनारे जां फैक्ट्री दे अंदर दी गल-बात। **Deep Residual Shrinkage Network**, **speech recognition** दी accuracy नू सुधारी सकदा ऐ। जां घट्टो-घट्ट, एह network इक ऐसा तरीका (methodology) दिंदा ऐ, जेह्ड़ा **speech recognition accuracy** नू बधाने दे काबिल ऐ।

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact (अकादमिक प्रभाव)

Google Scholar उप्पर इस paper नू 1,400 कोला बद्ध citations मिलियां न।

अधूरे आँकड़ें दे मुताबिक, researchers ने **Deep Residual Shrinkage Network (DRSN)** नू 1,000 कोला जादा publications/studies च apply कीता ऐ। एह applications मता सारें fields च न। इन्ना fields च शामिल न: mechanical engineering, electrical power, vision, healthcare, speech, text, radar, ते remote sensing.
