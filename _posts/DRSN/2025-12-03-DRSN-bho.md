---
layout: post
title: "Deep Residual Shrinkage Network: Highly Noisy Data खातिर एक Artificial Intelligence Method"
date: 2025-12-03
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network, Deep Residual Network के एक improved variant ह। असल में, ई Deep Residual Network, attention mechanisms, और soft thresholding functions के integration ह।**

**कुछ हद तक, Deep Residual Shrinkage Network के काम करे के तरीका अइसन समझल जा सकेला: ई attention mechanisms के use करके unimportant features के पहचानेला और soft thresholding functions के मदद से उनके zero set कर देला; वही दूसरी तरफ, ई important features के पहचान के उनके बचा के रखेला। ई process से deep neural network के noisy signals में से काम के features extract करे के क्षमता बढ़ जाला।**

## 1. Research Motivation

**सबसे पहले, जब हम samples के classify करेनी जा, त noise—जैसे कि Gaussian noise, pink noise, और Laplacian noise—के होना तय बा।** थोड़ा और detail में कही, त samples में अक्सर अइसन information होखला जेकर current classification task से कोई लेना-देना ना होखला, जेके भी noise समझल जा सकेला। ई noise classification performance पर खराब असर डाल सकेला। (Soft thresholding ढेर सारा signal denoising algorithms में एक बहुत जरूरी step होखला।)

Example खातिर, अगर सड़क के किनारे बात-चीत होत हो, त आवाज में गाड़ी के horn और पहिया के आवाज मिलल हो सकेला। जब इन signals पर speech recognition कइल जाई, त results पर इन background आवाजों के असर पड़ल तय बा। Deep learning के नजर से देखल जाव, त horn और पहिया वाला features के deep neural network के अंदर ही हटा देवे के चाही ताकि उ speech recognition results के खराब ना करे।

**दूसरा बात, एक ही dataset में भी, हर sample में noise के मात्रा अक्सर अलग-अलग होखला।** (ई attention mechanisms से काफी मिलता-जुलता बा; अगर एक image dataset के example ली, त target object के location हर image में अलग हो सकेला, और attention mechanisms हर image में target object के सही location पर focus कर सकेला।)

Example खातिर, जब एक cat-and-dog classifier train कइल जाला, त मान ली 5 गो images बा जेकर label "dog" बा। पहला image में dog और mouse हो सकेला, दूसरा में dog और goose, तीसरा में dog और chicken, चौथा में dog और donkey, और पांचवा में dog और duck हो सकेला। Training के दौरान, classifier के इन irrelevant objects—जैसे कि mouse, goose, chicken, donkey, और duck—से interference मिलल तय बा, जिससे classification accuracy कम हो सकेला। अगर हम इन irrelevant objects—mouse, goose, chicken, donkey, और duck—के notice कर लेईं और इनका corresponding features के eliminate कर देईं, त cat-and-dog classifier के accuracy बढ़ावे के संभावना बन जाला।

## 2. Soft Thresholding

**Soft thresholding ढेर सारा signal denoising algorithms के core step ह। ई उन features के हटा देला (eliminate) जिनकर absolute values एक particular threshold से कम होखला और उन features के zero के तरफ shrink करेला जिनकर absolute values threshold से ज्यादा होखला।** इसके नीचे दिए गइल formula से implement कइल जा सकेला:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Input के respect में soft thresholding output के derivative ई बा:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

जैसा कि ऊपर देखल गइल बा, soft thresholding के derivative या त 1 होखला या 0. ई property बिलकुल ReLU activation function जइसन ह। इसलिए, soft thresholding deep learning algorithms के gradient vanishing और gradient exploding के risk से बचावे में भी मदद कर सकेला।

**Soft thresholding function में, threshold set करते समय दो शर्तों (conditions) के पूरा होना जरूरी बा: पहला, threshold positive होना चाही; दूसरा, threshold input signal के maximum value से बड़ा नहीं होना चाही, नहीं त output पूरा zero हो जाई।**

**एक और बात, threshold के तीसरा condition भी पूरा करे के चाही: हर sample के खातिर ओकर noise content के हिसाब से अपना अलग independent threshold होके के चाही।**

अइसन इसलिए काहे कि अक्सर samples में noise content अलग-अलग होखला। Example खातिर, एक ही dataset में Sample A में कम noise हो सकेला और Sample B में ज्यादा noise हो सकेला। अइसन case में, जब denoising algorithm में soft thresholding कइल जाला, त Sample A खातिर छोटा threshold use कइल जाना चाही, जबकि Sample B खातिर बड़ा threshold. हालांकि deep neural networks में इन features और thresholds के कोई साफ़ physical definition ना होखला, लेकिन basic logic same रहेला। दूसरे शब्दों में, हर sample के पास ओकर specific noise content के हिसाब से अपना independent threshold होना चाही।

## 3. Attention Mechanism

Computer vision के field में Attention mechanisms के समझल आसान बा। जानवरों के visual system पूरा area के तेजी से scan करके target के पहचान सकेला, और फिर target object पर ध्यान (attention) लगा के और details extract कर सकेला, जबकि irrelevant information के ignore कर सकेला। ज्यादा जानकारी खातिर, attention mechanisms से related literature पढ़ सकत बानी।

Squeeze-and-Excitation Network (SENet) एक relatively नया deep learning method ह जो attention mechanisms के use करेला। अलग-अलग samples में, classification task में अलग-अलग feature channels के contribution अक्सर अलग-अलग होखला। SENet एक छोटा sub-network use करके **"Learn a set of weights"** (weights के एक set) प्राप्त करेला और फिर इन weights के corresponding channels के features से multiply कर देला ताकि हर channel के features के magnitude adjust कइल जा सके। ई process के अइसन देखल जा सकेला कि अलग-अलग feature channels पर अलग-अलग level के attention दीयल जात बा (**Apply weighting to each feature channel**).

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

ई तरीका में, हर sample के पास weights के अपना independent set होखला। दूसरे शब्दों में, कोई भी दो samples के weights अलग-अलग होखला। SENet में, weights प्राप्त करे के specific रास्ता (path) ह: "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Deep Attention Mechanism के साथ Soft Thresholding

Deep Residual Shrinkage Network ऊपर बतावल गइल SENet sub-network structure से idea लेले बा ताकि deep attention mechanism के अंदर soft thresholding implement कइल जा सके। Sub-network (जो लाल डब्बा/red box में देखल जा सकेला) के मदद से, **"Learn a set of thresholds"** संभव बा ताकि हर feature channel पर soft thresholding लगावल जा सके।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

इस sub-network में, सबसे पहले input feature map के सारा features के absolute values calculate कइल जाला। फिर, global average pooling और averaging के बाद, एक feature मिलेला, जेके A मानल जाला। दूसरी तरफ, global average pooling के बाद feature map के एक छोटे fully connected network में input कइल जाला। ई fully connected network Sigmoid function के अपने आखिरी layer के रूप में use करेला ताकि output 0 और 1 के बीच में normalize हो सके, जिससे एक coefficient मिलेला, जेके α मानल जाला। Final threshold के α×A के रूप में लिखल जा सकेला। इसलिए, threshold एक अइसन number ह जो 0 और 1 के बीच के value और feature map के absolute values के average के product (गुणनफल) ह। **ई तरीका ensure करेला कि threshold न सिर्फ positive हो, बल्कि बहुत बड़ा भी न हो।**

**इसके अलावा, अलग-अलग samples खातिर अलग-अलग thresholds बन जाला। नतीजन, कुछ हद तक, इसके एक specialized attention mechanism समझल जा सकेला: ई current task के लिए irrelevant features के notice करेला, उनके दो convolutional layers के जरिए 0 के करीब values में बदल देला, और फिर "Soft thresholding" के use करके उनके zero set कर देला; या फिर, ई current task के लिए relevant features के notice करेला, उनके दो convolutional layers के जरिए 0 से दूर values में बदल देला, और उनके बचा के रखेला।**

अंत में, कुछ **"Stack many basic modules"** (basic modules के stack करना) के साथ-साथ convolutional layers, batch normalization, activation functions, global average pooling, और fully connected output layers के जोड़ के, पूरा Deep Residual Shrinkage Network तैयार होखला। इसमे **"Identity path"** भी शामिल बा जो features के preserve करे में मदद करेला।

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability

Deep Residual Shrinkage Network असल में एक general feature learning method ह। अइसन इसलिए बा काहे कि ढेर सारा feature learning tasks में, samples में थोड़ा बहुत noise और irrelevant information होखला ही बा। ई noise और irrelevant information feature learning के performance पर असर डाल सकेला। Example खातिर:

Image classification में, अगर एक image में ढेर सारा दूसरा objects भी शामिल बा, त इन objects के "noise" समझल जा सकेला। Deep Residual Shrinkage Network शायद attention mechanism के use करके इस "noise" के notice कर सके और फिर soft thresholding के जरिए इस "noise" वाला features के zero set कर सके, जिससे image classification accuracy बढ़ सकेला।

Speech recognition में, ख़ास करके ज्यादा शोर-शराबा वाला माहौल में (जैसे सड़क किनारे या factory workshop में बात-चीत), Deep Residual Shrinkage Network speech recognition accuracy बढ़ा सकेला, या कम से कम एक अइसन methodology दे सकेला जो speech recognition accuracy बढ़ावे में सक्षम हो।

## Reference

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact

ई paper के Google Scholar पर 1,400 से ज्यादा citations मिल चुकल बा।

Incomplete statistics के मुताबिक, Deep Residual Shrinkage Network (DRSN) के directly apply कइल गइल बा या modify करके apply कइल गइल बा 1,000 से ज्यादा publications/studies में, जो कि mechanical engineering, electrical power, vision, healthcare, speech, text, radar, और remote sensing जैसे ढेर सारा fields में फैलल बा।
