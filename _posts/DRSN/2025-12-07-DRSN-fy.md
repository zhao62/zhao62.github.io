---
layout: post
title: "Deep Residual Shrinkage Network: In Artificial Intelligence metoade foar Highly Noisy Data"
date: 2025-12-07
tags: [Deep Learning, AI]
mathjax: true
---

**It Deep Residual Shrinkage Network is in ferbettere fariant fan it Deep Residual Network. Yn wêzen yntegrearret it Deep Residual Shrinkage Network it Deep Residual Network, attention mechanisms, en soft thresholding funksjes.**

**Wy kinne it wurkingprinsipe fan it Deep Residual Shrinkage Network op de folgjende manier begripe. Earst brûkt it netwurk attention mechanisms om ûnbelangrike features te identifisearjen. Dêrnei brûkt it netwurk soft thresholding funksjes om dizze ûnbelangrike features op nul te setten. Oarsom identifisearret it netwurk wichtige features en behâldt dizze. Dit proses fersterket de kapasiteit fan it deep neural network. Dit helpt it netwurk om nuttige features te heljen út sinjalen dy't noise befetsje.**

## 1. Undersyksmotivaasje (Research Motivation)

**Earst is noise ûntkomber as it algoritme samples klassifisearret. Foarbylden fan dizze noise binne Gaussian noise, pink noise, en Laplacian noise.** Mear algemien befetsje samples faak ynformaasje dy't net relevant is foar de hjoeddeistige klassifikaasjetaak. Wy kinne dizze irrelevante ynformaasje ynterpretearje as **noise**. Dizze **noise** kin de klassifikaasjeprestaasjes ferminderje. (**Soft thresholding** is in wichtige stap yn in protte algoritmen foar sinjaal-denoising.)

Tink bygelyks oan in petear oan de kant fan 'e dyk. De audio kin de lûden fan autotoeters en tsjillen befetsje. Wy soene **speech recognition** op dizze sinjalen útfiere kinne. De eftergrûnlûden sille de resultaten sûnder mis beynfloedzje. Fanút in **deep learning** perspektyf moat it **deep neural network** de features eliminearje dy't oerienkomme mei de toeters en tsjillen. Dizze eliminaasje foarkomt dat de features de resultaten fan de **speech recognition** beynfloedzje.

**Twads ferskilt de hoemannichte noise faak tusken samples. Dizze variaasje komt sels foar binnen deselde dataset.** (Dizze variaasje hat oerienkomsten mei **attention mechanisms**. Nim in ôfbyldingsdataset as foarbyld. De lokaasje fan it doelobjekt kin ferskille per ôfbylding. **Attention mechanisms** kinne fokusje op de spesifike lokaasje fan it doelobjekt yn elke ôfbylding.)

Litte wy sizze, wy train in kat-en-hûn **classifier** mei fiif ôfbyldings dy't as "hûn" labele binne. Ofbylding 1 kin in hûn en in mûs befetsje. Ofbylding 2 kin in hûn en in goes befetsje. Ofbylding 3 kin in hûn en in hin befetsje. Ofbylding 4 kin in hûn en in ezel befetsje. Ofbylding 5 kin in hûn en in ein befetsje. Tidens de training sille irrelevante objekten de **classifier** steure. Dizze objekten binne ûnder oare mûzen, guozzen, hinnen, ezels en einen. Dizze steuring resultearret yn in fermindering fan de klassifikaasje-krektens (accuracy). Stel dat wy dizze irrelevante objekten identifisearje kinne. Dan kinne wy de features eliminearje dy't oerienkomme mei dizze objekten. Op dizze manier kinne wy de krektens fan de kat-en-hûn **classifier** ferbetterje.

## 2. Soft Thresholding

**Soft thresholding is in kearnstad yn in protte algoritmen foar sinjaal-denoising. It algoritme eliminearret features as de absolute wearden fan de features leger binne as in bepaalde threshold. It algoritme krimpt ("shrinks") features rjochting nul as de absolute wearden fan de features heger binne as dizze threshold.** Undersikers kinne **soft thresholding** ymplemintearje mei de folgjende formule:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

De derivative fan de **soft thresholding** output oangeande de input is:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

De formule hjirboppe lit sjen dat de derivative fan **soft thresholding** of 1 of 0 is. Dizze eigenskip is identyk oan de eigenskip fan de **ReLU activation function**. Dêrom kin **soft thresholding** it risiko op **gradient vanishing** en **gradient exploding** yn **deep learning** algoritmen ferminderje.

**Yn de soft thresholding funksje moat de ynstelling fan de threshold oan twa betingsten foldwaan. Ten earste moat de threshold in posityf getal wêze. Ten twadde mei de threshold net heger wêze as de maksimale wearde fan it ynfiersinjaal. Oars sil de output folslein nul wêze.**

**Derneist soe de threshold by foarkar oan in tredde betingst foldwaan moatte. Elke sample soe syn eigen ûnôfhinklike threshold hawwe moatte op basis fan de noise ynhâld fan de sample.**

De reden is dat de **noise** ynhâld faak ferskilt tusken samples. Bygelyks, Sample A kin minder **noise** befetsje, wylst Sample B mear **noise** befettet binnen deselde **dataset**. Yn dit gefal soe Sample A in lytsere **threshold** brûke moatte tidens **soft thresholding**. Sample B soe in gruttere **threshold** brûke moatte. Yn **deep neural networks** ferlieze dizze features en **thresholds** harren eksplisite fysike definysjes. De basislogika bliuwt lykwols itselde. Mei oare wurden, elke sample soe in ûnôfhinklike **threshold** hawwe moatte. De spesifike **noise** ynhâld bepaalt dizze **threshold**.

## 3. Attention Mechanism

Undersikers kinne **attention mechanisms** maklik begripe op it mêd fan **computer vision**. De fisuele systemen fan bisten kinne doelen ûnderskiede troch it folsleine gebiet fluch te scannen. Dêrnei rjochtsje de fisuele systemen **attention** op it doelobjekt. Dizze aksje lit de systemen mear details helje. Tagelyk ûnderdrukke de systemen irrelevante ynformaasje. Foar details kinne jo ferwize nei literatuer oer **attention mechanisms**.

It **Squeeze-and-Excitation Network** (SENet) fertsjintwurdiget in relatyf nije **deep learning** metoade dy't **attention mechanisms** brûkt. Oer ferskate samples hinne drage ferskate **feature channels** oars by oan de klassifikaasjetaak. SENet brûkt in lyts sub-netwurk om in set **weights** te krijen (**Learn a set of weights**). Dan fermannichfâldiget SENet dizze **weights** mei de features fan de respektivelike kanalen. Dit toant it prinsipe fan "**Apply weighting to each feature channel**". Wy kinne dit proses sjen as it tapassen fan ferskate nivo's fan **attention** op ferskate **feature channels**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Yn dizze oanpak hat elke sample in ûnôfhinklike set **weights**. Mei oare wurden, de **weights** foar twa willekeurige samples binne oars. Yn SENet is it spesifike paad foar it krijen fan **weights**: "**Global Pooling** → **Fully Connected Layer** → **ReLU Function** → **Fully Connected Layer** → **Sigmoid Function**".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding mei Deep Attention Mechanism

It **Deep Residual Shrinkage Network** brûkt de struktuer fan it SENet sub-netwurk. It netwurk brûkt dizze struktuer om **soft thresholding** te ymplemintearjen ûnder in **deep attention mechanism**. It sub-netwurk (oanjûn yn it reade fak) leart in set **thresholds** (**Learn a set of thresholds**). Dan past it netwurk **soft thresholding** ta op elk **feature channel** mei dizze **thresholds**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Yn dit sub-netwurk berekkent it systeem earst de absolute wearden fan alle features yn de input **feature map**. Dan fiert it systeem **global average pooling** en middeling út om in feature te krijen, oantsjutten as A. Yn it oare paad (dy't wy kinne sjen as de **Identity path** en it proses fan **Weighting**) fiert it systeem de **feature map** yn in lyts **fully connected network** nei **global average pooling**. Dit **fully connected network** brûkt de **Sigmoid function** as de lêste **layer**. Dizze funksje normalisearret de output tusken 0 en 1. Dit proses smyt in koëffisjint op, oantsjutten as α. Wy kinne de úteinlike **threshold** útdrukke as α × A. Dêrom is de **threshold** it produkt fan twa getallen. Ien getal leit tusken 0 en 1. It oare getal is it gemiddelde fan de absolute wearden fan de **feature map**. **Dizze metoade soarget derfoar dat de threshold posityf is. Dizze metoade soarget der ek foar dat de threshold net te grut is.**

**Fierder resultearje ferskillende samples yn ferskillende thresholds. Dêrtroch kinne wy dizze metoade ynterpretearje as in spesjalisearre attention mechanism. It meganisme identifisearret features dy't net relevant binne foar de hjoeddeistige taak. It meganisme transformearret dizze features nei wearden tichtby nul fia twa convolutional layers. Dan set it meganisme dizze features op nul mei help fan soft thresholding. Oarsom identifisearret it meganisme features dy't relevant binne foar de hjoeddeistige taak. It meganisme transformearret dizze features nei wearden fier fan nul fia twa convolutional layers. As lêste behâldt it meganisme dizze features.**

Ta beslút steapelje wy in bepaald oantal **basic modules** (**Stack many basic modules**). Wy foegje ek **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, en **fully connected output layers** ta. Dit proses konstruearret it folsleine **Deep Residual Shrinkage Network**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalisaasjefermogen (Generalization Capability)

It **Deep Residual Shrinkage Network** is in algemiene metoade foar **feature learning**. De reden is dat samples faak **noise** befetsje yn in protte **feature learning** taken. Samples befetsje ek irrelevante ynformaasje. Dizze **noise** en irrelevante ynformaasje kinne de prestaasjes fan **feature learning** beynfloedzje. Bygelyks:

Tink oan **image classification**. In ôfbylding kin tagelyk in protte oare objekten befetsje. Wy kinne dizze objekten begripe as "**noise**". It **Deep Residual Shrinkage Network** is mooglik yn steat om it **attention mechanism** te brûken. It netwurk merkt dizze "**noise**" op. Dan brûkt it netwurk **soft thresholding** om de features dy't oerienkomme mei dizze "**noise**" op nul te setten. Dizze aksje ferbetteret mooglik de krektens fan **image classification**.

Tink oan **speech recognition**. Spesifyk, tink oan relatyf lûdroftige ("noisy") omjouwings lykas petearsettings oan de kant fan 'e dyk of binnen in fabrykshal. It **Deep Residual Shrinkage Network** kin de krektens fan **speech recognition** ferbetterje. Of op syn minst biedt it netwurk in metodology. Dizze metodology is by steat om de krektens fan **speech recognition** te ferbetterjen.

## Referinsje (Reference)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Akademyske Impact

Dit papier ("paper") hat mear as 1400 sitaten krigen op **Google Scholar**.

Op basis fan ûnfolsleine statistiken hawwe ûndersikers it **Deep Residual Shrinkage Network** (DRSN) tapast yn mear as 1000 publikaasjes/stúdzjes. Dizze tapassingen beslane in breed skala oan fjilden. Dizze fjilden omfetsje meganyske technyk, elektryske krêft, **vision**, sûnenssoarch, spraak, tekst, radar, en **remote sensing**.
