---
layout: post
title: "Deep Residual Shrinkage Network: Wanpela Artificial Intelligence Wei bilong Wok wantaim Highly Noisy Data"
subtitle: "An Artificial Intelligence Method for Highly Noisy Data"
date: 2025-12-12
tags: [Deep Learning, AI]
mathjax: true
---

**Deep Residual Shrinkage Network em i wanpela nupela kain Deep Residual Network we mipela i impruvim pinis. Long tok sot, Deep Residual Shrinkage Network em i miksim Deep Residual Network, Attention mechanisms, na Soft thresholding functions wantaim.**

**Yumi ken klia long hau Deep Residual Shrinkage Network i wok long dispela wei: Pastaim, network i usim Attention mechanisms bilong painim ol feature we i no impoten. Bihain, network i usim Soft thresholding functions bilong mekim ol dispela feature i kamap zero. Na long narapela sait, network i save painim ol impoten feature na holim ol gut. Dispela pasin i strongim pawa bilong deep neural network. Em i helpim network long kisim ol yusful feature long ol signal we i gat planti Noise.**

## 1. As Tingting bilong Resis (Research Motivation)

**Namba wan samting, taim algorithm i laik skelim (classify) ol sample, Noise em i samting yumi no inap abrusim. Ol kain Noise em olsem Gaussian noise, Pink noise, na Laplacian noise.** Long wei we i bikpela moa, ol **sample** i save gat planti toksave (information) we i no helpim wok bilong **classification**. Yumi ken tingim ol dispela toksave we i no helpim yumi olsem **Noise**. Dispela **Noise** inap mekim **classification performance** i go daun. (**Soft thresholding** em i wanpela ki step long planti **signal denoising algorithms**.)

Tingim sapos yumi stori arere long rot. **Audio** inap karim pairap bilong hon bilong kar na pairap bilong wil. Yumi laik mekim **speech recognition** long ol dispela **signal**. Tasol, pairap bilong bakgran bai bagarapim **result**. Long ai bilong **Deep Learning**, **deep neural network** i mas rausim ol **feature** bilong hon na wil. Dispela pasin bilong rausim em i bilong stopim ol dispela **feature** na bai ol i no ken bagarapim **speech recognition results**.

**Namba tu, hamas Noise i stap, em i no wankain long olgeta sample. Dispela senis i save kamap maski insait long wankain dataset.** (Dispela senis i wankain liklik long **Attention mechanisms**. Tingim wanpela **image dataset**. Ples we **target object** i stap em i no wankain long olgeta **image**. **Attention mechanisms** i ken lukluk stret long ples we **target object** i stap long wan wan **image**.)

Olsem, tingim yumi trenim wanpela **cat-and-dog classifier** wantaim faivpela **image** we i gat lebel "dog". **Image 1** i gat dok na mause. **Image 2** i gat dok na gus. **Image 3** i gat dok na kakaruk. **Image 4** i gat dok na donki. **Image 5** i gat dok na pato. Taim yumi wok long trenim, ol arapela samting we yumi no laikim bai bagarapim **classifier**. Ol dispela samting em mause, gus, kakaruk, donki, na pato. Dispela hevi bai mekim **classification accuracy** i go daun. Sapos yumi inap painim ol dispela samting we yumi no laikim. Orait, yumi ken rausim ol **feature** bilong ol. Long dispela wei, yumi ken impruvim **accuracy** bilong **cat-and-dog classifier**.

## 2. **Soft Thresholding**

**Soft thresholding em i wanpela krap (core) step long planti signal denoising algorithms. Algorithm bai rausim ol feature sapos absolute values bilong ol feature i liklik moa long wanpela threshold. Na algorithm bai mekim ol feature i go klostu long zero (shrink) sapos absolute values bilong ol feature i bikpela moa long dispela threshold.** Ol saveman i ken usim dispela formula bilong mekim **Soft thresholding**:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Dispela **derivative** bilong **Soft thresholding output** long **input** em olsem:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Dispela formula antap i soim olsem **derivative** bilong **soft thresholding** em i 1 o 0. Dispela pasin em i wankain stret long pasin bilong **ReLU activation function**. Olsem na, **soft thresholding** i ken daunim hevi bilong **gradient vanishing** na **gradient exploding** insait long **deep learning algorithms**.

**Insait long soft thresholding function, wei bilong setim threshold i mas bihainim tupela rule. Namba wan, threshold i mas stap positive number. Namba tu, threshold i no ken winim maximum value bilong input signal. Sapos nogat, output bai kamap olsem zero olgeta.**

**Na tu, i moabeta sapos threshold i bihainim namba tri rule. Wan wan sample i mas gat threshold bilong em yet, na dispela i dipen long hamas Noise i stap insait long sample.**

As bilong dispela em olsem: hamas **Noise** i stap, em i no wankain long olgeta **sample**. Olsem, **Sample A** inap gat liklik **Noise** tasol **Sample B** i gat planti **Noise** insait long wankain **dataset**. Long dispela kain taim, **Sample A** i mas usim liklik **threshold** taim yumi mekim **soft thresholding**. Na **Sample B** i mas usim bikpela **threshold**. Insait long **deep neural networks**, ol dispela **feature** na **threshold** i lusim ol mining bilong ol long fizikol world. Tasol, as tingting i stap wankain. Em i min olsem, wan wan **sample** i mas gat **independent threshold**. Hamas **noise** stret i stap, em bai makim dispela **threshold**.

## 3. **Attention Mechanism**

Ol saveman i ken klia kwiktaim long **Attention mechanisms** insait long **computer vision** fil. Ai bilong animal i save luksave long ol samting long rot bilong skanim kwiktaim olgeta eria. Bihain, ai bai putim **attention** (tingting) long **target object**. Dispela aksen i helpim sistem long kisim planti **details** moa. Long wankain taim, sistem i save pasim (suppress) ol toksave we i no impoten. Bilong save moa, plis lukim ol pepa long **Attention mechanisms**.

**Squeeze-and-Excitation Network (SENet)** em i makim wanpela nupela **deep learning method** we i usim **Attention mechanisms**. Namel long ol narapela narapela **sample**, ol **feature channel** i no helpim **classification task** long wankain wei. **SENet** i save usim wanpela liklik **sub-network** bilong kisim wanpela set bilong **weights**. Bihain, **SENet** i malitiplaim (multiply) ol dispela **weights** wantaim ol **feature** bilong ol **channel**. Dispela wok i save adjustim sais bilong ol **feature** long wan wan **channel**. Yumi ken lukim dispela wok olsem pasin bilong putim **attention** long ol **feature channel** long narapela narapela mak.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Long dispela wei, wan wan **sample** i gat **independent set of weights**. Em i min olsem, **weights** bilong tupela arapela **sample** i no wankain. Insait long **SENet**, rot bilong kisim **weights** em olsem: "**Global Pooling** → **Fully Connected Layer** → **ReLU Function** → **Fully Connected Layer** → **Sigmoid Function**".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. **Soft Thresholding** wantaim **Deep Attention Mechanism**

**Deep Residual Shrinkage Network** i usim strakta bilong **SENet sub-network**. **Network** i usim dispela strakta bilong mekim **soft thresholding** aninit long **deep attention mechanism**. Dispela **sub-network** (we i stap insait long retpela bokis) i save **Learn a set of thresholds**. Bihain, **network** i **Apply weighting to each feature channel** long rot bilong **soft thresholding** wantaim ol dispela **threshold**.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Insait long dispela **sub-network**, sistem i kalkuletim pastaim **absolute values** bilong olgeta **feature** insait long **input feature map**. Bihain, sistem i mekim **global average pooling** na **averaging** bilong kisim wanpela **feature**, yumi kolim A. Long narapela **Identity path**, sistem i putim **feature map** i go insait long wanpela liklik **fully connected network** bihain long **global average pooling**. Dispela **fully connected network** i usim **Sigmoid function** olsem las layer. Dispela **function** i save **normalize** ol **output** namel long 0 na 1. Dispela wok i save kamapim wanpela namba, yumi kolim α. Yumi ken raitim **threshold** olsem α x A. Olsem na, **threshold** em i tupela namba i malitiplai wantaim. Wanpela namba i stap namel long 0 na 1. Narapela namba em i **average** bilong **absolute values** bilong **feature map**. **Dispela wei i mekim sua olsem threshold i stap positive. Dispela wei i mekim sua tu olsem threshold i no bikpela tumas.**

**Na tu, narapela narapela sample bai kamapim narapela narapela threshold. So, yumi ken klia long dispela wei olsem wanpela spesol Attention mechanism. Dispela mechanism i save painim ol feature we i no impoten long wok yumi mekim. Dispela mechanism i save senisim ol dispela feature i go kamap namba we i klostu long zero long rot bilong tupela convolutional layer. Bihain, mechanism i setim ol dispela feature i go zero long rot bilong soft thresholding. Orait, mechanism i save painim ol feature we i impoten long wok. Mechanism i save senisim ol dispela feature i go kamap namba we i longwe long zero. Las samting, mechanism i holim na kipim ol dispela feature.**

Long pinis, yumi **Stack many basic modules**. Mipela i putim tu ol **convolutional layers**, **batch normalization**, **activation functions**, **global average pooling**, na **fully connected output layers**. Dispela wok i bildim **Deep Residual Shrinkage Network** we i redi olgeta.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Pawa bilong Generalais (Generalization Capability)

**Deep Residual Shrinkage Network** em i wanpela **general method** bilong **feature learning**. As bilong em olsem, long planti wok bilong **feature learning**, ol **sample** i save gat **Noise**. Ol **sample** i gat tu ol toksave we i no impoten. Ol dispela **Noise** na toksave we i no impoten inap bagarapim wok bilong **feature learning**. Tingim olsem:

Tingim **Image classification**. Wanpela piksa inap gat planti arapela samting insait. Yumi ken tingim ol dispela arapela samting olsem "**Noise**". **Deep Residual Shrinkage Network** inap usim **attention mechanism**. **Network** i luksave long ol dispela "**Noise**". Bihain, **network** i usim **soft thresholding** bilong setim ol **feature** bilong "**Noise**" i go zero. Dispela aksen inap impruvim **image classification accuracy**.

Tingim **Speech recognition**. Spesol tru long ol ples we i gat planti pairap olsem stori arere long rot o insait long faktori. **Deep Residual Shrinkage Network** inap impruvim **speech recognition accuracy**. O yumi ken tok, **network** i givim wanpela wei. Dispela wei i gat pawa bilong impruvim **speech recognition accuracy**.

## Referens (Reference)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Impakt long Akademik (Academic Impact)

Dispela pepa i kisim moa long 1,400 **citations** long **Google Scholar**.

Long ol statistik we i no ful, ol saveman i usim pinis **Deep Residual Shrinkage Network (DRSN)** insait long moa long 1,000 **publications** o stadi. Ol dispela aplikesen i karamapim planti kain fil. Ol dispela fil i inkludim **mechanical engineering**, **electrical power**, **vision**, **healthcare**, **speech**, **text**, **radar**, na **remote sensing**.
