---
layout: post
title: "Deep Residual Shrinkage Network: Sarong Artificial Intelligence Method para sa Highly Noisy Data"
date: 2025-12-11
tags: [Deep Learning, AI]
mathjax: true
description: "An Deep Residual Shrinkage Network ay sarong improved variant kan Deep Residual Network. Sa esensya, pig-iintegrate kan Deep Residual Shrinkage Network an Deep Residual Network, attention mechanisms, asin soft thresholding functions."
---

**An Deep Residual Shrinkage Network ay sarong improved variant kan Deep Residual Network. Sa esensya, pig-iintegrate kan Deep Residual Shrinkage Network an Deep Residual Network, attention mechanisms, asin soft thresholding functions.**

**Pwede ta intindihon an working principle kan Deep Residual Shrinkage Network sa arog kaining paagi. Enot, piggagamit kan network an attention mechanisms para ma-identify su mga unimportant features. Tapos, piggagamit kan network an soft thresholding functions para iset sa zero su mga unimportant features na ini. Sa kabaliktaran, pig-iidentipar kan network su mga important features asin pigreretain an mga ini. An prosesong ini, pinapakusog an abilidad kan deep neural network. Nakakatabang ini sa network na mag-extract ning useful features gikan sa signals na igwang noise.**

## 1. Research Motivation (Motibasyon sa Research)

**Enot, an noise, dai talaga maiiwasan pag an algorithm nagkaklasipikar ning samples. Ang mga halimbawa kan noise na ini ay an Gaussian noise, pink noise, asin Laplacian noise.** Sa mas mahiwas na paghiling, an mga samples ay kadalasang may laog na impormasyon na irrelevant o mayo ning labot sa current classification task. Pwede ta ikonsiderar an mga irrelevant information na ini bilang noise. Pwede kaining mapababa an classification performance. (An Soft thresholding ay sarong key step sa dakul na signal denoising algorithms.)

Halimbawa, isipon ta na may conversation sa gilid kan kalsada. Su audio, posibleng may laog na tanog kan car horns asin wheels. Pwede kitang mag-perform ning speech recognition sa mga signals na ini. Siguradong makaka-apekto sa results su background sounds. Sa perspektibo kan deep learning, dapat i-eliminate kan deep neural network an features na nagko-correspond sa horns asin wheels. An elimination na ini, mapupugulan an features na maka-apekto sa speech recognition results.

**Ikaduwa, an amount of noise ay kadalasang iba-iba sa kada samples. Nangyayari an variation na ini maski sa laog mismo kan parehong dataset.** (An variation na ini ay may pagkakapareho sa attention mechanisms. Isipon nindo an sarong image dataset bilang halimbawa. An location kan target object ay pwedeng magkaiba-iba sa kada images. Pwede mag-focus an attention mechanisms sa specific location kan target object sa kada image.)

Bilang halimbawa, isipon nindo na nagtetrain kita ning cat-and-dog classifier gamit an limang images na may label na "dog." An Image 1 ay pwedeng may dog asin mouse. An Image 2 pwedeng may dog asin goose. An Image 3 pwedeng may dog asin chicken. An Image 4 pwedeng may dog asin donkey. An Image 5 pwedeng may dog asin duck. Habang nagtetrain, an mga irrelevant objects ay magtatao ning interference sa classifier. An mga objects na ini ay su mice, geese, chickens, donkeys, asin ducks. An interference na ini, magreresulta sa pagbaba kan classification accuracy. Kung kaya tang ma-identify an mga irrelevant objects na ini. Tapos, pwede tang ma-eliminate an features na nagko-correspond sa mga objects na yan. Sa paaging ini, mai-improve ta an accuracy kan cat-and-dog classifier.

## 2. Soft Thresholding

**An Soft thresholding ay sarong core step sa dakul na signal denoising algorithms. Pig-eeliminate kan algorithm an features kung an absolute values kan features ay mas hababa sa sarong specific threshold. Pig-iishrink o pinapasadit kan algorithm an features pasiring sa zero kung an absolute values kan features ay mas halangkaw sa threshold na ini.** Pwedeng i-implement kan mga researchers an soft thresholding gamit an formula na ini:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

An derivative kan soft thresholding output with respect sa input ay:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Pinapahiling kan formula sa itaas na an derivative kan soft thresholding ay pwedeng 1 o kaya 0. An property na ini ay identical o kapareho kan property kan ReLU activation function. Kaya, an soft thresholding, kayang bawasan an risk kan gradient vanishing asin gradient exploding sa deep learning algorithms.

**Sa soft thresholding function, an setting kan threshold dapat mag-satisfy sa duwang conditions. Enot, an threshold dapat positive number. Ikaduwa, an threshold dai pwedeng mag-exceed sa maximum value kan input signal. Kung dai, an output magiging entirely zero.**

**Saka, mas maray kung an threshold ay mag-satisfy man sa sarong third condition. Kada sample dapat igwang sadiring independent threshold base sa noise content kan sample.**

An rason ay dahil an noise content, kadalasang nag-aiba-iba sa mga samples. Halimbawa, an Sample A pwedeng may less noise habang an Sample B ay may more noise sa laog kan parehong dataset. Sa kasong ini, dapat maggamit an Sample A ning mas sadit na threshold habang nagso-soft thresholding. An Sample B naman, dapat maggamit ning mas dakula na threshold. Nawawara an explicit physical definitions kan mga features asin thresholds na ini sa deep neural networks. Pero, an basic underlying logic ay pareho man giraray. Boot sabihon, kada sample dapat may independent threshold. An specific noise content an nagdedeterminar kan threshold na ini.

## 3. Attention Mechanism

Madali man maiintindihan kan mga researchers an attention mechanisms sa field kan computer vision. An visual systems kan mga hayop, kayang mag-distinguish ning targets sa paagi kan mabilis na pag-scan sa bilog na area. Pagkatapos, nagpopokus an visual systems ning attention sa target object. An action na ini, nagtutugot sa systems na mag-extract ning mas dakul na details. Kasabay kaini, pigsu-suppress kan systems an irrelevant information. Para sa specifics, hilingon tabi an literature dapit sa attention mechanisms.

An Squeeze-and-Excitation Network (SENet) ay nagrerepresentar ning sarong medyo bagong deep learning method na naggagamit ning attention mechanisms. Sa iba-ibang samples, an iba-ibang feature channels ay may iba-ibang contribution sa classification task. Naggagamit an SENet ning sarong sadit na sub-network para makakua ning sarong set of weights. Tapos, pigmumultiply kan SENet an mga weights na ini sa features kan respective channels. An operation na ini, ina-adjust an magnitude kan features sa kada channel. Pwede ta hilingon an process na ini bilang pag-aapply ning varying levels of attention sa iba-ibang feature channels.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Sa approach na ini, kada sample ay nagkakaigwa ning independent set of weights. Sa ibang tataramon, an weights para sa maski arin na duwang arbitrary samples ay magkaiba. Sa SENet, an specific path para makakua ning weights ay "Global Pooling → Fully Connected Layer → ReLU Function → Fully Connected Layer → Sigmoid Function."

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/SENET_en_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding with Deep Attention Mechanism (Soft Thresholding na may Deep Attention Mechanism)

Piggagamit kan Deep Residual Shrinkage Network an structure kan SENet sub-network. Piggagamit kan network an structure na ini para ma-implement an soft thresholding under sa sarong deep attention mechanism. An sub-network (na naka-indicate sa laog kan red box) ay nagle-learn ning sarong set of thresholds (**Learn a set of thresholds**). Tapos, ina-apply kan network an soft thresholding sa kada feature channel gamit an mga thresholds na ini.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Sa sub-network na ini, enot na kinakalkula kan system an absolute values kan gabos na features sa input feature map. Tapos, pigpe-perform kan system an global average pooling asin averaging para makakua ning feature, na denotado bilang A. Sa ibong na path, pig-iinput kan system an feature map sa sarong sadit na fully connected network pagkatapos kan global average pooling. An fully connected network na ini, naggagamit ning Sigmoid function bilang final layer. Pigno-normalize kan function na ini an output sa pagitan kan 0 asin 1. An process na ini, nagbubunga ning coefficient, na denotado bilang *α*. Pwede tang i-express an final threshold bilang *α × A*. Kaya, an threshold ay product kan duwang numero. An sarong numero ay nasa pagitan kan 0 asin 1. An saro naman ay an average kan absolute values kan feature map. **Sinisigurado kan method na ini na an threshold ay positive. Sinisigurado man kan method na ini na an threshold ay bakong excessively large.**

**Saka, an iba-ibang samples ay nagreresulta sa iba-ibang thresholds. Bilang resulta, pwede ta intindihon an method na ini bilang sarong specialized attention mechanism. Pig-iidentipar kan mechanism an features na irrelevant sa current task. Pig-tatransform kan mechanism an mga features na ini pasiring sa values na harani sa zero gamit an duwang convolutional layers. Tapos, pigseset kan mechanism an features na ini sa zero gamit an soft thresholding. O kaya, pig-iidentipar kan mechanism an features na relevant sa current task. Pig-tatransform kan mechanism an mga features na ini pasiring sa values na harayo sa zero gamit an duwang convolutional layers. Sa final, pigpre-preserve kan mechanism an mga features na ini.**

Sa katapusan, pig-iistack ta an sarong number of basic modules (**Stack many basic modules**). Nag-iinclude man kita ning convolutional layers, batch normalization, activation functions, global average pooling, asin fully connected output layers. Binibilog kan process na ini an kumpletong Deep Residual Shrinkage Network.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-25-DRSN-en/DRSN_en_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Generalization Capability (Kapasidad sa Generalization)

An Deep Residual Shrinkage Network ay sarong general method para sa feature learning. An rason ay dahil an samples kadalasang may laog na noise sa dakul na feature learning tasks. An samples ay may laog man na irrelevant information. An noise asin irrelevant information na ini, pwedeng maka-apekto sa performance kan feature learning. Halimbawa:

Isipon nindo an image classification. An sarong image pwedeng may laog na dakul na ibang objects nang sabay-sabay. Pwede ta intindihon an mga objects na ini bilang "noise." An Deep Residual Shrinkage Network, posibleng magamit an attention mechanism. Pagnonotice kan network an "noise" na ini. Tapos, gagamiton kan network an soft thresholding para iset sa zero su features na nagko-correspond sa "noise" na ini. An action na ini, posibleng ma-improve an image classification accuracy.

Isipon nindo an speech recognition. Specifically, isipon an mga relatively noisy environments arog kan conversational settings sa gilid kan kalsada o sa laog kan factory workshop. An Deep Residual Shrinkage Network, pwedeng ma-improve an speech recognition accuracy. O maski paano, nagtatao an network ning sarong methodology. An methodology na ini ay capable na ma-improve an speech recognition accuracy.

## Reference (Reperensya)

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Academic Impact (Epekto sa Akademya)

An paper na ini ay nakaresibi na ning over 1,400 citations sa Google Scholar.

Base sa incomplete statistics, nai-apply na kan mga researchers an Deep Residual Shrinkage Network (DRSN) sa over 1,000 publications/studies. An mga applications na ini, sakop an mahiwas na range of fields. Kaiba sa mga fields na ini an mechanical engineering, electrical power, vision, healthcare, speech, text, radar, asin remote sensing.
