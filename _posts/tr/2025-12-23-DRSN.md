---
layout: post
title: "Derin Artık Büzülme Ağları (Deep Residual Shrinkage Networks): Yüksek Gürültülü Veriler İçin Bir Yapay Zeka Yöntemi"
date: 2025-12-23
tags: [Deep Learning, AI]
mathjax: true
description: "Derin Artık Büzülme Ağları (Deep Residual Shrinkage Networks), Derin Artık Ağların (Deep Residual Networks - ResNet) geliştirilmiş bir versiyonudur. Aslında bu yapı; Derin Artık Ağlar, dikkat mekanizması (attention mechanism) ve yumuşak eşikleme (soft thresholding) fonksiyonlarının bir entegrasyonudur."
---

**Derin Artık Büzülme Ağları (Deep Residual Shrinkage Networks), Derin Artık Ağların (Deep Residual Networks - ResNet) geliştirilmiş bir versiyonudur. Aslında bu yapı; Derin Artık Ağlar, dikkat mekanizması (attention mechanism) ve yumuşak eşikleme (soft thresholding) fonksiyonlarının bir entegrasyonudur.**

**Bir bakıma, Derin Artık Büzülme Ağlarının çalışma prensibi şu şekilde anlaşılabilir: Dikkat mekanizması aracılığıyla önemli olmayan öznitelikleri fark eder ve yumuşak eşikleme fonksiyonu ile bunları sıfırlar; veya tam tersi, dikkat mekanizması ile önemli öznitelikleri fark eder ve bunları korur. Böylece, derin sinir ağının gürültülü sinyallerden yararlı öznitelikleri çıkarma yeteneğini güçlendirir.**

## 1. Araştırma Motivasyonu
**Öncelikle, örnekler sınıflandırılırken, Gauss gürültüsü, pembe gürültü ve Laplace gürültüsü gibi bazı gürültülerin örneklerde bulunması kaçınılmazdır.** Daha geniş bir perspektiften bakıldığında, örnekler genellikle mevcut sınıflandırma göreviyle ilgisi olmayan bilgiler içerebilir ve bu bilgiler de "gürültü" olarak yorumlanabilir. Bu gürültüler, sınıflandırma performansını olumsuz etkileyebilir. (Yumuşak eşikleme, birçok sinyal gürültü giderme algoritmasında kilit bir adımdır.)

Örneğin, yol kenarında sohbet ederken, konuşma sesine araç kornaları ve tekerlek sesleri karışabilir. Bu ses sinyalleri üzerinde konuşma tanıma (speech recognition) yapıldığında, tanıma performansı kaçınılmaz olarak korna ve tekerlek seslerinden etkilenecektir. Derin öğrenme (Deep Learning) açısından bakıldığında, korna ve tekerlek seslerine karşılık gelen özniteliklerin (features), konuşma tanıma performansını etkilememesi için derin sinir ağı içinde silinmesi gerekir.

**İkincisi, aynı veri setinde olsalar bile, her bir örneğin gürültü miktarı genellikle birbirinden farklıdır.** (Bu durum, dikkat mekanizmasıyla benzerlik gösterir; örneğin bir görüntü veri setinde, hedef nesnenin konumu her resimde farklı olabilir ve dikkat mekanizması her resim için hedef nesnenin bulunduğu konuma odaklanabilir.)

Örneğin, bir kedi-köpek sınıflandırıcısı eğitilirken, etiketi "köpek" olan 5 görüntü düşünelim. 1. görüntü hem köpek hem fare, 2. görüntü hem köpek hem kaz, 3. görüntü hem köpek hem tavuk, 4. görüntü hem köpek hem eşek ve 5. görüntü hem köpek hem ördek içerebilir. Kedi-köpek sınıflandırıcısını eğitirken; fare, kaz, tavuk, eşek ve ördek gibi ilgisiz nesnelerin girişimiyle kaçınılmaz olarak karşılaşırız ve bu durum sınıflandırma doğruluğunun düşmesine neden olur. Eğer bu ilgisiz fare, kaz, tavuk, eşek ve ördekleri fark edip onlara karşılık gelen öznitelikleri silebilirsek, kedi-köpek sınıflandırıcısının doğruluğunu artırmamız mümkün olabilir.

## 2. Yumuşak Eşikleme (Soft Thresholding)
**Yumuşak eşikleme, birçok sinyal gürültü giderme (denoising) algoritmasının temel adımıdır; mutlak değeri belirli bir eşik değerinden (threshold) küçük olan öznitelikleri siler, mutlak değeri bu eşik değerinden büyük olan öznitelikleri ise sıfıra doğru "büzüştürür" (shrink).** Aşağıdaki formül ile ifade edilebilir:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Yumuşak eşikleme çıkışının girişe göre türevi şöyledir:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Yukarıdan da görüleceği üzere, yumuşak eşiklemenin türevi ya 1'dir ya da 0'dır. Bu özellik, ReLU aktivasyon fonksiyonu ile aynıdır. Bu nedenle, yumuşak eşikleme, derin öğrenme algoritmalarının "gradient vanishing" (türev kaybolması) ve "gradient exploding" (türev patlaması) riskleriyle karşılaşma olasılığını da azaltabilir.

**Yumuşak eşikleme fonksiyonunda, eşik değerinin ayarlanması iki koşulu sağlamalıdır: Birincisi, eşik değeri pozitif bir sayı olmalıdır; ikincisi, eşik değeri giriş sinyalinin maksimum değerinden büyük olmamalıdır, aksi takdirde çıkışın tamamı sıfır olur.**

**Aynı zamanda, eşik değerinin üçüncü bir koşulu daha sağlaması en iyisidir: Her örnek, kendi gürültü içeriğine göre kendisine özgü bağımsız bir eşik değerine sahip olmalıdır.**

Bunun nedeni, birçok örneğin gürültü içeriğinin genellikle farklı olmasıdır. Örneğin, aynı veri seti içinde Örnek A'nın daha az gürültü, Örnek B'nin ise daha çok gürültü içerdiği durumlarla sıkça karşılaşılır. Bu durumda, bir gürültü giderme algoritmasında yumuşak eşikleme yapılırken, Örnek A için daha küçük bir eşik değeri, Örnek B için ise daha büyük bir eşik değeri kullanılmalıdır. Derin sinir ağlarında bu öznitelikler ve eşik değerleri net fiziksel anlamlarını yitirse de, temel mantık aynıdır. Yani, her örnek kendi gürültü içeriğine göre bağımsız bir eşik değerine sahip olmalıdır.

## 3. Dikkat Mekanizması (Attention Mechanism)
Dikkat mekanizması, bilgisayarlı görü (Computer Vision) alanında anlaşılması nispeten kolay bir kavramdır. Hayvanların görme sistemi, tüm alanı hızla tarayarak hedef nesneyi bulabilir ve ardından ilgisiz bilgileri bastırırken daha fazla ayrıntı çıkarmak için dikkatini hedef nesneye odaklayabilir. Ayrıntılar için lütfen dikkat mekanizması ile ilgili literatüre bakınız.

Squeeze-and-Excitation Network (SENet), dikkat mekanizması tabanlı nispeten yeni bir derin öğrenme yöntemidir. Farklı örneklerde, farklı öznitelik kanallarının (feature channels) sınıflandırma görevine katkısı genellikle farklıdır. SENet, bir ağırlık grubu elde etmek için küçük bir alt ağ (sub-network) kullanır ve ardından her bir kanalın öznitelik büyüklüğünü ayarlamak için bu ağırlık grubunu ilgili kanalın öznitelikleriyle çarpar. Bu sürecin, her bir öznitelik kanalına farklı büyüklükte "dikkat" uygulamak olduğu düşünülebilir.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-tr/SENET_tr_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Bu yöntemde, her örneğin kendine özgü bağımsız bir ağırlık grubu vardır. Başka bir deyişle, rastgele iki örneğin ağırlıkları birbirinden farklıdır. SENet'te ağırlıkları elde etme yolu şöyledir: "Küresel Ortalama Havuzlama (Global Average Pooling) → Tam Bağlantılı Katman (Fully Connected Layer) → ReLU Fonksiyonu → Tam Bağlantılı Katman → Sigmoid Fonksiyonu".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-tr/SENET_tr_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Derin Dikkat Mekanizması Altında Yumuşak Eşikleme
Derin Artık Büzülme Ağları, derin dikkat mekanizması altında yumuşak eşiklemeyi gerçekleştirmek için yukarıda bahsedilen SENet'in alt ağ yapısından esinlenmiştir. Kırmızı kutu (görsellerdeki) içindeki alt ağ sayesinde, her bir öznitelik kanalına yumuşak eşikleme uygulamak üzere bir dizi eşik değeri öğrenilebilir.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-tr/DRSN_tr_1.png" alt="Derin Artık Büzülme Ağları (DRSN) Yapısı" width="75%">
</p>

Bu alt ağda, öncelikle giriş öznitelik haritasındaki (feature map) tüm özniteliklerin mutlak değerleri alınır. Ardından Küresel Ortalama Havuzlama (Global Average Pooling) ve ortalama alma işlemi ile A olarak adlandırılan bir öznitelik elde edilir. Diğer yolda, Küresel Ortalama Havuzlama işleminden geçen öznitelik haritası, küçük bir tam bağlantılı ağa (FC network) girer. Bu tam bağlantılı ağ, son katman olarak Sigmoid fonksiyonunu kullanır ve çıktıyı 0 ile 1 arasına normalize ederek α olarak adlandırılan bir katsayı elde eder. Nihai eşik değeri α×A olarak ifade edilebilir. Yani eşik değeri; 0 ile 1 arasında bir sayı ile öznitelik haritasının mutlak değerlerinin ortalamasının çarpımıdır. **Bu yöntem, eşik değerinin sadece pozitif olmasını sağlamakla kalmaz, aynı zamanda çok büyük olmamasını da garanti eder.**

**Ayrıca, farklı örnekler farklı eşik değerlerine sahip olur. Bu nedenle, bu yapı bir dereceye kadar özel bir dikkat mekanizması olarak anlaşılabilir: Mevcut görevle ilgisi olmayan öznitelikleri fark eder, iki konvolüsyon katmanı (convolutional layer) aracılığıyla bu öznitelikleri 0'a yakın değerlere dönüştürür ve yumuşak eşikleme yoluyla bunları tamamen sıfırlar; veya mevcut görevle ilgili öznitelikleri fark eder, bunları 0'dan uzak değerlere dönüştürür ve korur.**

Son olarak, belirli sayıda temel modül ile birlikte konvolüsyon katmanları, yığın normalizasyonu (Batch Normalization), aktivasyon fonksiyonları, küresel ortalama havuzlama ve tam bağlantılı çıkış katmanları yığınlanarak (stacking) tam bir Derin Artık Büzülme Ağı elde edilir.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-tr/DRSN_tr_2.png" alt="Derin Artık Büzülme Ağları (DRSN) Yapısı" width="55%">
</p>

## 5. Genelleştirilebilirlik
Derin Artık Büzülme Ağları aslında genel amaçlı bir öznitelik öğrenme yöntemidir. Çünkü birçok öznitelik öğrenme görevinde, örnekler az ya da çok bir miktar gürültü ve ilgisiz bilgi içerir. Bu gürültü ve ilgisiz bilgiler, öznitelik öğrenme performansını etkileyebilir. Örneğin:

Görüntü sınıflandırmada, eğer görüntü aynı anda birçok başka nesne içeriyorsa, bu nesneler "gürültü" olarak anlaşılabilir; Derin Artık Büzülme Ağları, dikkat mekanizması sayesinde bu "gürültüyü" fark edebilir ve ardından yumuşak eşikleme yardımıyla bu "gürültüye" karşılık gelen öznitelikleri sıfırlayarak görüntü sınıflandırma doğruluğunu artırabilir.

Konuşma tanımada (speech recognition), sesin nispeten gürültülü olduğu ortamlarda, örneğin yol kenarında veya fabrika atölyesinde sohbet ederken, Derin Artık Büzülme Ağları konuşma tanıma doğruluğunu artırabilir veya en azından doğruluğu artırabilecek bir yaklaşım sunabilir.

## Referanslar:

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

## Akademik Etki Durumu

Bu makalenin Google Akademik (Google Scholar) atıf sayısı 1400'ü aşmıştır.

Tam olmayan istatistiklere göre, Derin Artık Büzülme Ağları (DRSN) 1000'den fazla yayında doğrudan kullanılmış veya geliştirilerek makine, elektrik, bilgisayarlı görü, tıp, ses, metin, radar, uzaktan algılama gibi birçok alana uygulanmıştır.
