---
layout: post
title: "Deep Residual Shrinkage Network: Sebuah Metode Artificial Intelligence untuk Data dengan Noise Tinggi"
date: 2025-12-23
author: "Minghang Zhao, Harbin Institute of Technology"
tags: [Deep Learning, AI]
mathjax: true
description: "Deep Residual Shrinkage Network adalah varian penyempurnaan dari Deep Residual Network (ResNet). Pada dasarnya, ini adalah integrasi dari Deep Residual Network, mekanisme attention, dan fungsi soft thresholding."
lang: id
categories: id
ref: drsn-2025
buttons:
  - type: hit
    text: HIT Homepage
    url: https://homepage.hit.edu.cn/zhaominghang?lang=zh
  - type: scholar
    text: Google Scholar
    url: https://scholar.google.com/citations?user=k82TzLwAAAAJ&hl=en
  - type: ieee
    text: IEEE Paper
    url: https://ieeexplore.ieee.org/document/8850096
  - type: github
    text: GitHub Code
    url: https://github.com/zhao62/Deep-Residual-Shrinkage-Networks
  - type: citation
    text: "Citations: 1400+"
    url: https://scholar.google.com/citations?user=k82TzLwAAAAJ&hl=en
---

**Deep Residual Shrinkage Network adalah varian penyempurnaan dari Deep Residual Network (ResNet). Pada dasarnya, ini adalah integrasi dari Deep Residual Network, mekanisme *attention*, dan fungsi *soft thresholding*.**

**Sampai batas tertentu, prinsip kerja Deep Residual Shrinkage Network dapat dipahami sebagai berikut: metode ini menggunakan mekanisme *attention* untuk memperhatikan fitur yang tidak penting, lalu menggunakan fungsi *soft thresholding* untuk mengubah nilainya menjadi nol; atau sebaliknya, memperhatikan fitur yang penting dan mempertahankannya. Proses ini memperkuat kemampuan *deep neural network* untuk mengekstrak fitur yang berguna dari sinyal yang mengandung *noise*.**

## 1. Motivasi Penelitian

**Pertama, saat mengklasifikasikan sampel, keberadaan *noise*—seperti *Gaussian noise*, *pink noise*, dan *Laplacian noise*—tidak dapat dihindari.** Secara lebih luas, sampel sering kali memuat informasi yang tidak relevan dengan tugas klasifikasi saat ini, yang juga bisa dianggap sebagai *noise*. *Noise* ini bisa berdampak buruk pada hasil klasifikasi. (*Soft thresholding* adalah langkah kunci dalam banyak algoritma *signal denoising*.)

Sebagai contoh, saat mengobrol di pinggir jalan, suara percakapan mungkin bercampur dengan bunyi klakson dan roda kendaraan. Ketika melakukan pengenalan suara (*speech recognition*) pada sinyal-sinyal ini, hasilnya pasti akan terganggu oleh suara latar belakang tersebut. Dari sudut pandang *deep learning*, fitur-fitur yang berkaitan dengan klakson dan roda tersebut seharusnya dihapus di dalam *deep neural network* agar tidak memengaruhi hasil pengenalan suara.

**Kedua, bahkan dalam dataset yang sama, jumlah *noise* sering kali berbeda antara satu sampel dengan sampel lainnya.** (Hal ini memiliki kemiripan dengan mekanisme *attention*; mengambil contoh dataset gambar, lokasi objek target mungkin berbeda di setiap gambar, dan mekanisme *attention* dapat fokus pada lokasi spesifik objek target di setiap gambar tersebut.)

Misalnya, saat melatih klasifikasi kucing dan anjing, bayangkan ada 5 gambar berlabel "anjing". Gambar ke-1 mungkin berisi anjing dan tikus, gambar ke-2 anjing dan angsa, gambar ke-3 anjing dan ayam, gambar ke-4 anjing dan keledai, dan gambar ke-5 anjing dan bebek. Saat melatih model, kita pasti akan terganggu oleh objek tidak relevan seperti tikus, angsa, ayam, keledai, dan bebek tersebut, yang menyebabkan penurunan akurasi. Jika kita bisa menyadari keberadaan objek-objek tidak relevan ini (tikus, angsa, ayam, keledai, dan bebek) dan menghapus fitur yang sesuai dengannya, kita berpotensi meningkatkan akurasi klasifikasi kucing dan anjing.

## 2. Soft Thresholding

**Soft thresholding adalah langkah inti dalam banyak algoritma *signal denoising*. Metode ini menghapus fitur yang nilai absolutnya lebih kecil dari *threshold* (ambang batas) tertentu, dan "menyusutkan" (*shrink*) fitur yang nilai absolutnya lebih besar dari *threshold* tersebut mendekati nol.** Ini dapat diimplementasikan dengan rumus berikut:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

Turunan dari output *soft thresholding* terhadap inputnya adalah:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

Seperti terlihat di atas, turunan dari *soft thresholding* bernilai 1 atau 0. Sifat ini identik dengan fungsi aktivasi ReLU. Oleh karena itu, *soft thresholding* juga dapat mengurangi risiko algoritma *deep learning* mengalami masalah *gradient vanishing* dan *gradient exploding*.

**Dalam fungsi *soft thresholding*, pengaturan *threshold* harus memenuhi dua kondisi: pertama, *threshold* harus bernilai positif; kedua, *threshold* tidak boleh melebihi nilai maksimum sinyal input, jika tidak, outputnya akan menjadi nol semua.**

**Selain itu, sebaiknya *threshold* juga memenuhi kondisi ketiga: setiap sampel harus memiliki *threshold* independennya sendiri berdasarkan kandungan *noise*-nya.**

Alasannya adalah kandungan *noise* sering kali berbeda antar sampel. Contohnya, dalam dataset yang sama, sering terjadi Sampel A memiliki sedikit *noise* sedangkan Sampel B memiliki banyak *noise*. Dalam kasus ini, saat melakukan *soft thresholding* di algoritma *denoising*, Sampel A seharusnya menggunakan *threshold* yang lebih kecil, sedangkan Sampel B menggunakan *threshold* yang lebih besar. Di dalam *deep neural network*, meskipun fitur dan *threshold* ini kehilangan definisi fisik eksplisitnya, logika dasarnya tetap sama. Artinya, setiap sampel harus memiliki *threshold* independen yang ditentukan oleh kandungan *noise* spesifiknya.

## 3. Mekanisme Attention (Attention Mechanism)

Mekanisme *attention* relatif mudah dipahami dalam bidang *computer vision*. Sistem visual hewan dapat memindai seluruh area dengan cepat untuk menemukan objek target, lalu memusatkan perhatian pada objek tersebut untuk mengekstrak lebih banyak detail, sekaligus menekan informasi yang tidak relevan. Untuk detailnya, silakan merujuk pada literatur mengenai mekanisme *attention*.

Squeeze-and-Excitation Network (SENet) adalah metode *deep learning* yang relatif baru yang menggunakan mekanisme *attention*. Di antara sampel yang berbeda, kontribusi *channel* fitur yang berbeda terhadap tugas klasifikasi sering kali berbeda. SENet menggunakan *sub-network* kecil untuk mendapatkan sekumpulan bobot (*weights*), lalu mengalikan bobot ini dengan fitur dari masing-masing *channel* untuk menyesuaikan besaran fitur tersebut. Proses ini bisa dianggap sebagai memberikan tingkat "perhatian" (*attention*) yang berbeda pada setiap *channel* fitur.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-id/SENET_id_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

Dengan cara ini, setiap sampel akan memiliki sekumpulan bobot independennya sendiri. Dengan kata lain, bobot untuk dua sampel mana pun akan berbeda. Di SENet, jalur spesifik untuk mendapatkan bobot adalah "Global Pooling → Fully Connected Layer → Fungsi ReLU → Fully Connected Layer → Fungsi Sigmoid".

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-id/SENET_id_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. Soft Thresholding dengan Deep Attention Mechanism

Deep Residual Shrinkage Network mengadopsi struktur *sub-network* SENet yang disebutkan di atas untuk mengimplementasikan *soft thresholding* di bawah mekanisme *deep attention*. Melalui *sub-network* (ditandai dalam kotak merah pada diagram arsitektur), sekumpulan *threshold* dapat dipelajari untuk menerapkan *soft thresholding* pada setiap *channel* fitur.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-id/DRSN_id_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

Dalam *sub-network* ini, pertama-tama kita menghitung nilai absolut dari semua fitur pada *feature map* input. Kemudian, melalui *global average pooling* dan perata-rataan, diperoleh sebuah fitur yang dicatat sebagai A. Di jalur lain, *feature map* setelah *global average pooling* dimasukkan ke dalam jaringan *fully connected* kecil. Jaringan ini menggunakan fungsi Sigmoid sebagai lapisan terakhir untuk menormalisasi output antara 0 dan 1, menghasilkan koefisien yang dicatat sebagai α. *Threshold* akhirnya dapat dinyatakan sebagai α × A. Jadi, *threshold* adalah hasil kali antara angka 0 hingga 1 dengan rata-rata nilai absolut dari *feature map*. **Cara ini menjamin bahwa *threshold* tidak hanya bernilai positif, tetapi juga tidak terlalu besar.**

**Terlebih lagi, sampel yang berbeda akan menghasilkan *threshold* yang berbeda. Oleh karena itu, sampai batas tertentu, ini dapat dipahami sebagai mekanisme *attention* khusus: ia menyadari fitur yang tidak relevan dengan tugas saat ini, mengubahnya menjadi nilai yang mendekati nol melalui dua lapisan konvolusi, dan meng-nol-kannya menggunakan *soft thresholding*; atau sebaliknya, menyadari fitur yang relevan, mengubahnya menjadi nilai yang jauh dari nol, dan mempertahankannya.**

Terakhir, dengan menumpuk sejumlah modul dasar bersama dengan lapisan konvolusi (*convolutional layers*), *batch normalization*, fungsi aktivasi, *global average pooling*, dan lapisan output *fully connected*, kita mendapatkan Deep Residual Shrinkage Network yang utuh.

<p align="center">
  <img src="/assets/img/DRSN/2025-11-28-DRSN-id/DRSN_id_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. Kemampuan Generalisasi

Deep Residual Shrinkage Network pada dasarnya adalah metode pembelajaran fitur (*feature learning*) yang umum. Hal ini karena dalam banyak tugas pembelajaran fitur, sampel kurang lebih mengandung *noise* serta informasi yang tidak relevan. *Noise* dan informasi tak relevan ini dapat memengaruhi kinerja pembelajaran fitur. Sebagai contoh:

Dalam klasifikasi gambar, jika sebuah gambar memuat banyak objek lain secara bersamaan, objek-objek ini bisa dianggap sebagai "*noise*". Deep Residual Shrinkage Network mungkin mampu memanfaatkan mekanisme *attention* untuk menyadari "*noise*" ini, lalu menggunakan *soft thresholding* untuk meng-nol-kan fitur yang sesuai dengan "*noise*" tersebut, sehingga berpotensi meningkatkan akurasi klasifikasi gambar.

Dalam pengenalan suara, khususnya di lingkungan yang cukup bising seperti percakapan di pinggir jalan atau di dalam bengkel pabrik, Deep Residual Shrinkage Network mungkin dapat meningkatkan akurasi pengenalan suara, atau setidaknya, memberikan sebuah gagasan yang mampu meningkatkan akurasi tersebut.

## 6. Dampak Akademis

Makalah ini telah dikutip lebih dari 1400 kali di Google Scholar.

Menurut statistik yang belum lengkap, Deep Residual Shrinkage Network (DRSN) telah diterapkan secara langsung atau dikembangkan lebih lanjut dalam lebih dari 1000 publikasi ilmiah di berbagai bidang, termasuk teknik mesin, kelistrikan, *computer vision*, medis, pemrosesan suara, teks, radar, penginderaan jauh (*remote sensing*), dan lain-lain.

## Referensi

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

[https://ieeexplore.ieee.org/document/8850096](https://ieeexplore.ieee.org/document/8850096)

## BibTeX
```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```
