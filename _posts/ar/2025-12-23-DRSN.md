---
layout: post
title: "<div style='text-align: center; direction: rtl;'>Deep Residual Shrinkage Network: طريقة ذكاء اصطناعي للبيانات ذات الضجيج العالي</div>"
subtitle: "<div style='text-align: center; direction: ltr;'>An AI Method for Highly Noisy Data</div>"
date: 2025-12-23
author: "Minghang Zhao (赵明航)"
tags: [Deep Learning, AI]
mathjax: true
description: "تُعد Deep Residual Shrinkage Network (التي يمكن ترجمتها إلى شبكة الانكماش المتبقية العميقة) نسخة مُحسنة من Deep Residual Network (أو ما يُعرف بـ ResNet). وهي في الجوهر عبارة عن دمج بين ثلاثة عناصر: Deep Residual Network، وآليات الانتباه (Attention Mechanisms)، ودوال العتبة الناعمة (Soft Thresholding)."
lang: ar
dir: rtl
categories: ar
ref: drsn-2025
---

<!-- 核心修正：添加一段强制样式的 CSS -->
<style>
  /* 1. 强制所有层级的标题 (H1-H6) 必须右对齐 */
  h1, h2, h3, h4, h5, h6 {
    text-align: right !important;
  }
  
  /* 2. 确保正文里的列表也靠右 */
  ul, ol {
    text-align: right !important;
    direction: rtl !important;
    padding-right: 40px; /* 让列表的小圆点出现在右侧 */
    padding-left: 0;
  }

  /* 3. 强制代码块 (BibTeX) 从左向右显示，否则代码会乱 */
  pre, code {
    text-align: left !important;
    direction: ltr !important;
  }
  
  /* 4. 确保参考文献列表中的英文左对齐 */
  .references-ltr {
    text-align: left !important;
    direction: ltr !important;
  }
</style>

<!-- 保持之前的 markdown="1" 容器不变 -->
<div dir="rtl" markdown="1" style="text-align: right; direction: rtl; font-family: 'Sakkal Majalla', 'Traditional Arabic', serif; font-size: 1.1em;">

تُعد <strong>Deep Residual Shrinkage Network</strong> (التي يمكن ترجمتها إلى "شبكة الانكماش المتبقية العميقة") نسخة مُحسنة من <strong>Deep Residual Network</strong> (أو ما يُعرف بـ <strong>ResNet</strong>). وهي في الجوهر عبارة عن دمج بين ثلاثة عناصر: <strong>Deep Residual Network</strong>، وآليات الانتباه (<strong>Attention Mechanisms</strong>)، ودوال العتبة الناعمة (<strong>Soft Thresholding</strong>).

إلى حد معين، يمكن فهم مبدأ عمل <strong>Deep Residual Shrinkage Network</strong> كالتالي: تقوم الشبكة باستخدام <strong>Attention Mechanism</strong> لملاحظة الميزات (Features) غير المهمة، ثم تستخدم <strong>Soft Thresholding</strong> لتحويل قيم هذه الميزات إلى صفر؛ وبالمقابل، تلاحظ الميزات المهمة وتبقي عليها. هذه العملية تعزز قدرة الشبكة العصبية العميقة (<strong>Deep Neural Network</strong>) على استخراج الميزات المفيدة من الإشارات التي تحتوي على ضجيج (<strong>Noise</strong>).

## 1. دافع البحث (Research Motivation)

**أولاً، عند تصنيف العينات (Samples)، من المحتم وجود بعض الضجيج (<strong>Noise</strong>)، مثل <strong>Gaussian Noise</strong>، و <strong>Pink Noise</strong>، و <strong>Laplacian Noise</strong> وغيرها.** وبمفهوم أوسع، غالباً ما تحتوي العينات على معلومات لا علاقة لها بمهمة التصنيف الحالية، ويمكن اعتبار هذه المعلومات أيضاً بمثابة "ضجيج". هذا الضجيج قد يؤثر سلباً على دقة التصنيف. (تُعد <strong>Soft Thresholding</strong> خطوة أساسية في العديد من خوارزميات إزالة الضجيج من الإشارات أو ما يسمى <strong>Signal Denoising</strong>).

على سبيل المثال، عند التحدث بجانب الطريق، قد يختلط صوت المحادثة بأصوات أبواق السيارات وعجلات المركبات. عند إجراء "التعرف على الكلام" (<strong>Speech Recognition</strong>) على هذه الإشارات الصوتية، ستتأثر النتيجة حتماً بهذه الأصوات الدخيلة. من منظور <strong>Deep Learning</strong>، يجب حذف الميزات (<strong>Features</strong>) التي تمثل أصوات الأبواق والعجلات داخل الشبكة العصبية، لتجنب تأثيرها على دقة التعرف على الكلام.

**ثانياً، حتى داخل مجموعة البيانات (<strong>Dataset</strong>) الواحدة، غالباً ما تختلف كمية الضجيج من عينة إلى أخرى.** (هذا يتشابه مع <strong>Attention Mechanism</strong>؛ فإذا أخذنا مجموعة صور كمثال، قد يختلف موقع "الجسم المستهدف" من صورة لأخرى؛ وهنا تقوم آلية الانتباه بالتركيز على الموقع المحدد للجسم في كل صورة على حدة).

لنفترض مثلاً أننا ندرب مُصنِّفاً للقطط والكلاب. بالنسبة لـ 5 صور تحمل علامة "كلب": الصورة الأولى قد تحتوي على كلب وفأر، الثانية كلب وإوزة، الثالثة كلب ودجاجة، الرابعة كلب وحمار، والخامسة كلب وبطة. عند تدريب المُصنِّف، سنتعرض حتماً لتشويش من الكائنات غير ذات الصلة (الفأر، الإوزة، الدجاجة، الحمار، البطة)، مما يقلل من دقة التصنيف. إذا تمكنا من ملاحظة هذه الكائنات الدخيلة وحذف الميزات (<strong>Features</strong>) الخاصة بها، فمن المحتمل أن نرفع دقة تصنيف القطط والكلاب.

## 2. العتبة الناعمة (Soft Thresholding)

**تُعتبر <strong>Soft Thresholding</strong> خطوة جوهرية في العديد من خوارزميات <strong>Signal Denoising</strong>. فهي تقوم بحذف الميزات التي تكون قيمتها المطلقة أقل من عتبة (<strong>Threshold</strong>) معينة، وتقوم بـ "تقليص" (Shrink) الميزات التي تكون قيمتها المطلقة أكبر من هذه العتبة باتجاه الصفر.** يمكن التعبير عنها بالمعادلة التالية:

$$
y = \begin{cases} 
x - \tau & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
x + \tau & x < -\tau 
\end{cases}
$$

أما مُشتقة دالة <strong>Soft Thresholding</strong> بالنسبة للمُدخل فهي:

$$
\frac{\partial y}{\partial x} = \begin{cases} 
1 & x > \tau \\ 
0 & -\tau \le x \le \tau \\ 
1 & x < -\tau 
\end{cases}
$$

كما نلاحظ أعلاه، فإن مشتقة <strong>Soft Thresholding</strong> تكون إما 1 أو 0. هذه الخاصية مماثلة لدالة التفعيل <strong>ReLU</strong>. لذلك، فإن <strong>Soft Thresholding</strong> تساعد أيضاً في تقليل مخاطر تعرض خوارزميات <strong>Deep Learning</strong> لمشاكل تلاشي التدرج (<strong>Gradient Vanishing</strong>) أو انفجار التدرج (<strong>Gradient Exploding</strong>).

**في دالة <strong>Soft Thresholding</strong>، يجب أن يحقق إعداد العتبة (<strong>Threshold</strong>) شرطين: الأول، أن تكون العتبة رقماً موجباً؛ والثاني، ألا تكون العتبة أكبر من القيمة العظمى للإشارة المُدخلة، وإلا ستكون المخرجات جميعها أصفاراً.**

**بالإضافة إلى ذلك، يُفضل أن تحقق العتبة شرطاً ثالثاً: يجب أن يكون لكل عينة عتبة مستقلة خاصة بها بناءً على كمية الضجيج التي تحتويها.**

السبب في ذلك هو أن محتوى الضجيج يختلف غالباً بين العينات. فمثلاً، من الشائع في نفس مجموعة البيانات أن تحتوي العينة A على ضجيج قليل، بينما تحتوي العينة B على ضجيج كثير. في هذه الحالة، عند تطبيق <strong>Soft Thresholding</strong> داخل خوارزمية إزالة الضجيج، يجب أن تستخدم العينة A عتبة صغيرة، بينما تستخدم العينة B عتبة كبيرة. في الشبكات العصبية العميقة، ورغم أن هذه الميزات والعتبات تفقد معانيها الفيزيائية الملموسة، إلا أن المنطق الأساسي يظل كما هو. بمعنى آخر، يجب أن تمتلك كل عينة عتبة مستقلة خاصة بها تتناسب مع محتواها من الضجيج.

## 3. آلية الانتباه (Attention Mechanism)

من السهل فهم <strong>Attention Mechanism</strong> في مجال الرؤية الحاسوبية (<strong>Computer Vision</strong>). فالنظام البصري لدى الحيوانات يمكنه مسح كامل المنطقة بسرعة لاكتشاف الجسم المستهدف، ومن ثم تركيز الانتباه عليه لاستخراج تفاصيل أكثر، مع تجاهل المعلومات غير ذات الصلة. لمزيد من التفاصيل، يرجى الرجوع إلى الأبحاث المتعلقة بـ <strong>Attention Mechanism</strong>.

تُعد شبكة <strong>Squeeze-and-Excitation Network (SENet)</strong> طريقة حديثة نسبياً في <strong>Deep Learning</strong> تعتمد على آليات الانتباه. في العينات المختلفة، تختلف مساهمة "قنوات الميزات" (<strong>Feature Channels</strong>) المختلفة في مهمة التصنيف. تستخدم <strong>SENet</strong> شبكة فرعية صغيرة للحصول على مجموعة من الأوزان (<strong>Weights</strong>)، ثم تضرب هذه الأوزان في ميزات القنوات المختلفة لضبط حجم الميزات في كل قناة. يمكن اعتبار هذه العملية بمثابة تطبيق درجات متفاوتة من "الانتباه" على قنوات الميزات المختلفة.

<p align="center">
  <img src="/assets/img/DRSN/2025-12-01-DRSN-ar/SENET_ar_1.png" alt="Squeeze-and-Excitation Network" width="90%">
</p>

بهذه الطريقة، تمتلك كل عينة مجموعة مستقلة من الأوزان. بعبارة أخرى، الأوزان الخاصة بأي عينتين تكون مختلفة. في <strong>SENet</strong>، المسار المحدد للحصول على الأوزان هو: "**Global Pooling** ← **Fully Connected Layer** ← **ReLU** ← **Fully Connected Layer** ← **Sigmoid**".

<p align="center">
  <img src="/assets/img/DRSN/2025-12-01-DRSN-ar/SENET_ar_2.png" alt="Squeeze-and-Excitation Network" width="60%">
</p>

## 4. تطبيق Soft Thresholding تحت مظلة Deep Attention Mechanism

استوحت <strong>Deep Residual Shrinkage Network</strong> هيكلية الشبكة الفرعية من <strong>SENet</strong> المذكورة أعلاه، لتحقيق <strong>Soft Thresholding</strong> ضمن آلية انتباه عميقة. من خلال الشبكة الفرعية (الموضحة داخل الإطار الأحمر في المخططات الهيكلية)، يمكن تعلم مجموعة من العتبات لتطبيق <strong>Soft Thresholding</strong> على كل قناة من قنوات الميزات.

<p align="center">
  <img src="/assets/img/DRSN/2025-12-01-DRSN-ar/DRSN_ar_1.png" alt="Deep Residual Shrinkage Network" width="75%">
</p>

في هذه الشبكة الفرعية، يتم أولاً حساب القيم المطلقة لجميع الميزات في خريطة الميزات (<strong>Feature Map</strong>). ثم، من خلال <strong>Global Average Pooling</strong> وحساب المتوسط، نحصل على ميزة نرمز لها بـ A. في مسار آخر، يتم إدخال خريطة الميزات (بعد عملية الـ Global Average Pooling) إلى شبكة صغيرة متصلة بالكامل (<strong>Fully Connected Network</strong>). تنتهي هذه الشبكة بدالة <strong>Sigmoid</strong> كآخر طبقة لضبط المخرجات بين 0 و 1، مما يعطينا معاملاً نرمز له بـ α.
يمكن التعبير عن العتبة النهائية بـ α × A. وبالتالي، العتبة هي عبارة عن: (رقم بين 0 و 1) × (متوسط القيم المطلقة لخريطة الميزات). **تضمن هذه الطريقة أن تكون العتبة موجبة، وألا تكون كبيرة جداً في نفس الوقت.**

**علاوة على ذلك، ستحصل العينات المختلفة على عتبات مختلفة. لذلك، إلى حد معين، يمكن فهم هذا كنوع خاص من <strong>Attention Mechanism</strong>: حيث تلاحظ الشبكة الميزات غير المتعلقة بالمهمة الحالية، وتقوم بتحويلها عبر طبقتين من <strong>Convolutional Layers</strong> إلى قيم قريبة من الصفر، ثم تستخدم <strong>Soft Thresholding</strong> لجعلها صفراً تماماً؛ أو تلاحظ الميزات المتعلقة بالمهمة، وتحولها إلى قيم بعيدة عن الصفر، لتقوم بالاحتفاظ بها.**

أخيراً، من خلال تكديس عدد معين من الوحدات الأساسية بالإضافة إلى <strong>Convolutional Layers</strong>، و <strong>Batch Normalization</strong>، و <strong>Activation Functions</strong>، و <strong>Global Average Pooling</strong>، وطبقة الإخراج <strong>Fully Connected Layer</strong>، نحصل على <strong>Deep Residual Shrinkage Network</strong> الكاملة.

<p align="center">
  <img src="/assets/img/DRSN/2025-12-01-DRSN-ar/DRSN_ar_2.png" alt="Deep Residual Shrinkage Network" width="55%">
</p>

## 5. قابلية التعميم (Generalization Capability)

في الواقع، تُعد <strong>Deep Residual Shrinkage Network</strong> طريقة عامة لتعلم الميزات (<strong>Feature Learning</strong>). وذلك لأن العينات في العديد من مهام تعلم الميزات تحتوي، بدرجة أو بأخرى، على بعض الضجيج والمعلومات غير ذات الصلة. هذا الضجيج والمعلومات غير المهمة قد تؤثر على جودة التعلم. على سبيل المثال:

في تصنيف الصور (<strong>Image Classification</strong>)، إذا كانت الصورة تحتوي على العديد من الأشياء الأخرى في نفس الوقت، يمكن اعتبار هذه الأشياء "ضجيجاً". قد تتمكن <strong>Deep Residual Shrinkage Network</strong>، بمساعدة <strong>Attention Mechanism</strong>، من ملاحظة هذا "الضجيج"، ثم استخدام <strong>Soft Thresholding</strong> لجعل الميزات المقابلة لهذا الضجيج صفراً، مما قد يرفع من دقة تصنيف الصور.

في التعرف على الكلام (<strong>Speech Recognition</strong>)، خاصة في البيئات الصاخبة مثل المحادثات بجانب الطريق أو داخل ورش المصانع، قد تتمكن <strong>Deep Residual Shrinkage Network</strong> من تحسين دقة التعرف على الكلام، أو على الأقل تقديم منهجية قادرة على تحسين الدقة.


## 6. التأثير الأكاديمي (Academic Impact)

تجاوز عدد الاستشهادات بهذه الورقة البحثية في "Google Scholar" أكثر من 1400 استشهاد.

وفقاً لإحصائيات غير كاملة، تم استخدام <strong>Deep Residual Shrinkage Networks</strong> (أو تحسينها وتطبيقها) في أكثر من 1000 بحث منشور في مجالات متعددة تشمل الهندسة الميكانيكية، والطاقة الكهربائية، والرؤية الحاسوبية، والمجال الطبي، ومعالجة الصوت والنصوص، والرادار، والاستشعار عن بعد.

## المراجع (Reference)

<div class="references-ltr" dir="ltr" style="text-align: left; direction: ltr;">

Minghang Zhao, Shisheng Zhong, Xuyun Fu, Baoping Tang, Michael Pecht, Deep residual shrinkage networks for fault diagnosis, IEEE Transactions on Industrial Informatics, 2020, 16(7): 4681-4690.

<a href="https://ieeexplore.ieee.org/document/8850096">https://ieeexplore.ieee.org/document/8850096</a>

</div>

## BibTeX

```bibtex
@article{Zhao2020,
  author    = {Minghang Zhao and Shisheng Zhong and Xuyun Fu and Baoping Tang and Michael Pecht},
  title     = {Deep Residual Shrinkage Networks for Fault Diagnosis},
  journal   = {IEEE Transactions on Industrial Informatics},
  year      = {2020},
  volume    = {16},
  number    = {7},
  pages     = {4681-4690},
  doi       = {10.1109/TII.2019.2943898}
}
```

</div>
